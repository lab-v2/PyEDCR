{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZJ5EN5YTEHB"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchinfo\n",
    "!pip install LTNtorch\n",
    "!pip install timm\n",
    "!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  7 21:01:18 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A30          On   | 00000000:21:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    30W / 165W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1gi5XVBtQYhj"
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "from typing import Tuple, List, Dict\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Data Manipulation and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Machine Learning Frameworks\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchsummary import summary\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "\n",
    "# Model Architectures\n",
    "import timm\n",
    "\n",
    "# Experimentation and Optimization\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, AsyncHyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from hyperopt import hp\n",
    "\n",
    "# Logic Tensor Network\n",
    "import ltn\n",
    "\n",
    "# Miscellaneous\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Parallel Processing\n",
    "from typing import List\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "# Other\n",
    "from torchvision import datasets\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from abc import ABC\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bR1C9in6P9Sp"
   },
   "outputs": [],
   "source": [
    "def create_label_dict(category_dict):\n",
    "    # Initialize dictionaries\n",
    "    coarse_label_dict = {}\n",
    "    fine_label_dict = {}\n",
    "    coarse_to_fine = {}\n",
    "\n",
    "    # Assign numerical labels\n",
    "    coarse_label_counter = 0\n",
    "    fine_label_counter = len(category_dict)\n",
    "\n",
    "    # Iterate through the input dictionary\n",
    "    for category, labels in category_dict.items():\n",
    "        # Assign a numerical label to the coarse category\n",
    "        coarse_label_dict[category] = coarse_label_counter\n",
    "\n",
    "        # Create an empty list to store fine labels for this coarse category\n",
    "        coarse_to_fine[coarse_label_counter] = []\n",
    "\n",
    "        # Iterate through labels in the category\n",
    "        for label in labels:\n",
    "            # Assign a numerical label to the fine label\n",
    "            fine_label_dict[label] = fine_label_counter\n",
    "\n",
    "            # Add the fine label to the list of fine labels for this coarse category\n",
    "            coarse_to_fine[coarse_label_counter].append(fine_label_counter)\n",
    "\n",
    "            # Increment the fine label counter\n",
    "            fine_label_counter += 1\n",
    "\n",
    "        # Increment the coarse label counter\n",
    "        coarse_label_counter += 1\n",
    "\n",
    "    # Return the resulting dictionaries\n",
    "    return coarse_label_dict, fine_label_dict, coarse_to_fine\n",
    "\n",
    "\n",
    "def create_one_hot_tensors(fine_label_dict, coarse_label_dict):\n",
    "    l = {}\n",
    "    num_labels = len(coarse_label_dict)+len(fine_label_dict)\n",
    "    for label in range(num_labels):\n",
    "        one_hot = torch.zeros(num_labels)\n",
    "        one_hot[label] = 1.0\n",
    "        l[label] = ltn.Constant(one_hot, trainable=True)\n",
    "    return l\n",
    "\n",
    "\n",
    "def create_inverse_dict(coarse_label_dict, fine_label_dict):\n",
    "    inverse_dict = {}\n",
    "    for label, value in coarse_label_dict.items():\n",
    "        inverse_dict[value] = label\n",
    "\n",
    "    for label, value in fine_label_dict.items():\n",
    "        inverse_dict[value] = label\n",
    "\n",
    "    return inverse_dict\n",
    "\n",
    "\n",
    "def extract_labels(folder_path):\n",
    "    parts = folder_path.split(os.path.sep)\n",
    "    coarse_label = parts[-2]\n",
    "    fine_label = parts[-1]\n",
    "    return coarse_label, fine_label\n",
    "\n",
    "\n",
    "def search_for_images_and_labels(folder):\n",
    "    data = []\n",
    "    for image_path in glob.glob(os.path.join(folder, \"*.jpg\")):\n",
    "        coarse_label, fine_label = extract_labels(folder)\n",
    "        data.append({\n",
    "            'completed_relative_path': os.path.abspath(image_path),\n",
    "            'Coarse label': coarse_label,\n",
    "            'fine label': fine_label\n",
    "        })\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            data.extend(search_for_images_and_labels(subfolder_path))\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_image_folders(base_train_folder, base_test_folder):\n",
    "    # Process train folder\n",
    "    train_data = search_for_images_and_labels(base_train_folder)\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_train['Coarse label'] = df_train['Coarse label'].replace(\n",
    "        coarse_label_dict)\n",
    "    df_train['fine label'] = df_train['fine label'].replace(fine_label_dict)\n",
    "\n",
    "    # Filter train dataset\n",
    "    coarse_train_labels = [label for _, label in coarse_label_dict.items()]\n",
    "    fine_train_labels = [label for _, label in fine_label_dict.items()]\n",
    "    filter_train_coarse = df_train['Coarse label'].isin(coarse_train_labels)\n",
    "    filter_train_fine = df_train['fine label'].isin(fine_train_labels)\n",
    "    df_train = df_train[filter_train_coarse &\n",
    "                        filter_train_fine].reset_index(drop=True)\n",
    "\n",
    "    # Process test folder\n",
    "    test_data = search_for_images_and_labels(base_test_folder)\n",
    "    df_test = pd.DataFrame(test_data)\n",
    "    df_test['Coarse label'] = df_test['Coarse label'].replace(\n",
    "        coarse_label_dict)\n",
    "    df_test['fine label'] = df_test['fine label'].replace(fine_label_dict)\n",
    "\n",
    "    # Filter test dataset\n",
    "    coarse_test_labels = [label for _, label in coarse_label_dict.items()]\n",
    "    fine_test_labels = [label for _, label in fine_label_dict.items()]\n",
    "    filter_test_coarse = df_test['Coarse label'].isin(coarse_test_labels)\n",
    "    filter_test_fine = df_test['fine label'].isin(fine_test_labels)\n",
    "    df_test = df_test[filter_test_coarse &\n",
    "                      filter_test_fine].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "class DatasetGenerator():\n",
    "    \"\"\"\n",
    "    Create a dataloader to efficiently get data. The argument include:\n",
    "        - dataset: the dataframe containing image_path and label\n",
    "        - image_resize: size of the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, image_resize):\n",
    "        self.dataset = dataset\n",
    "        self.image_resize = image_resize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get index\n",
    "        idx = index % len(self.dataset)\n",
    "        image_path = self.dataset['completed_relative_path'][idx]\n",
    "        image = Image.open(image_path)\n",
    "        image_rgb = Image.new(\"RGB\", image.size)\n",
    "        image_rgb.paste(image)\n",
    "\n",
    "        coarse_label = self.dataset['Coarse label'][idx]\n",
    "        fine_label = self.dataset['fine label'][idx]\n",
    "\n",
    "        # Change image to float, resize image and\n",
    "\n",
    "        imagenet_stats = ([0.5] * 3, [0.5] * 3)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((self.image_resize, self.image_resize)),\n",
    "            transforms.RandomResizedCrop(\n",
    "                max((self.image_resize, self.image_resize))),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*imagenet_stats)\n",
    "        ])\n",
    "\n",
    "        image_rgb = preprocess(image_rgb)\n",
    "\n",
    "        return image_rgb, coarse_label, fine_label, image_path\n",
    "\n",
    "\n",
    "def create_data_loaders(df_train, df_test, image_resize, batch_size, num_coarse_label, num_all_label):\n",
    "    \"\"\"\n",
    "    Create data loaders for the training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): Training dataset.\n",
    "        df_test (pd.DataFrame): Testing dataset.\n",
    "        image_resize (int): Size to which images will be resized.\n",
    "        batch_size (int): Number of samples in each batch.\n",
    "        num_coarse_label (int): Number of coarse labels.\n",
    "        num_all_label (int): Total number of labels including fine and coarse labels.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: Training data loader.\n",
    "        DataLoader: Testing data loader.\n",
    "    \"\"\"\n",
    "    train_dataset = DatasetGenerator(df_train, image_resize)\n",
    "    test_dataset = DatasetGenerator(df_test, image_resize)\n",
    "\n",
    "    # Compute class weights for weighted sampling\n",
    "    fine_distribution = df_train[\"fine label\"].value_counts().tolist()\n",
    "    class_weights = [1 / df_train[\"fine label\"].value_counts()[i]\n",
    "                     for i in range(num_coarse_label, num_all_label)]\n",
    "    class_weights = [0] * num_coarse_label + class_weights\n",
    "    image_weights = [class_weights[i] for i in df_train['fine label']]\n",
    "    weight_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        image_weights, len(df_train))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              num_workers=4, pin_memory=True, sampler=weight_sampler)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "class ClearCache:\n",
    "    def __init__(self, device: torch.device):\n",
    "        self.device_backend = {'cuda': torch.cuda,\n",
    "                               'cpu': None}[device]\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.device_backend:\n",
    "            self.device_backend.empty_cache()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.device_backend:\n",
    "            self.device_backend.empty_cache()\n",
    "\n",
    "\n",
    "class FineTuner(torch.nn.Module, ABC):\n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__.split('Fine')[0].lower()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "# TODO: Whenever there is the change in loss function, check the implementation accordingly, to whether include\n",
    "# softmax or sigmoid on classifier or not\n",
    "\n",
    "\n",
    "class VITFineTuner(FineTuner):\n",
    "    def __init__(self,\n",
    "                 vit_model_index: int,\n",
    "                 num_classes: int):\n",
    "        super().__init__()\n",
    "        vit_model_name = ['b_16',\n",
    "                          'b_32',\n",
    "                          'l_16',\n",
    "                          'l_32',\n",
    "                          'h_14']\n",
    "        self.vit_model_name = vit_model_name[vit_model_index]\n",
    "        if self.vit_model_name == 'b_16':\n",
    "            vit_model = torchvision.models.vit_b_16\n",
    "        elif self.vit_model_name == 'b_32':\n",
    "            vit_model = torchvision.models.vit_b_32\n",
    "        elif self.vit_model_name == 'l_16':\n",
    "            vit_model = torchvision.models.vit_l_16\n",
    "        elif self.vit_model_name == 'l_32':\n",
    "            vit_model = torchvision.models.vit_l_32\n",
    "        elif self.vit_model_name == 'h_14':\n",
    "            vit_model = torchvision.models.vit_h_14\n",
    "        else:\n",
    "            # Handle the case when the model name is not recognized\n",
    "            raise ValueError(f\"Invalid vit_model_name: {self.vit_model_name}\")\n",
    "\n",
    "        vit_weights = eval(f\"torchvision.models.ViT_{'_'.join([s.upper() for s in self.vit_model_name.split('_')])}\"\n",
    "                           f\"_Weights.DEFAULT\")\n",
    "        self.vit = vit_model(weights=vit_weights)\n",
    "        self.vit.heads[-1] = torch.nn.Linear(in_features=self.vit.hidden_dim,\n",
    "                                             out_features=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.vit(x)\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{super().__str__()}_{self.vit_model_name}'\n",
    "\n",
    "# TODO: Whenever there is the change in loss function, check the implementation accordingly, to whether include\n",
    "# softmax or sigmoid on classifier or not\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label d. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class d.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        probs = self.sigmoid(x)\n",
    "        out = torch.sum(probs * d, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def compute_sat_normally(base_model,\n",
    "                         logits_to_predicate,\n",
    "                         data, labels_coarse, labels_fine,\n",
    "                         coarse_label_dict, fine_label_dict,\n",
    "                         coarse_to_fine, fine_grain_only=False, train_mode=False):\n",
    "    \"\"\"\n",
    "    compute satagg function for rules\n",
    "    argument:\n",
    "      - base_model: get probability of the class\n",
    "      - logits_to_predicate: get the satisfaction of a variable given the label\n",
    "      - data, labels_coarse, labels_fine\n",
    "      - coarse_label_dict, fine_label_dict,\n",
    "      - coarse_to_fine\n",
    "      - fine_grain_only: if true, the sat is changed accordingly\n",
    "      - train: whether to train model again, when data is still not convert to prediction yet\n",
    "\n",
    "    return:\n",
    "      sat_agg: sat_agg for all the rules\n",
    "\n",
    "    \"\"\"\n",
    "    Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "    And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "    Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "    Forall = ltn.Quantifier(\n",
    "        ltn.fuzzy_ops.AggregPMeanError(p=4), quantifier=\"f\")\n",
    "    SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "    if train_mode:\n",
    "        prediction = base_model(data)\n",
    "    else:\n",
    "        prediction = data\n",
    "\n",
    "    x = ltn.Variable(\"x\", prediction)\n",
    "\n",
    "    x_variables = {}\n",
    "    for name, label in fine_label_dict.items():\n",
    "        x_variables[label] = ltn.Variable(\n",
    "            name, prediction[labels_fine == label])\n",
    "    for name, label in coarse_label_dict.items():\n",
    "        x_variables[label] = ltn.Variable(\n",
    "            name, prediction[labels_coarse == label])\n",
    "\n",
    "    sat_agg_list = []\n",
    "    sat_agg_label = []\n",
    "\n",
    "    # Coarse labels: for all x[i], x[i] -> l[i]\n",
    "\n",
    "    for i in coarse_label_dict.values():\n",
    "        if x_variables[i].value.numel() != 0:\n",
    "            sat_agg_label.append(\n",
    "                f'for all (coarse label) x[{i}], x[{i}] -> l[{i}]')\n",
    "            sat_agg_list.append(\n",
    "                Forall(x_variables[i], logits_to_predicate(x_variables[i], l[i])))\n",
    "\n",
    "    # TODO: double check the rule\n",
    "    # Coarse Label: for all x[coarse], - {x[different coarse]}\n",
    "\n",
    "    # for i in coarse_label_dict.values():\n",
    "    #     for j in coarse_label_dict.values():\n",
    "    #         if i != j and x_variables[i].value.numel() != 0:\n",
    "    #             sat_agg_list.append(\n",
    "    #                 Forall(x_variables[i], Not(logits_to_predicate(x_variables[i], l[j]))))\n",
    "\n",
    "    # Coarse Label: for all x[coarse], - {x[coarse] and x[different coarse]}\n",
    "    for i in coarse_label_dict.values():\n",
    "        for j in coarse_label_dict.values():\n",
    "            if i != j :\n",
    "                sat_agg_list.append(Forall(x, Not(And(logits_to_predicate(x, l[i]), logits_to_predicate(x, l[j])))))\n",
    "\n",
    "    # Fine to coarse label: for all x[fine], x[fine] and x[correspond coarse]\n",
    "\n",
    "    for label_coarse, label_fine_list in coarse_to_fine.items():\n",
    "        for label_fine in label_fine_list:\n",
    "            if x_variables[label_fine].value.numel() != 0:\n",
    "              sat_agg_list.append(Forall(x_variables[label_fine],\n",
    "                                            And(logits_to_predicate(x_variables[label_fine], l[label_fine]), logits_to_predicate(x_variables[label_fine], l[label_coarse])))\n",
    "                                    )\n",
    "\n",
    "    # Fine labels: for all x[i], x[i] -> l[i]\n",
    "\n",
    "    for i in fine_label_dict.values():\n",
    "        if x_variables[i].value.numel() != 0:\n",
    "            sat_agg_list.append(\n",
    "                Forall(x_variables[i], logits_to_predicate(x_variables[i], l[i])))\n",
    "\n",
    "    # TODO: Double check the rule\n",
    "    # Fine Label: for all x[fine], - {x[different fine]}\n",
    "\n",
    "    # for i in fine_label_dict.values():\n",
    "    #     for j in fine_label_dict.values():\n",
    "    #         if i != j and x_variables[i].value.numel() != 0:\n",
    "    #             sat_agg_list.append(\n",
    "    #                 Forall(x_variables[i], Not(logits_to_predicate(x_variables[i], l[j]))))\n",
    "\n",
    "    # Fine Label: for all x[fine], -{x[fine], x[diff_fine]}\n",
    "\n",
    "    for i in fine_label_dict.values():\n",
    "        for j in fine_label_dict.values():\n",
    "            if i != j :\n",
    "                sat_agg_list.append(Forall(x, Not(And(logits_to_predicate(x, l[i]), logits_to_predicate(x, l[j])))))\n",
    "\n",
    "    sat_agg = SatAgg(\n",
    "        *sat_agg_list\n",
    "    )\n",
    "    return sat_agg\n",
    "\n",
    "\n",
    "def train(dataloader,\n",
    "          base_model: FineTuner, logits_to_predicate,\n",
    "          beta,\n",
    "          epoch,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          loss_mode,\n",
    "          fine_grain_only=False, mode='normal',\n",
    "          device=torch.device('cpu'),\n",
    "          coarse_label_dict={}, fine_label_dict={}, coarse_to_fine={}):\n",
    "    \"\"\"\n",
    "    Train the model using the provided dataloader for one epoch.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): Dataloader for training data.\n",
    "        base_model (FineTuner): The model to be trained.\n",
    "        logits_to_predicate: Function to convert logits to predicates.\n",
    "        beta (float): specify proportion of ltn and normal loss\n",
    "        epoch (int): training iteration\n",
    "        fine_grain_only (bool): If True, train only on fine-grained labels.\n",
    "        mode (str): Training mode: 'normal', 'ltn_normal', or 'ltn_combine'\n",
    "        coarse_label_dict (dict, optional): Dictionary mapping coarse labels to numerical labels. Default is an empty dictionary.\n",
    "        fine_label_dict (dict, optional): Dictionary mapping fine labels to numerical labels. Default is an empty dictionary.\n",
    "        coarse_to_fine (dict, optional): Dictionary mapping coarse labels to corresponding fine labels. Default is an empty dictionary..\n",
    "\n",
    "    Returns:\n",
    "        float: Running loss.\n",
    "        float: Precision for fine-grained labels.\n",
    "        float: Recall for fine-grained labels.\n",
    "        float: Precision for coarse labels.\n",
    "        float: Recall for coarse labels.\n",
    "    \"\"\"\n",
    "    num_coarse_label = len(coarse_label_dict)\n",
    "    num_fine_label = len(fine_label_dict)\n",
    "    num_all_label = num_fine_label+num_coarse_label\n",
    "    loss_fc = nn.CrossEntropyLoss()\n",
    "\n",
    "    base_model.train()\n",
    "    size = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    fine_label_ground_truth = []\n",
    "    fine_label_prediction = []\n",
    "    coarse_label_ground_truth = []\n",
    "    coarse_label_prediction = []\n",
    "\n",
    "    with tqdm(total=size) as pbar:\n",
    "        description = \"Epoch \" + str(epoch)\n",
    "        pbar.set_description_str(description)\n",
    "\n",
    "        for batch_idx, (data, labels_coarse, labels_fine, image_path) in enumerate(dataloader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Put image to device\n",
    "            data, labels_coarse, labels_fine = data.to(\n",
    "                device), labels_coarse.to(device), labels_fine.to(device)\n",
    "            labels_coarse_one_hot = torch.nn.functional.one_hot(\n",
    "                labels_coarse, num_classes=num_all_label).float()\n",
    "            labels_fine_one_hot = torch.nn.functional.one_hot(\n",
    "                labels_fine, num_classes=num_all_label).float()\n",
    "\n",
    "            # make prediction\n",
    "            prediction = base_model(data)\n",
    "\n",
    "            labels_one_hot = labels_fine_one_hot + labels_coarse_one_hot\n",
    "\n",
    "            if mode == 'normal':\n",
    "                if loss_mode == 'binary':\n",
    "                    loss_fc = torch.nn.BCEWithLogitsLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot)\n",
    "                elif loss_mode == 'marginal':\n",
    "                    loss_fc = torch.nn.MultiLabelMarginLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot.long())\n",
    "                elif loss_mode == 'softmarginal':\n",
    "                    loss_fc = torch.nn.MultiLabelSoftMarginLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot)\n",
    "\n",
    "            if mode == 'ltn_normal':\n",
    "                sat_agg = compute_sat_normally(base_model, logits_to_predicate,\n",
    "                                               prediction, labels_coarse, labels_fine,\n",
    "                                               coarse_label_dict, fine_label_dict, coarse_to_fine,\n",
    "                                               fine_grain_only)\n",
    "                loss = 1. - sat_agg\n",
    "\n",
    "            if mode == 'ltn_combine':\n",
    "                sat_agg = compute_sat_normally(base_model, logits_to_predicate,\n",
    "                                               prediction, labels_coarse, labels_fine,\n",
    "                                               coarse_label_dict, fine_label_dict, coarse_to_fine,\n",
    "                                               fine_grain_only)\n",
    "                if loss_mode == 'binary':\n",
    "                    loss_fc = torch.nn.BCEWithLogitsLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot))\n",
    "                elif loss_mode == 'marginal':\n",
    "                    loss_fc = torch.nn.MultiLabelMarginLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot.long()))\n",
    "                elif loss_mode == 'softmarginal':\n",
    "                    loss_fc = torch.nn.MultiLabelSoftMarginLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot))\n",
    "                elif loss_mode == 'josh':\n",
    "                    criterion = torch.nn.CrossEntropyLoss()\n",
    "                    # Get coarse label prediction and loss\n",
    "                    prediction_coarse_label = prediction[:, :num_coarse_label]\n",
    "                    batch_coarse_grain_loss = criterion(prediction_coarse_label, labels_coarse)\n",
    "\n",
    "                    # Get fine label prediction and loss (- num_coarse_label)\n",
    "                    prediction_fine_label = prediction[:, num_coarse_label:]\n",
    "                    labels_fine = labels_fine - num_coarse_label\n",
    "                    batch_fine_grain_loss = criterion(prediction_fine_label, labels_fine)\n",
    "\n",
    "                    # Loss function\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        ((num_coarse_label / num_all_label) * batch_coarse_grain_loss + \\\n",
    "                         (num_fine_label / num_all_label) * batch_fine_grain_loss)\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(base_model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy evaluation of coarse and fine grain\n",
    "            prediction = prediction.cpu().detach()\n",
    "\n",
    "            # Get coarse label prediction and ground truth\n",
    "            prediction_coarse_label = prediction[:, :num_coarse_label]\n",
    "            coarse_label_prediction_batch = torch.argmax(\n",
    "                prediction_coarse_label, dim=1)\n",
    "            coarse_label_prediction.extend(coarse_label_prediction_batch)\n",
    "            coarse_label_ground_truth.extend(labels_coarse.cpu().detach())\n",
    "\n",
    "            # Get fine label prediction and ground truth\n",
    "            prediction_fine_label = prediction[:, num_coarse_label:]\n",
    "            fine_label_prediction_batch = torch.argmax(\n",
    "                prediction_fine_label, dim=1)\n",
    "            fine_label_prediction.extend(fine_label_prediction_batch)\n",
    "            fine_label_ground_truth.extend(\n",
    "                labels_fine.cpu().detach())\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "        # Compute running loss\n",
    "        running_loss = running_loss / size\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy_fine = accuracy_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, normalize=True)\n",
    "        precision_fine = precision_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, average='macro')\n",
    "        recall_fine = recall_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, average='macro')\n",
    "        accuracy_coarse = accuracy_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, normalize=True)\n",
    "        precision_coarse = precision_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, average='macro')\n",
    "        recall_coarse = recall_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, average='macro')\n",
    "\n",
    "        # print evaluation metric:\n",
    "\n",
    "        pbar.set_postfix_str(\" epoch %d | loss %.4f | Train coarse acc %.3f |Train coarse Prec %.3f | Train coarse Rec %.3f | Train fine acc %.3f |Train fine Prec %.3f | Train fine Rec %.3f\" %\n",
    "                             (epoch, running_loss, accuracy_coarse, precision_coarse, recall_coarse, accuracy_fine, precision_fine, recall_fine))\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        save_metric = [running_loss,\n",
    "                       accuracy_fine, precision_fine, recall_fine,\n",
    "                       accuracy_coarse, precision_coarse, recall_coarse]\n",
    "\n",
    "    return save_metric\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid(dataloader,\n",
    "          base_model, logits_to_predicate,\n",
    "          beta,\n",
    "          loss_mode,\n",
    "          fine_grain_only=False, mode='normal',\n",
    "          device=torch.device('cpu'),\n",
    "          coarse_label_dict={}, fine_label_dict={}, coarse_to_fine={},):\n",
    "    \"\"\"\n",
    "    Validate the model using the provided dataloader.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): Dataloader for validation data.\n",
    "        base_model (FineTuner): The model to be evaluated.\n",
    "        logits_to_predicate (function): Function to convert logits to predicates.\n",
    "        beta: specify proportion of ltn and normal loss\n",
    "        fine_grain_only (bool, optional): If True, validate only on fine-grained labels. Default is False.\n",
    "        mode (str, optional): Validation mode: 'normal', 'ltn_normal', or 'ltn_combine'. Default is 'normal'.\n",
    "        device (torch.device, optional): Device to perform computations on. Default is 'cuda'.\n",
    "        coarse_label_dict (dict, optional): Dictionary mapping coarse labels to numerical labels. Default is an empty dictionary.\n",
    "        fine_label_dict (dict, optional): Dictionary mapping fine labels to numerical labels. Default is an empty dictionary.\n",
    "        coarse_to_fine (dict, optional): Dictionary mapping coarse labels to corresponding fine labels. Default is an empty dictionary.\n",
    "        model_name (string, optional): Name of the model to save, default is empty string\n",
    "\n",
    "    Returns:\n",
    "        float: Running loss.\n",
    "        float: Precision for fine-grained labels.\n",
    "        float: Recall for fine-grained labels.\n",
    "        float: Precision for coarse labels.\n",
    "        float: Recall for coarse labels.\n",
    "    \"\"\"\n",
    "    num_coarse_label = len(coarse_label_dict)\n",
    "    num_fine_label = len(fine_label_dict)\n",
    "    num_all_label = num_fine_label + num_coarse_label\n",
    "    base_model.eval()\n",
    "    size = len(dataloader)\n",
    "    running_loss = 0.0\n",
    "    fine_label_ground_truth = []\n",
    "    fine_label_prediction = []\n",
    "    coarse_label_ground_truth = []\n",
    "    coarse_label_prediction = []\n",
    "\n",
    "    with tqdm(total=size) as pbar:\n",
    "        description = \"Evaluate test set: \"\n",
    "        pbar.set_description_str(description)\n",
    "\n",
    "        for batch_idx, (data, labels_coarse, labels_fine, image_path) in enumerate(dataloader):\n",
    "            # Put image to device\n",
    "            data, labels_coarse, labels_fine = data.to(\n",
    "                device), labels_coarse.to(device), labels_fine.to(device)\n",
    "\n",
    "            # get ground truth\n",
    "            labels_coarse_one_hot = torch.nn.functional.one_hot(\n",
    "                labels_coarse, num_classes=num_all_label).float()\n",
    "            labels_fine_one_hot = torch.nn.functional.one_hot(\n",
    "                labels_fine, num_classes=num_all_label).float()\n",
    "\n",
    "            # make prediction\n",
    "            prediction = base_model(data)\n",
    "\n",
    "            labels_one_hot = labels_fine_one_hot + labels_coarse_one_hot\n",
    "\n",
    "            if mode == 'normal':\n",
    "                if loss_mode == 'binary':\n",
    "                    loss_fc = torch.nn.BCEWithLogitsLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot)\n",
    "                elif loss_mode == 'marginal':\n",
    "                    loss_fc = torch.nn.MultiLabelMarginLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot.long())\n",
    "                elif loss_mode == 'softmarginal':\n",
    "                    loss_fc = torch.nn.MultiLabelSoftMarginLoss()\n",
    "                    loss = loss_fc(prediction, labels_one_hot)\n",
    "\n",
    "            if mode == 'ltn_normal':\n",
    "                sat_agg = compute_sat_normally(base_model, logits_to_predicate,\n",
    "                                               prediction, labels_coarse, labels_fine,\n",
    "                                               coarse_label_dict, fine_label_dict, coarse_to_fine,\n",
    "                                               fine_grain_only)\n",
    "                loss = 1. - sat_agg\n",
    "\n",
    "            if mode == 'ltn_combine':\n",
    "                sat_agg = compute_sat_normally(base_model, logits_to_predicate,\n",
    "                                               prediction, labels_coarse, labels_fine,\n",
    "                                               coarse_label_dict, fine_label_dict, coarse_to_fine,\n",
    "                                               fine_grain_only)\n",
    "                if loss_mode == 'binary':\n",
    "                    loss_fc = torch.nn.BCEWithLogitsLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot))\n",
    "                elif loss_mode == 'marginal':\n",
    "                    loss_fc = torch.nn.MultiLabelMarginLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot.long()))\n",
    "                elif loss_mode == 'softmarginal':\n",
    "                    loss_fc = torch.nn.MultiLabelSoftMarginLoss()\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        (loss_fc(prediction, labels_one_hot))\n",
    "                elif loss_mode == 'josh':\n",
    "                    criterion = torch.nn.CrossEntropyLoss()\n",
    "                    # Get coarse label prediction and loss\n",
    "                    prediction_coarse_label = prediction[:, :num_coarse_label]\n",
    "                    batch_coarse_grain_loss = criterion(prediction_coarse_label, labels_coarse)\n",
    "\n",
    "                    # Get fine label prediction and loss (- num_coarse_label)\n",
    "                    prediction_fine_label = prediction[:, num_coarse_label:]\n",
    "                    labels_fine = labels_fine - num_coarse_label\n",
    "                    batch_fine_grain_loss = criterion(prediction_fine_label, labels_fine)\n",
    "\n",
    "                    # Loss function\n",
    "                    loss = beta*(1. - sat_agg) + (1 - beta) * \\\n",
    "                        ((num_coarse_label / num_all_label) * batch_coarse_grain_loss + \\\n",
    "                         (num_fine_label / num_all_label) * batch_fine_grain_loss)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Accuracy evaluation of coarse and fine grain\n",
    "            prediction = prediction.cpu().detach()\n",
    "\n",
    "            prediction_coarse_label = prediction[:, :num_coarse_label]\n",
    "            coarse_label_prediction_batch = torch.argmax(\n",
    "                prediction_coarse_label, dim=1)\n",
    "            coarse_label_prediction.extend(coarse_label_prediction_batch)\n",
    "            coarse_label_ground_truth.extend(labels_coarse.cpu().detach())\n",
    "\n",
    "            prediction_fine_label = prediction[:, num_coarse_label:]\n",
    "            fine_label_prediction_batch = torch.argmax(\n",
    "                prediction_fine_label, dim=1)\n",
    "            fine_label_prediction.extend(fine_label_prediction_batch)\n",
    "            fine_label_ground_truth.extend(\n",
    "                labels_fine.cpu().detach())\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "        # Compute running loss\n",
    "        running_loss = running_loss / size\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy_fine = accuracy_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, normalize=True)\n",
    "        precision_fine = precision_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, average='macro')\n",
    "        recall_fine = recall_score(\n",
    "            fine_label_ground_truth, fine_label_prediction, average='macro')\n",
    "        accuracy_coarse = accuracy_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, normalize=True)\n",
    "        precision_coarse = precision_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, average='macro')\n",
    "        recall_coarse = recall_score(\n",
    "            coarse_label_ground_truth, coarse_label_prediction, average='macro')\n",
    "\n",
    "        # print the training metrics\n",
    "\n",
    "        pbar.set_postfix_str(\" loss %.4f | Train coarse acc %.3f |Train coarse Prec %.3f | Train coarse Rec %.3f | Train fine acc %.3f |Train fine Prec %.3f | Train fine Rec %.3f\" %\n",
    "                             (running_loss, accuracy_coarse, precision_coarse, recall_coarse, accuracy_fine, precision_fine, recall_fine))\n",
    "\n",
    "        save_metric = [running_loss,\n",
    "                       accuracy_fine, precision_fine, recall_fine,\n",
    "                       accuracy_coarse, precision_coarse, recall_coarse]\n",
    "\n",
    "    return save_metric\n",
    "\n",
    "\n",
    "def transform_evaluation_metric(metric_list):\n",
    "    transformed_metrics = []\n",
    "    for metric_dict in metric_list:\n",
    "        try:\n",
    "            transformed_metrics.append({\n",
    "                'running_loss': metric_dict[0],\n",
    "                'accuracy_fine': metric_dict[1],\n",
    "                'precision_fine': metric_dict[2],\n",
    "                'recall_fine': metric_dict[3],\n",
    "                'accuracy_coarse': metric_dict[4],\n",
    "                'precision_coarse': metric_dict[5],\n",
    "                'recall_coarse': metric_dict[6]\n",
    "            })\n",
    "        except:\n",
    "            print('error in getting some metric')\n",
    "    return transformed_metrics\n",
    "\n",
    "\n",
    "def save_evaluation_metric(evaluation_metric_train_raw, evaluation_metric_valid_raw, path: str, description):\n",
    "    \"\"\"\n",
    "    Save the evaluation metric plot.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path to save the plot.\n",
    "        evaluation_metric_train (list): List of dictionaries containing evaluation metrics for training data.\n",
    "        evaluation_metric_valid (list): List of dictionaries containing evaluation metrics for validation data.\n",
    "        description (str)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    num_epochs = len(evaluation_metric_train_raw)\n",
    "    evaluation_metric_train = transform_evaluation_metric(\n",
    "        evaluation_metric_train_raw)\n",
    "    evaluation_metric_valid = transform_evaluation_metric(\n",
    "        evaluation_metric_valid_raw)\n",
    "    y_limits = [0.0, 1.0]\n",
    "\n",
    "    for metric in ['running_loss', 'accuracy_fine', 'precision_fine', 'recall_fine', 'accuracy_coarse', 'precision_coarse', 'recall_coarse']:\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(num_epochs), [\n",
    "                 element[metric] for element in evaluation_metric_train], label='Training', color='green')\n",
    "        plt.plot(range(num_epochs), [\n",
    "                 element[metric] for element in evaluation_metric_valid], label='Validation', color='blue')\n",
    "        plt.ylim(y_limits)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.title(f'{metric.capitalize()} Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        save_path = f'{path}/{description}_{metric.capitalize()}.png'\n",
    "\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()  # Close the plot to clear the memory\n",
    "\n",
    "\n",
    "def calculate_metrics_per_label(y_true: List[int], y_pred: List[int],\n",
    "                                labels: List[int]) -> Tuple[List[float], List[float], List[float], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1 score, and confusion matrix for each label.\n",
    "\n",
    "    Args:\n",
    "        y_true (List[int]): True labels.\n",
    "        y_pred (List[int]): Predicted labels.\n",
    "        labels (List[int]): List of label indices.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float], List[float], List[List[int]]]: Precision, recall, F1 score, and confusion matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    # accuracy_per_label = accuracy_score(y_true, y_pred)\n",
    "    precision_per_label = precision_score(\n",
    "        y_true, y_pred, average=None, labels=labels)\n",
    "    recall_per_label = recall_score(\n",
    "        y_true, y_pred, average=None, labels=labels)\n",
    "    accuracy_per_label = [precision * recall for precision,\n",
    "                          recall in zip(precision_per_label, recall_per_label)]\n",
    "    f1_per_label = f1_score(y_true, y_pred, average=None, labels=labels)\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    return accuracy_per_label, precision_per_label, recall_per_label, f1_per_label, confusion_mat\n",
    "\n",
    "\n",
    "def save_metrics_to_excel(y_true: List[int], y_pred: List[int],\n",
    "                          label_dict: Dict[int, str],\n",
    "                          model_name: str, path: str, description: str) -> None:\n",
    "    \"\"\"\n",
    "    Calculates metrics per label and saves the results to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        y_true (List[int]): True labels.\n",
    "        y_pred (List[int]): Predicted labels.\n",
    "        label_dict (Dict[int, str]): Dictionary mapping label indices to label names.\n",
    "        model_name (str): Name of the model.\n",
    "        path (str): Directory where the Excel file will be saved.\n",
    "        description (str)\n",
    "    \"\"\"\n",
    "    label_temp = [i for i in label_dict.values()]\n",
    "    accuracy, precision, recall, f1, confusion = calculate_metrics_per_label(y_true, y_pred, label_temp)\n",
    "    \n",
    "    # Create a list to store dictionaries with metrics\n",
    "    metrics_list = []\n",
    "    \n",
    "    inverse_dict = {value: key for key, value in label_dict.items()}\n",
    "    \n",
    "    for label_idx, label in enumerate(label_temp):\n",
    "        metrics_dict = {\n",
    "            'Label': inverse_dict[int(label)],\n",
    "            'Accuracy': accuracy[label_idx],\n",
    "            'Precision': precision[label_idx],\n",
    "            'Recall': recall[label_idx],\n",
    "            'F1': f1[label_idx],\n",
    "            'True Positives': confusion[label_idx][label_idx],\n",
    "            'True Negatives': confusion.sum() - confusion[label_idx].sum() - confusion[:, label_idx].sum() + confusion[label_idx][label_idx],\n",
    "            'False Positives': confusion[:, label_idx].sum() - confusion[label_idx][label_idx],\n",
    "            'False Negatives': confusion[label_idx].sum() - confusion[label_idx][label_idx]\n",
    "        }\n",
    "        metrics_list.append(metrics_dict)\n",
    "    \n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    metrics_df = pd.DataFrame(metrics_list, columns=['Label', 'Accuracy', 'Precision', 'Recall', 'F1', 'True Positives', 'True Negatives', 'False Positives', 'False Negatives'])\n",
    "\n",
    "\n",
    "    metrics_df.to_excel(\n",
    "        f'{path}/{description}_coarse_grained_{model_name}_test_metric.xlsx', index=False)\n",
    "\n",
    "\n",
    "def save_confusion_matrices(num_coarse_label: int, num_all_labels: int,\n",
    "                            base_model, test_loader: DataLoader,\n",
    "                            save_path: str,\n",
    "                            fine_grain_only: bool,\n",
    "                            description: str,) -> None:\n",
    "    \"\"\"\n",
    "    Compute and save confusion matrices for coarse and fine labels based on the predictions\n",
    "    from the provided base_model and test_loader. Save the generated matrices as images.\n",
    "\n",
    "    Args:\n",
    "        num_coarse_label (int): Number of coarse labels.\n",
    "        num_all_labels (int): Total number of labels (including coarse and fine labels).\n",
    "        base_model: PyTorch model for prediction.\n",
    "        test_loader (DataLoader): DataLoader containing test data.\n",
    "        save_path (str): Path to save the generated confusion matrix images.\n",
    "        fine_grain_only (bool): Train fine grain only or not\n",
    "        description (str)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    coarse_index = slice(num_coarse_label)\n",
    "    fine_index = slice(num_coarse_label, num_all_labels)\n",
    "\n",
    "    coarse_label_ground_truth = []\n",
    "    coarse_label_prediction = []\n",
    "    fine_label_ground_truth = []\n",
    "    fine_label_prediction = []\n",
    "    image_path_list = []\n",
    "\n",
    "    print(\"Save confusion matrices\")\n",
    "\n",
    "    # Iterate through the test data and make predictions\n",
    "    for batch_idx, (data, labels_coarse, labels_fine, image_path) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        prediction = base_model(data).cpu().detach()\n",
    "\n",
    "        prediction_coarse_label = prediction[:, :num_coarse_label]\n",
    "        coarse_label_prediction_batch = torch.argmax(\n",
    "            prediction_coarse_label, dim=1)\n",
    "        coarse_label_prediction.extend(coarse_label_prediction_batch)\n",
    "        coarse_label_ground_truth.extend(labels_coarse)\n",
    "\n",
    "        prediction_fine_label = prediction[:, num_coarse_label:]\n",
    "        fine_label_prediction_batch = torch.argmax(\n",
    "            prediction_fine_label, dim=1) + num_coarse_label\n",
    "        fine_label_prediction.extend(fine_label_prediction_batch)\n",
    "        fine_label_ground_truth.extend(labels_fine)\n",
    "\n",
    "        # get image path\n",
    "        image_path_list.extend(image_path)\n",
    "\n",
    "    # Compute confusion matrix for coarse labels\n",
    "    confusion_matrix_coarse = metrics.confusion_matrix(\n",
    "        coarse_label_ground_truth, coarse_label_prediction)\n",
    "    display_labels_coarse = [str(label)\n",
    "                             for label in range(num_coarse_label)]\n",
    "\n",
    "    # Plot and save coarse label confusion matrix\n",
    "    fig_coarse, ax_coarse = plt.subplots(figsize=(15, 15))\n",
    "    cm_display_coarse = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_coarse,\n",
    "                                               display_labels=display_labels_coarse)\n",
    "    cm_display_coarse.plot(ax=ax_coarse, values_format='d')\n",
    "    ax_coarse.set_title('Coarse Label Confusion Matrix')\n",
    "    plt.savefig(\n",
    "        f'{save_path}/{description}_coarse_label_confusion_matrix.png')\n",
    "    plt.close(fig_coarse)\n",
    "\n",
    "    print('Saved coarse label confusion matrix successfully')\n",
    "\n",
    "    # Compute confusion matrix for fine labels\n",
    "    confusion_matrix_fine = metrics.confusion_matrix(\n",
    "        fine_label_ground_truth, fine_label_prediction)\n",
    "    display_labels_fine = [str(label) for label in range(\n",
    "        num_coarse_label, num_all_labels)]\n",
    "\n",
    "    # Plot and save fine label confusion matrix\n",
    "    fig_fine, ax_fine = plt.subplots(figsize=(15, 15))\n",
    "    cm_display_fine = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_fine,\n",
    "                                             display_labels=display_labels_fine)\n",
    "    cm_display_fine.plot(ax=ax_fine, values_format='d')\n",
    "    ax_fine.set_title('Fine Label Confusion Matrix')\n",
    "    plt.savefig(f'{save_path}/{description}_fine_label_confusion_matrix.png')\n",
    "    plt.close(fig_fine)\n",
    "\n",
    "    print('Saved fine label confusion matrix successfully')\n",
    "\n",
    "    if not fine_grain_only:\n",
    "        print('save coarse grain excel file')\n",
    "        save_metrics_to_excel(coarse_label_ground_truth, coarse_label_prediction,\n",
    "                              coarse_label_dict, base_model,\n",
    "                              save_path,\n",
    "                              description + '_coarse'\n",
    "                              )\n",
    "\n",
    "    print('save fine grain excel file')\n",
    "    save_metrics_to_excel(fine_label_ground_truth, fine_label_prediction,\n",
    "                          fine_label_dict, base_model,\n",
    "                          save_path,\n",
    "                          description + '_fine'\n",
    "                          )\n",
    "\n",
    "    print('save excel file successfully')\n",
    "\n",
    "    # Save result for later use\n",
    "\n",
    "    coarse_label_prediction = np.array(coarse_label_prediction)\n",
    "    coarse_label_ground_truth = np.array(coarse_label_ground_truth)\n",
    "    image_path_list = np.array(image_path_list)\n",
    "\n",
    "    # Concatenate the arrays along the second axis (axis=1)\n",
    "    concatenated_array = np.column_stack(\n",
    "        (coarse_label_prediction, coarse_label_ground_truth, image_path_list))\n",
    "\n",
    "    # Save the concatenated array\n",
    "    with open(f'{save_path}/concatenated_data_coarse_{description}.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(concatenated_array, pickle_file)\n",
    "\n",
    "    print('Save concatenated coarse data successfully!')\n",
    "\n",
    "    fine_label_prediction = np.array(fine_label_prediction)\n",
    "    fine_label_ground_truth = np.array(fine_label_ground_truth)\n",
    "    image_path_list = np.array(image_path_list)\n",
    "\n",
    "    # Concatenate the arrays along the second axis (axis=1)\n",
    "    concatenated_array = np.column_stack(\n",
    "        (fine_label_prediction, fine_label_ground_truth, image_path_list))\n",
    "\n",
    "    # Save the concatenated array\n",
    "    with open(f'{save_path}/concatenated_data_fine_{description}.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(concatenated_array, pickle_file)\n",
    "\n",
    "    print('Save concatenated fine data successfully!')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lX_PU42TSbmC"
   },
   "outputs": [],
   "source": [
    "def hyper_parameter_tune(config):\n",
    "    # Load dataset\n",
    "    df_train, df_test = process_image_folders(\n",
    "        base_train_folder, base_test_folder)\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        df_train, df_test, image_resize, batch_size, num_coarse_label, num_all_label)\n",
    "\n",
    "    # Model Initialization\n",
    "    base_model = VITFineTuner(vit_model_index, num_output).to(device)\n",
    "    logits_to_predicate = ltn.Predicate(LogitsToPredicate()).to(ltn.device)\n",
    "\n",
    "    load_checkpoint_path = f\"/home/ngocbach/model/model_“b-16_normal_1e-4”.pth\"\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(load_checkpoint_path)\n",
    "\n",
    "    # Load model and optimizer states\n",
    "    base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print('load checkpoint successfully')\n",
    "\n",
    "    # Training Configuration\n",
    "    optimizer = torch.optim.Adam(base_model.parameters(), config['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, 2, 0.95)\n",
    "    beta = config['beta']\n",
    "\n",
    "    evaluation_metric_train = []\n",
    "    evaluation_metric_valid = []\n",
    "    accuracy_recent_coarse = []\n",
    "    accuracy_recent_fine = []\n",
    "\n",
    "    # Update description:\n",
    "    description = 'model ' + str(vit_model_index) + \\\n",
    "        ' ' + \"ltn_combine\" + ' ' + loss_mode + \\\n",
    "        ' ' + str(config['lr']) + ' ' + str(config['beta']) + ' ' + str(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    print(description)\n",
    "\n",
    "    for epoch in range(loaded_epoch, num_epochs):\n",
    "        with ClearCache(device):\n",
    "            evaluation_metric_train.append(train(train_loader,\n",
    "                                                  base_model, logits_to_predicate,\n",
    "                                                  beta,\n",
    "                                                  epoch,\n",
    "                                                  optimizer,\n",
    "                                                  scheduler,\n",
    "                                                  loss_mode,\n",
    "                                                  fine_grain_only, mode,\n",
    "                                                  device,\n",
    "                                                  coarse_label_dict, fine_label_dict, coarse_to_fine))\n",
    "            evaluation_metric_valid.append(valid(test_loader,\n",
    "                                                  base_model, logits_to_predicate,\n",
    "                                                  beta,\n",
    "                                                  loss_mode,\n",
    "                                                  fine_grain_only, mode,\n",
    "                                                  device,\n",
    "                                                  coarse_label_dict, fine_label_dict, coarse_to_fine))\n",
    "\n",
    "            accuracy_recent_coarse.append(evaluation_metric_valid[-1][1])\n",
    "        \n",
    "            accuracy_recent_fine.append(evaluation_metric_valid[-1][4])\n",
    "\n",
    "            # Save best checkpoint according to sum accuracy\n",
    "            if max(accuracy_recent_coarse) == accuracy_recent_coarse[-1] and max(accuracy_recent_fine) == accuracy_recent_fine[-1]:\n",
    "                torch.save({\"model_state_dict\": base_model.state_dict(),\n",
    "                            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                            \"scheduler\": scheduler.state_dict()},\n",
    "                            f\"{base_path}/model/model_{description}.pth\")\n",
    "                print(f\"Saved PyTorch Model State to {description}\")\n",
    "            \n",
    "            # Saving evaluation_metric_train\n",
    "            with open(f'{base_path}/model/evaluation_metric_train_{description}.pkl', 'wb') as f:\n",
    "                pickle.dump(evaluation_metric_train, f)\n",
    "\n",
    "            # Saving evaluation_metric_valid\n",
    "            with open(f'{base_path}/model/evaluation_metric_valid_{description}.pkl', 'wb') as f:\n",
    "                pickle.dump(evaluation_metric_valid, f)\n",
    "\n",
    "        print('#' * 100)\n",
    "\n",
    "    # Create a folder with the name 'description' inside the 'result' folder\n",
    "    result_folder_path = os.path.join(base_path, \"result\", description)\n",
    "    os.makedirs(result_folder_path, exist_ok=True)\n",
    "\n",
    "    save_evaluation_metric(evaluation_metric_train,\n",
    "                           evaluation_metric_valid, result_folder_path, description)\n",
    "\n",
    "    # Save confusion matrices to the result folder with the description\n",
    "    save_confusion_matrices(num_coarse_label, num_all_label,\n",
    "                            base_model, test_loader, result_folder_path,\n",
    "                            fine_grain_only, description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PiAkQp_Qnnu",
    "outputId": "5b6fcf37-967a-4865-b648-c8b68838169a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 ltn_combine binary 1e-05 0.8 2023-12-14\n",
      "coarse_label_dict:\n",
      "{'Air Defence': 0, 'BMP': 1, 'BTR': 2, 'Tank': 3, 'SPA': 4, 'BMD': 5, 'MT_LB': 6}\n",
      "\n",
      "fine_label_dict:\n",
      "{'30N6E': 7, 'Iskander': 8, 'Pantsir-S1': 9, 'Rs-24': 10, 'BMP-1': 11, 'BMP-2': 12, 'BMP-T15': 13, 'BRDM': 14, 'BTR-60': 15, 'BTR-70': 16, 'BTR-80': 17, 'T-14': 18, 'T-62': 19, 'T-64': 20, 'T-72': 21, 'T-80': 22, 'T-90': 23, '2S19_MSTA': 24, 'BM-30': 25, 'D-30': 26, 'Tornado': 27, 'TOS-1': 28, 'BMD': 29, 'MT_LB': 30}\n",
      "\n",
      "coarse_to_fine:\n",
      "{0: [7, 8, 9, 10], 1: [11, 12, 13], 2: [14, 15, 16, 17], 3: [18, 19, 20, 21, 22, 23], 4: [24, 25, 26, 27, 28], 5: [29], 6: [30]}\n",
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get dataset successfully\n",
      "model initialization successfully\n",
      "load checkpoint successfully\n"
     ]
    }
   ],
   "source": [
    "# Assigning argparse values to variables\n",
    "base_path = \"/home/ngocbach\"\n",
    "mode = \"ltn_combine\"\n",
    "vit_model_index = 0\n",
    "beta = 0.8\n",
    "lr = 0.00001\n",
    "fine_grain_only = False\n",
    "loss_mode = \"binary\"\n",
    "load_checkpoint = True\n",
    "num_epochs = 25\n",
    "batch_size = 32\n",
    "description = 'model ' + str(vit_model_index) + \\\n",
    "    ' ' + \"ltn_combine\" + ' ' + loss_mode + \\\n",
    "    ' ' + str(0.00001) + ' ' + str(0.8) + \" \" + str(datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "print(description)\n",
    "\n",
    "\n",
    "# All label\n",
    "category_dict = {\n",
    "    'Air Defence': ['30N6E', 'Iskander', 'Pantsir-S1', 'Rs-24'],\n",
    "    'BMP': ['BMP-1', 'BMP-2', 'BMP-T15'],\n",
    "    'BTR': ['BRDM', 'BTR-60', 'BTR-70', 'BTR-80'],\n",
    "    'Tank': ['T-14', 'T-62', 'T-64', 'T-72', 'T-80', 'T-90'],\n",
    "    'SPA': ['2S19_MSTA', 'BM-30', 'D-30', 'Tornado', 'TOS-1'],\n",
    "    'BMD': ['BMD'],\n",
    "    'MT_LB': ['MT_LB']\n",
    "}\n",
    "\n",
    "coarse_label_dict, fine_label_dict, coarse_to_fine = create_label_dict(\n",
    "    category_dict)\n",
    "\n",
    "# Print the resulting dictionaries\n",
    "print(\"coarse_label_dict:\")\n",
    "print(coarse_label_dict)\n",
    "print(\"\\nfine_label_dict:\")\n",
    "print(fine_label_dict)\n",
    "print(\"\\ncoarse_to_fine:\")\n",
    "print(coarse_to_fine)\n",
    "\n",
    "l = create_one_hot_tensors(fine_label_dict, coarse_label_dict)\n",
    "inverse_dict = create_inverse_dict(coarse_label_dict, fine_label_dict)\n",
    "\n",
    "# Constants and Configuration\n",
    "image_resize = 224\n",
    "num_coarse_label = len(coarse_label_dict)\n",
    "num_fine_label = len(fine_label_dict)\n",
    "num_all_label = num_fine_label + num_coarse_label\n",
    "num_output = num_all_label\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device: ', device)\n",
    "base_train_folder = f\"{base_path}/dataset/train\"\n",
    "base_test_folder = f\"{base_path}/dataset/test\"\n",
    "load_checkpoint_path = f\"/home/ngocbach/model/model_“b-16_normal_1e-4”.pth\"\n",
    "\n",
    "# Load dataset\n",
    "df_train, df_test = process_image_folders(\n",
    "    base_train_folder, base_test_folder)\n",
    "train_loader, test_loader = create_data_loaders(\n",
    "    df_train, df_test, image_resize, batch_size, num_coarse_label, num_all_label)\n",
    "\n",
    "print('get dataset successfully')\n",
    "\n",
    "# Model Initialization\n",
    "base_model = VITFineTuner(vit_model_index, num_output).to(device)\n",
    "logits_to_predicate = ltn.Predicate(LogitsToPredicate()).to(ltn.device)\n",
    "print('model initialization successfully')\n",
    "\n",
    "# Training Configuration\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.1)\n",
    "\n",
    "if load_checkpoint:\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(load_checkpoint_path)\n",
    "\n",
    "    # Load model and optimizer states\n",
    "    base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    # Load scheduler state, if available in the checkpoint\n",
    "    if 'scheduler' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "        # Retrieve the epoch information if available in the checkpoint\n",
    "        loaded_epoch = scheduler.last_epoch\n",
    "\n",
    "    print('load checkpoint successfully')\n",
    "\n",
    "else:\n",
    "    print('train from beginning')\n",
    "    loaded_epoch = 0  # If not loading a checkpoint, start training from epoch 0\n",
    "\n",
    "search_space = {\n",
    "    \"lr\": tune.grid_search([0.00001]),\n",
    "    \"beta\": tune.grid_search([0.75]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hwEYePAPSjuD",
    "outputId": "9218d1ec-458c-4046-f128-655203b50230"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-14 15:58:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:24:07.71        </td></tr>\n",
       "<tr><td>Memory:      </td><td>32.8/503.1 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A30)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  beta</th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>hyper_parameter_tune_fb41d_00000</td><td>RUNNING </td><td>10.139.126.233:446250</td><td style=\"text-align: right;\">  0.75</td><td style=\"text-align: right;\">1e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m load checkpoint successfully\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m model 0 ltn_combine binary 1e-05 0.75 2023-12-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:   0%|          | 0/247 [00:00<?, ?it/s]\n",
      "Epoch 20:   0%|          | 1/247 [00:03<14:16,  3.48s/it]\n",
      "Epoch 20:   1%|          | 2/247 [00:04<09:23,  2.30s/it]\n",
      "Epoch 20:   1%|          | 3/247 [00:06<07:45,  1.91s/it]\n",
      "Epoch 20:   2%|▏         | 4/247 [00:07<06:53,  1.70s/it]\n",
      "Epoch 20:   2%|▏         | 5/247 [00:09<06:27,  1.60s/it]\n",
      "Epoch 20:   2%|▏         | 6/247 [00:10<06:09,  1.54s/it]\n",
      "Epoch 20:   3%|▎         | 7/247 [00:12<06:00,  1.50s/it]\n",
      "Epoch 20:   3%|▎         | 8/247 [00:13<05:53,  1.48s/it]\n",
      "Epoch 20:   4%|▎         | 9/247 [00:15<05:59,  1.51s/it]\n",
      "Epoch 20:   4%|▍         | 10/247 [00:16<05:52,  1.49s/it]\n",
      "Epoch 20:   4%|▍         | 11/247 [00:17<05:49,  1.48s/it]\n",
      "Epoch 20:   5%|▍         | 12/247 [00:19<05:44,  1.47s/it]\n",
      "Epoch 20:   5%|▌         | 13/247 [00:20<05:42,  1.46s/it]\n",
      "Epoch 20:   6%|▌         | 14/247 [00:22<05:38,  1.45s/it]\n",
      "Epoch 20:   6%|▌         | 15/247 [00:23<05:33,  1.44s/it]\n",
      "Epoch 20:   6%|▋         | 16/247 [00:25<05:33,  1.45s/it]\n",
      "Epoch 20:   7%|▋         | 17/247 [00:26<05:31,  1.44s/it]\n",
      "Epoch 20:   7%|▋         | 18/247 [00:28<05:31,  1.45s/it]\n",
      "Epoch 20:   8%|▊         | 19/247 [00:29<05:30,  1.45s/it]\n",
      "Epoch 20:   8%|▊         | 20/247 [00:30<05:27,  1.44s/it]\n",
      "Epoch 20:   9%|▊         | 21/247 [00:32<05:28,  1.45s/it]\n",
      "Epoch 20:   9%|▉         | 22/247 [00:33<05:26,  1.45s/it]\n",
      "Epoch 20:   9%|▉         | 23/247 [00:35<05:23,  1.45s/it]\n",
      "Epoch 20:  10%|▉         | 24/247 [00:36<05:24,  1.46s/it]\n",
      "Epoch 20:  10%|█         | 25/247 [00:38<05:23,  1.46s/it]\n",
      "Epoch 20:  11%|█         | 26/247 [00:39<05:22,  1.46s/it]\n",
      "Epoch 20:  11%|█         | 27/247 [00:41<05:18,  1.45s/it]\n",
      "Epoch 20:  11%|█▏        | 28/247 [00:42<05:18,  1.45s/it]\n",
      "Epoch 20:  12%|█▏        | 29/247 [00:44<05:16,  1.45s/it]\n",
      "Epoch 20:  12%|█▏        | 30/247 [00:45<05:14,  1.45s/it]\n",
      "Epoch 20:  13%|█▎        | 31/247 [00:46<05:11,  1.44s/it]\n",
      "Epoch 20:  13%|█▎        | 32/247 [00:48<05:12,  1.46s/it]\n",
      "Epoch 20:  13%|█▎        | 33/247 [00:49<05:11,  1.46s/it]\n",
      "Epoch 20:  14%|█▍        | 34/247 [00:51<05:19,  1.50s/it]\n",
      "Epoch 20:  14%|█▍        | 35/247 [00:52<05:14,  1.49s/it]\n",
      "Epoch 20:  15%|█▍        | 36/247 [00:54<05:10,  1.47s/it]\n",
      "Epoch 20:  15%|█▍        | 37/247 [00:55<05:06,  1.46s/it]\n",
      "Epoch 20:  15%|█▌        | 38/247 [00:57<05:04,  1.46s/it]\n",
      "Epoch 20:  16%|█▌        | 39/247 [00:58<05:03,  1.46s/it]\n",
      "Epoch 20:  16%|█▌        | 40/247 [01:00<05:02,  1.46s/it]\n",
      "Epoch 20:  17%|█▋        | 41/247 [01:01<04:59,  1.45s/it]\n",
      "Epoch 20:  17%|█▋        | 42/247 [01:03<04:58,  1.45s/it]\n",
      "Epoch 20:  17%|█▋        | 43/247 [01:04<04:56,  1.45s/it]\n",
      "Epoch 20:  18%|█▊        | 44/247 [01:05<04:55,  1.46s/it]\n",
      "Epoch 20:  18%|█▊        | 45/247 [01:07<04:54,  1.46s/it]\n",
      "Epoch 20:  19%|█▊        | 46/247 [01:08<04:53,  1.46s/it]\n",
      "Epoch 20:  19%|█▉        | 47/247 [01:10<04:50,  1.45s/it]\n",
      "Epoch 20:  19%|█▉        | 48/247 [01:11<04:48,  1.45s/it]\n",
      "Epoch 20:  20%|█▉        | 49/247 [01:13<04:46,  1.45s/it]\n",
      "Epoch 20:  20%|██        | 50/247 [01:14<04:47,  1.46s/it]\n",
      "Epoch 20:  21%|██        | 51/247 [01:16<04:45,  1.46s/it]\n",
      "Epoch 20:  21%|██        | 52/247 [01:17<04:44,  1.46s/it]\n",
      "Epoch 20:  21%|██▏       | 53/247 [01:19<04:42,  1.46s/it]\n",
      "Epoch 20:  22%|██▏       | 54/247 [01:20<04:39,  1.45s/it]\n",
      "Epoch 20:  22%|██▏       | 55/247 [01:21<04:37,  1.45s/it]\n",
      "Epoch 20:  23%|██▎       | 56/247 [01:23<04:36,  1.45s/it]\n",
      "Epoch 20:  23%|██▎       | 57/247 [01:24<04:35,  1.45s/it]\n",
      "Epoch 20:  23%|██▎       | 58/247 [01:26<04:33,  1.45s/it]\n",
      "Epoch 20:  24%|██▍       | 59/247 [01:27<04:33,  1.46s/it]\n",
      "Epoch 20:  24%|██▍       | 60/247 [01:29<04:38,  1.49s/it]\n",
      "Epoch 20:  25%|██▍       | 61/247 [01:30<04:35,  1.48s/it]\n",
      "Epoch 20:  25%|██▌       | 62/247 [01:32<04:32,  1.47s/it]\n",
      "Epoch 20:  26%|██▌       | 63/247 [01:33<04:28,  1.46s/it]\n",
      "Epoch 20:  26%|██▌       | 64/247 [01:35<04:26,  1.45s/it]\n",
      "Epoch 20:  26%|██▋       | 65/247 [01:36<04:24,  1.45s/it]\n",
      "Epoch 20:  27%|██▋       | 66/247 [01:37<04:22,  1.45s/it]\n",
      "Epoch 20:  27%|██▋       | 67/247 [01:39<04:22,  1.46s/it]\n",
      "Epoch 20:  28%|██▊       | 68/247 [01:40<04:20,  1.45s/it]\n",
      "Epoch 20:  28%|██▊       | 69/247 [01:42<04:18,  1.45s/it]\n",
      "Epoch 20:  28%|██▊       | 70/247 [01:43<04:16,  1.45s/it]\n",
      "Epoch 20:  29%|██▊       | 71/247 [01:45<04:15,  1.45s/it]\n",
      "Epoch 20:  29%|██▉       | 72/247 [01:46<04:14,  1.45s/it]\n",
      "Epoch 20:  30%|██▉       | 73/247 [01:48<04:12,  1.45s/it]\n",
      "Epoch 20:  30%|██▉       | 74/247 [01:49<04:08,  1.44s/it]\n",
      "Epoch 20:  30%|███       | 75/247 [01:51<04:08,  1.44s/it]\n",
      "Epoch 20:  31%|███       | 76/247 [01:52<04:07,  1.45s/it]\n",
      "Epoch 20:  31%|███       | 77/247 [01:53<04:07,  1.45s/it]\n",
      "Epoch 20:  32%|███▏      | 78/247 [01:55<04:05,  1.46s/it]\n",
      "Epoch 20:  32%|███▏      | 79/247 [01:56<04:04,  1.46s/it]\n",
      "Epoch 20:  32%|███▏      | 80/247 [01:58<04:03,  1.46s/it]\n",
      "Epoch 20:  33%|███▎      | 81/247 [01:59<04:00,  1.45s/it]\n",
      "Epoch 20:  33%|███▎      | 82/247 [02:01<03:58,  1.44s/it]\n",
      "Epoch 20:  34%|███▎      | 83/247 [02:02<03:56,  1.44s/it]\n",
      "Epoch 20:  34%|███▍      | 84/247 [02:04<03:55,  1.45s/it]\n",
      "Epoch 20:  34%|███▍      | 85/247 [02:05<03:55,  1.45s/it]\n",
      "Epoch 20:  35%|███▍      | 86/247 [02:07<04:00,  1.49s/it]\n",
      "Epoch 20:  35%|███▌      | 87/247 [02:08<03:57,  1.48s/it]\n",
      "Epoch 20:  36%|███▌      | 88/247 [02:10<03:53,  1.47s/it]\n",
      "Epoch 20:  36%|███▌      | 89/247 [02:11<03:52,  1.47s/it]\n",
      "Epoch 20:  36%|███▋      | 90/247 [02:12<03:48,  1.46s/it]\n",
      "Epoch 20:  37%|███▋      | 91/247 [02:14<03:47,  1.46s/it]\n",
      "Epoch 20:  37%|███▋      | 92/247 [02:15<03:45,  1.46s/it]\n",
      "Epoch 20:  38%|███▊      | 93/247 [02:17<03:43,  1.45s/it]\n",
      "Epoch 20:  38%|███▊      | 94/247 [02:18<03:41,  1.45s/it]\n",
      "Epoch 20:  38%|███▊      | 95/247 [02:20<03:40,  1.45s/it]\n",
      "Epoch 20:  39%|███▉      | 96/247 [02:21<03:38,  1.45s/it]\n",
      "Epoch 20:  39%|███▉      | 97/247 [02:23<03:37,  1.45s/it]\n",
      "Epoch 20:  40%|███▉      | 98/247 [02:24<03:37,  1.46s/it]\n",
      "Epoch 20:  40%|████      | 99/247 [02:26<03:35,  1.46s/it]\n",
      "Epoch 20:  40%|████      | 100/247 [02:27<03:34,  1.46s/it]\n",
      "Epoch 20:  41%|████      | 101/247 [02:28<03:32,  1.46s/it]\n",
      "Epoch 20:  41%|████▏     | 102/247 [02:30<03:29,  1.44s/it]\n",
      "Epoch 20:  42%|████▏     | 103/247 [02:31<03:29,  1.45s/it]\n",
      "Epoch 20:  42%|████▏     | 104/247 [02:33<03:28,  1.46s/it]\n",
      "Epoch 20:  43%|████▎     | 105/247 [02:34<03:26,  1.46s/it]\n",
      "Epoch 20:  43%|████▎     | 106/247 [02:36<03:25,  1.46s/it]\n",
      "Epoch 20:  43%|████▎     | 107/247 [02:37<03:23,  1.45s/it]\n",
      "Epoch 20:  44%|████▎     | 108/247 [02:39<03:20,  1.44s/it]\n",
      "Epoch 20:  44%|████▍     | 109/247 [02:40<03:19,  1.45s/it]\n",
      "Epoch 20:  45%|████▍     | 110/247 [02:41<03:18,  1.45s/it]\n",
      "Epoch 20:  45%|████▍     | 111/247 [02:43<03:16,  1.45s/it]\n",
      "Epoch 20:  45%|████▌     | 112/247 [02:44<03:21,  1.49s/it]\n",
      "Epoch 20:  46%|████▌     | 113/247 [02:46<03:18,  1.48s/it]\n",
      "Epoch 20:  46%|████▌     | 114/247 [02:47<03:15,  1.47s/it]\n",
      "Epoch 20:  47%|████▋     | 115/247 [02:49<03:12,  1.46s/it]\n",
      "Epoch 20:  47%|████▋     | 116/247 [02:50<03:11,  1.46s/it]\n",
      "Epoch 20:  47%|████▋     | 117/247 [02:52<03:10,  1.46s/it]\n",
      "Epoch 20:  48%|████▊     | 118/247 [02:53<03:07,  1.46s/it]\n",
      "Epoch 20:  48%|████▊     | 119/247 [02:55<03:06,  1.46s/it]\n",
      "Epoch 20:  49%|████▊     | 120/247 [02:56<03:04,  1.45s/it]\n",
      "Epoch 20:  49%|████▉     | 121/247 [02:58<03:03,  1.46s/it]\n",
      "Epoch 20:  49%|████▉     | 122/247 [02:59<03:00,  1.44s/it]\n",
      "Epoch 20:  50%|████▉     | 123/247 [03:00<02:59,  1.45s/it]\n",
      "Epoch 20:  50%|█████     | 124/247 [03:02<02:57,  1.45s/it]\n",
      "Epoch 20:  51%|█████     | 125/247 [03:03<02:58,  1.46s/it]\n",
      "Epoch 20:  51%|█████     | 126/247 [03:05<02:56,  1.46s/it]\n",
      "Epoch 20:  51%|█████▏    | 127/247 [03:06<02:54,  1.46s/it]\n",
      "Epoch 20:  52%|█████▏    | 128/247 [03:08<02:57,  1.49s/it]\n",
      "Epoch 20:  52%|█████▏    | 129/247 [03:09<03:00,  1.53s/it]\n",
      "Epoch 20:  53%|█████▎    | 130/247 [03:11<02:59,  1.53s/it]\n",
      "Epoch 20:  53%|█████▎    | 131/247 [03:13<02:56,  1.52s/it]\n",
      "Epoch 20:  53%|█████▎    | 132/247 [03:14<02:56,  1.54s/it]\n",
      "Epoch 20:  54%|█████▍    | 133/247 [03:16<02:54,  1.53s/it]\n",
      "Epoch 20:  54%|█████▍    | 134/247 [03:17<02:53,  1.53s/it]\n",
      "Epoch 20:  55%|█████▍    | 135/247 [03:19<02:52,  1.54s/it]\n",
      "Epoch 20:  55%|█████▌    | 136/247 [03:20<02:50,  1.54s/it]\n",
      "Epoch 20:  55%|█████▌    | 137/247 [03:22<02:56,  1.61s/it]\n",
      "Epoch 20:  56%|█████▌    | 138/247 [03:24<02:52,  1.58s/it]\n",
      "Epoch 20:  56%|█████▋    | 139/247 [03:25<02:47,  1.56s/it]\n",
      "Epoch 20:  57%|█████▋    | 140/247 [03:27<02:44,  1.54s/it]\n",
      "Epoch 20:  57%|█████▋    | 141/247 [03:28<02:42,  1.53s/it]\n",
      "Epoch 20:  57%|█████▋    | 142/247 [03:30<02:40,  1.52s/it]\n",
      "Epoch 20:  58%|█████▊    | 143/247 [03:31<02:38,  1.53s/it]\n",
      "Epoch 20:  58%|█████▊    | 144/247 [03:33<02:36,  1.52s/it]\n",
      "Epoch 20:  59%|█████▊    | 145/247 [03:34<02:34,  1.51s/it]\n",
      "Epoch 20:  59%|█████▉    | 146/247 [03:36<02:32,  1.51s/it]\n",
      "Epoch 20:  60%|█████▉    | 147/247 [03:37<02:32,  1.52s/it]\n",
      "Epoch 20:  60%|█████▉    | 148/247 [03:39<02:29,  1.51s/it]\n",
      "Epoch 20:  60%|██████    | 149/247 [03:40<02:26,  1.50s/it]\n",
      "Epoch 20:  61%|██████    | 150/247 [03:42<02:26,  1.51s/it]\n",
      "Epoch 20:  61%|██████    | 151/247 [03:43<02:25,  1.52s/it]\n",
      "Epoch 20:  62%|██████▏   | 152/247 [03:45<02:24,  1.52s/it]\n",
      "Epoch 20:  62%|██████▏   | 153/247 [03:46<02:22,  1.52s/it]\n",
      "Epoch 20:  62%|██████▏   | 154/247 [03:48<02:22,  1.53s/it]\n",
      "Epoch 20:  63%|██████▎   | 155/247 [03:49<02:20,  1.52s/it]\n",
      "Epoch 20:  63%|██████▎   | 156/247 [03:51<02:16,  1.50s/it]\n",
      "Epoch 20:  64%|██████▎   | 157/247 [03:52<02:15,  1.51s/it]\n",
      "Epoch 20:  64%|██████▍   | 158/247 [03:54<02:12,  1.49s/it]\n",
      "Epoch 20:  64%|██████▍   | 159/247 [03:55<02:10,  1.48s/it]\n",
      "Epoch 20:  65%|██████▍   | 160/247 [03:57<02:08,  1.48s/it]\n",
      "Epoch 20:  65%|██████▌   | 161/247 [03:58<02:07,  1.48s/it]\n",
      "Epoch 20:  66%|██████▌   | 162/247 [04:00<02:11,  1.55s/it]\n",
      "Epoch 20:  66%|██████▌   | 163/247 [04:01<02:10,  1.55s/it]\n",
      "Epoch 20:  66%|██████▋   | 164/247 [04:03<02:08,  1.55s/it]\n",
      "Epoch 20:  67%|██████▋   | 165/247 [04:04<02:05,  1.54s/it]\n",
      "Epoch 20:  67%|██████▋   | 166/247 [04:06<02:03,  1.52s/it]\n",
      "Epoch 20:  68%|██████▊   | 167/247 [04:07<02:01,  1.52s/it]\n",
      "Epoch 20:  68%|██████▊   | 168/247 [04:09<02:00,  1.53s/it]\n",
      "Epoch 20:  68%|██████▊   | 169/247 [04:11<01:58,  1.52s/it]\n",
      "Epoch 20:  69%|██████▉   | 170/247 [04:12<01:57,  1.53s/it]\n",
      "Epoch 20:  69%|██████▉   | 171/247 [04:14<01:55,  1.52s/it]\n",
      "Epoch 20:  70%|██████▉   | 172/247 [04:15<01:53,  1.52s/it]\n",
      "Epoch 20:  70%|███████   | 173/247 [04:17<01:52,  1.52s/it]\n",
      "Epoch 20:  70%|███████   | 174/247 [04:18<01:49,  1.51s/it]\n",
      "Epoch 20:  71%|███████   | 175/247 [04:20<01:47,  1.50s/it]\n",
      "Epoch 20:  71%|███████▏  | 176/247 [04:21<01:46,  1.50s/it]\n",
      "Epoch 20:  72%|███████▏  | 177/247 [04:23<01:45,  1.51s/it]\n",
      "Epoch 20:  72%|███████▏  | 178/247 [04:24<01:44,  1.51s/it]\n",
      "Epoch 20:  72%|███████▏  | 179/247 [04:26<01:41,  1.49s/it]\n",
      "Epoch 20:  73%|███████▎  | 180/247 [04:27<01:39,  1.49s/it]\n",
      "Epoch 20:  73%|███████▎  | 181/247 [04:29<01:38,  1.50s/it]\n",
      "Epoch 20:  74%|███████▎  | 182/247 [04:30<01:37,  1.49s/it]\n",
      "Epoch 20:  74%|███████▍  | 183/247 [04:31<01:34,  1.48s/it]\n",
      "Epoch 20:  74%|███████▍  | 184/247 [04:33<01:33,  1.49s/it]\n",
      "Epoch 20:  75%|███████▍  | 185/247 [04:34<01:32,  1.50s/it]\n",
      "Epoch 20:  75%|███████▌  | 186/247 [04:36<01:31,  1.50s/it]\n",
      "Epoch 20:  76%|███████▌  | 187/247 [04:38<01:30,  1.51s/it]\n",
      "Epoch 20:  76%|███████▌  | 188/247 [04:39<01:29,  1.52s/it]\n",
      "Epoch 20:  77%|███████▋  | 189/247 [04:41<01:28,  1.52s/it]\n",
      "Epoch 20:  77%|███████▋  | 190/247 [04:42<01:26,  1.52s/it]\n",
      "Epoch 20:  77%|███████▋  | 191/247 [04:44<01:24,  1.51s/it]\n",
      "Epoch 20:  78%|███████▊  | 192/247 [04:45<01:27,  1.58s/it]\n",
      "Epoch 20:  78%|███████▊  | 193/247 [04:47<01:24,  1.56s/it]\n",
      "Epoch 20:  79%|███████▊  | 194/247 [04:48<01:21,  1.54s/it]\n",
      "Epoch 20:  79%|███████▉  | 195/247 [04:50<01:19,  1.53s/it]\n",
      "Epoch 20:  79%|███████▉  | 196/247 [04:51<01:17,  1.52s/it]\n",
      "Epoch 20:  80%|███████▉  | 197/247 [04:53<01:15,  1.52s/it]\n",
      "Epoch 20:  80%|████████  | 198/247 [04:54<01:13,  1.50s/it]\n",
      "Epoch 20:  81%|████████  | 199/247 [04:56<01:11,  1.49s/it]\n",
      "Epoch 20:  81%|████████  | 200/247 [04:57<01:09,  1.48s/it]\n",
      "Epoch 20:  81%|████████▏ | 201/247 [04:59<01:07,  1.48s/it]\n",
      "Epoch 20:  82%|████████▏ | 202/247 [05:00<01:06,  1.48s/it]\n",
      "Epoch 20:  82%|████████▏ | 203/247 [05:02<01:05,  1.50s/it]\n",
      "Epoch 20:  83%|████████▎ | 204/247 [05:03<01:05,  1.51s/it]\n",
      "Epoch 20:  83%|████████▎ | 205/247 [05:05<01:03,  1.52s/it]\n",
      "Epoch 20:  83%|████████▎ | 206/247 [05:06<01:02,  1.52s/it]\n",
      "Epoch 20:  84%|████████▍ | 207/247 [05:08<01:00,  1.52s/it]\n",
      "Epoch 20:  84%|████████▍ | 208/247 [05:09<00:58,  1.50s/it]\n",
      "Epoch 20:  85%|████████▍ | 209/247 [05:11<00:57,  1.52s/it]\n",
      "Epoch 20:  85%|████████▌ | 210/247 [05:12<00:55,  1.51s/it]\n",
      "Epoch 20:  85%|████████▌ | 211/247 [05:14<00:54,  1.51s/it]\n",
      "Epoch 20:  86%|████████▌ | 212/247 [05:15<00:52,  1.51s/it]\n",
      "Epoch 20:  86%|████████▌ | 213/247 [05:17<00:51,  1.53s/it]\n",
      "Epoch 20:  87%|████████▋ | 214/247 [05:19<00:50,  1.54s/it]\n",
      "Epoch 20:  87%|████████▋ | 215/247 [05:20<00:49,  1.54s/it]\n",
      "Epoch 20:  87%|████████▋ | 216/247 [05:22<00:47,  1.54s/it]\n",
      "Epoch 20:  88%|████████▊ | 217/247 [05:23<00:45,  1.53s/it]\n",
      "Epoch 20:  88%|████████▊ | 218/247 [05:25<00:46,  1.60s/it]\n",
      "Epoch 20:  89%|████████▊ | 219/247 [05:26<00:44,  1.58s/it]\n",
      "Epoch 20:  89%|████████▉ | 220/247 [05:28<00:42,  1.57s/it]\n",
      "Epoch 20:  89%|████████▉ | 221/247 [05:29<00:40,  1.56s/it]\n",
      "Epoch 20:  90%|████████▉ | 222/247 [05:31<00:38,  1.55s/it]\n",
      "Epoch 20:  90%|█████████ | 223/247 [05:33<00:37,  1.54s/it]\n",
      "Epoch 20:  91%|█████████ | 224/247 [05:34<00:35,  1.54s/it]\n",
      "Epoch 20:  91%|█████████ | 225/247 [05:36<00:33,  1.53s/it]\n",
      "Epoch 20:  91%|█████████▏| 226/247 [05:37<00:32,  1.53s/it]\n",
      "Epoch 20:  92%|█████████▏| 227/247 [05:39<00:30,  1.52s/it]\n",
      "Epoch 20:  92%|█████████▏| 228/247 [05:40<00:28,  1.52s/it]\n",
      "Epoch 20:  93%|█████████▎| 229/247 [05:42<00:27,  1.52s/it]\n",
      "Epoch 20:  93%|█████████▎| 230/247 [05:43<00:25,  1.53s/it]\n",
      "Epoch 20:  94%|█████████▎| 231/247 [05:45<00:24,  1.52s/it]\n",
      "Epoch 20:  94%|█████████▍| 232/247 [05:46<00:22,  1.53s/it]\n",
      "Epoch 20:  94%|█████████▍| 233/247 [05:48<00:21,  1.52s/it]\n",
      "Epoch 20:  95%|█████████▍| 234/247 [05:49<00:19,  1.52s/it]\n",
      "Epoch 20:  95%|█████████▌| 235/247 [05:51<00:18,  1.53s/it]\n",
      "Epoch 20:  96%|█████████▌| 236/247 [05:52<00:16,  1.54s/it]\n",
      "Epoch 20:  96%|█████████▌| 237/247 [05:54<00:15,  1.53s/it]\n",
      "Epoch 20:  96%|█████████▋| 238/247 [05:55<00:13,  1.53s/it]\n",
      "Epoch 20:  97%|█████████▋| 239/247 [05:57<00:12,  1.52s/it]\n",
      "Epoch 20:  97%|█████████▋| 240/247 [05:58<00:10,  1.52s/it]\n",
      "Epoch 20:  98%|█████████▊| 241/247 [06:00<00:09,  1.53s/it]\n",
      "Epoch 20:  98%|█████████▊| 242/247 [06:02<00:07,  1.53s/it]\n",
      "Epoch 20:  98%|█████████▊| 243/247 [06:03<00:06,  1.53s/it]\n",
      "Epoch 20:  99%|█████████▉| 244/247 [06:05<00:04,  1.54s/it]\n",
      "Epoch 20:  99%|█████████▉| 245/247 [06:06<00:03,  1.62s/it]\n",
      "Epoch 20: 100%|█████████▉| 246/247 [06:08<00:01,  1.59s/it]\n",
      "Epoch 20: 100%|██████████| 247/247 [06:09<00:00,  1.51s/it]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 20: 100%|██████████| 247/247 [06:12<00:00,  1.51s/it,  epoch 20 | loss 0.0749 | Train coarse acc 0.948 |Train coarse Prec 0.946 | Train coarse Rec 0.938 | Train fine acc 0.002 |Train fine Prec 0.002 | Train fine Rec 0.002]\n",
      "Evaluate test set:   0%|          | 0/51 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Evaluate test set:   2%|▏         | 1/51 [00:01<01:07,  1.34s/it]\n",
      "Evaluate test set:   4%|▍         | 2/51 [00:02<00:48,  1.01it/s]\n",
      "Evaluate test set:   6%|▌         | 3/51 [00:02<00:42,  1.13it/s]\n",
      "Evaluate test set:   8%|▊         | 4/51 [00:03<00:38,  1.21it/s]\n",
      "Evaluate test set:  10%|▉         | 5/51 [00:04<00:36,  1.25it/s]\n",
      "Evaluate test set:  12%|█▏        | 6/51 [00:05<00:35,  1.28it/s]\n",
      "Evaluate test set:  14%|█▎        | 7/51 [00:05<00:33,  1.30it/s]\n",
      "Evaluate test set:  16%|█▌        | 8/51 [00:06<00:32,  1.31it/s]\n",
      "Evaluate test set:  18%|█▊        | 9/51 [00:07<00:31,  1.32it/s]\n",
      "Evaluate test set:  20%|█▉        | 10/51 [00:08<00:31,  1.32it/s]\n",
      "Evaluate test set:  22%|██▏       | 11/51 [00:08<00:30,  1.32it/s]\n",
      "Evaluate test set:  24%|██▎       | 12/51 [00:09<00:29,  1.32it/s]\n",
      "Evaluate test set:  25%|██▌       | 13/51 [00:10<00:28,  1.32it/s]\n",
      "Evaluate test set:  27%|██▋       | 14/51 [00:11<00:28,  1.32it/s]\n",
      "Evaluate test set:  29%|██▉       | 15/51 [00:11<00:27,  1.32it/s]\n",
      "Evaluate test set:  31%|███▏      | 16/51 [00:12<00:26,  1.33it/s]\n",
      "Evaluate test set:  33%|███▎      | 17/51 [00:13<00:25,  1.33it/s]\n",
      "Evaluate test set:  35%|███▌      | 18/51 [00:14<00:24,  1.33it/s]\n",
      "Evaluate test set:  37%|███▋      | 19/51 [00:14<00:24,  1.33it/s]\n",
      "Evaluate test set:  39%|███▉      | 20/51 [00:15<00:23,  1.33it/s]\n",
      "Evaluate test set:  41%|████      | 21/51 [00:16<00:22,  1.33it/s]\n",
      "Evaluate test set:  43%|████▎     | 22/51 [00:17<00:21,  1.33it/s]\n",
      "Evaluate test set:  45%|████▌     | 23/51 [00:17<00:20,  1.34it/s]\n",
      "Evaluate test set:  47%|████▋     | 24/51 [00:18<00:20,  1.33it/s]\n",
      "Evaluate test set:  49%|████▉     | 25/51 [00:19<00:19,  1.33it/s]\n",
      "Evaluate test set:  51%|█████     | 26/51 [00:20<00:18,  1.33it/s]\n",
      "Evaluate test set:  53%|█████▎    | 27/51 [00:20<00:18,  1.32it/s]\n",
      "Evaluate test set:  55%|█████▍    | 28/51 [00:21<00:17,  1.32it/s]\n",
      "Evaluate test set:  57%|█████▋    | 29/51 [00:22<00:16,  1.32it/s]\n",
      "Evaluate test set:  59%|█████▉    | 30/51 [00:23<00:15,  1.33it/s]\n",
      "Evaluate test set:  61%|██████    | 31/51 [00:23<00:15,  1.33it/s]\n",
      "Evaluate test set:  63%|██████▎   | 32/51 [00:24<00:14,  1.33it/s]\n",
      "Evaluate test set:  65%|██████▍   | 33/51 [00:25<00:13,  1.33it/s]\n",
      "Evaluate test set:  67%|██████▋   | 34/51 [00:26<00:12,  1.33it/s]\n",
      "Evaluate test set:  69%|██████▊   | 35/51 [00:26<00:12,  1.32it/s]\n",
      "Evaluate test set:  71%|███████   | 36/51 [00:27<00:11,  1.32it/s]\n",
      "Evaluate test set:  73%|███████▎  | 37/51 [00:28<00:10,  1.32it/s]\n",
      "Evaluate test set:  75%|███████▍  | 38/51 [00:29<00:09,  1.31it/s]\n",
      "Evaluate test set:  76%|███████▋  | 39/51 [00:29<00:09,  1.31it/s]\n",
      "Evaluate test set:  78%|███████▊  | 40/51 [00:30<00:08,  1.31it/s]\n",
      "Evaluate test set:  80%|████████  | 41/51 [00:31<00:07,  1.31it/s]\n",
      "Evaluate test set:  82%|████████▏ | 42/51 [00:32<00:06,  1.30it/s]\n",
      "Evaluate test set:  84%|████████▍ | 43/51 [00:33<00:06,  1.30it/s]\n",
      "Evaluate test set:  86%|████████▋ | 44/51 [00:33<00:05,  1.31it/s]\n",
      "Evaluate test set:  88%|████████▊ | 45/51 [00:34<00:04,  1.31it/s]\n",
      "Evaluate test set:  90%|█████████ | 46/51 [00:35<00:03,  1.32it/s]\n",
      "Evaluate test set:  92%|█████████▏| 47/51 [00:36<00:03,  1.32it/s]\n",
      "Evaluate test set:  94%|█████████▍| 48/51 [00:36<00:02,  1.32it/s]\n",
      "Evaluate test set:  96%|█████████▌| 49/51 [00:37<00:01,  1.32it/s]\n",
      "Evaluate test set:  98%|█████████▊| 50/51 [00:38<00:00,  1.33it/s]\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:39<00:00,  1.34it/s]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:39<00:00,  1.29it/s,  loss 0.0787 | Train coarse acc 0.811 |Train coarse Prec 0.803 | Train coarse Rec 0.786 | Train fine acc 0.007 |Train fine Prec 0.006 | Train fine Rec 0.005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m Saved PyTorch Model State to model 0 ltn_combine binary 1e-05 0.75 2023-12-14\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m ####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21:   0%|          | 0/247 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Epoch 21:   0%|          | 1/247 [00:01<07:46,  1.89s/it]\n",
      "Epoch 21:   1%|          | 2/247 [00:03<06:50,  1.67s/it]\n",
      "Epoch 21:   1%|          | 3/247 [00:04<06:27,  1.59s/it]\n",
      "Epoch 21:   2%|▏         | 4/247 [00:06<06:16,  1.55s/it]\n",
      "Epoch 21:   2%|▏         | 5/247 [00:07<06:09,  1.53s/it]\n",
      "Epoch 21:   2%|▏         | 6/247 [00:09<06:02,  1.50s/it]\n",
      "Epoch 21:   3%|▎         | 7/247 [00:10<06:00,  1.50s/it]\n",
      "Epoch 21:   3%|▎         | 8/247 [00:12<05:57,  1.49s/it]\n",
      "Epoch 21:   4%|▎         | 9/247 [00:13<05:53,  1.48s/it]\n",
      "Epoch 21:   4%|▍         | 10/247 [00:15<05:52,  1.49s/it]\n",
      "Epoch 21:   4%|▍         | 11/247 [00:16<06:02,  1.54s/it]\n",
      "Epoch 21:   5%|▍         | 12/247 [00:18<05:56,  1.52s/it]\n",
      "Epoch 21:   5%|▌         | 13/247 [00:19<05:50,  1.50s/it]\n",
      "Epoch 21:   6%|▌         | 14/247 [00:21<05:48,  1.50s/it]\n",
      "Epoch 21:   6%|▌         | 15/247 [00:22<05:48,  1.50s/it]\n",
      "Epoch 21:   6%|▋         | 16/247 [00:24<05:45,  1.50s/it]\n",
      "Epoch 21:   7%|▋         | 17/247 [00:25<05:40,  1.48s/it]\n",
      "Epoch 21:   7%|▋         | 18/247 [00:27<05:41,  1.49s/it]\n",
      "Epoch 21:   8%|▊         | 19/247 [00:28<05:39,  1.49s/it]\n",
      "Epoch 21:   8%|▊         | 20/247 [00:30<05:38,  1.49s/it]\n",
      "Epoch 21:   9%|▊         | 21/247 [00:31<05:35,  1.49s/it]\n",
      "Epoch 21:   9%|▉         | 22/247 [00:33<05:32,  1.48s/it]\n",
      "Epoch 21:   9%|▉         | 23/247 [00:34<05:33,  1.49s/it]\n",
      "Epoch 21:  10%|▉         | 24/247 [00:36<05:29,  1.48s/it]\n",
      "Epoch 21:  10%|█         | 25/247 [00:37<05:29,  1.48s/it]\n",
      "Epoch 21:  11%|█         | 26/247 [00:39<05:23,  1.46s/it]\n",
      "Epoch 21:  11%|█         | 27/247 [00:40<05:22,  1.46s/it]\n",
      "Epoch 21:  11%|█▏        | 28/247 [00:41<05:19,  1.46s/it]\n",
      "Epoch 21:  12%|█▏        | 29/247 [00:43<05:19,  1.47s/it]\n",
      "Epoch 21:  12%|█▏        | 30/247 [00:44<05:17,  1.46s/it]\n",
      "Epoch 21:  13%|█▎        | 31/247 [00:46<05:17,  1.47s/it]\n",
      "Epoch 21:  13%|█▎        | 32/247 [00:47<05:17,  1.48s/it]\n",
      "Epoch 21:  13%|█▎        | 33/247 [00:49<05:14,  1.47s/it]\n",
      "Epoch 21:  14%|█▍        | 34/247 [00:50<05:13,  1.47s/it]\n",
      "Epoch 21:  14%|█▍        | 35/247 [00:52<05:10,  1.46s/it]\n",
      "Epoch 21:  15%|█▍        | 36/247 [00:53<05:17,  1.51s/it]\n",
      "Epoch 21:  15%|█▍        | 37/247 [00:55<05:13,  1.49s/it]\n",
      "Epoch 21:  15%|█▌        | 38/247 [00:56<05:11,  1.49s/it]\n",
      "Epoch 21:  16%|█▌        | 39/247 [00:58<05:10,  1.49s/it]\n",
      "Epoch 21:  16%|█▌        | 40/247 [00:59<05:10,  1.50s/it]\n",
      "Epoch 21:  17%|█▋        | 41/247 [01:01<05:07,  1.49s/it]\n",
      "Epoch 21:  17%|█▋        | 42/247 [01:02<05:08,  1.51s/it]\n",
      "Epoch 21:  17%|█▋        | 43/247 [01:04<05:02,  1.49s/it]\n",
      "Epoch 21:  18%|█▊        | 44/247 [01:05<05:02,  1.49s/it]\n",
      "Epoch 21:  18%|█▊        | 45/247 [01:07<05:01,  1.49s/it]\n",
      "Epoch 21:  19%|█▊        | 46/247 [01:08<05:00,  1.50s/it]\n",
      "Epoch 21:  19%|█▉        | 47/247 [01:10<04:56,  1.48s/it]\n",
      "Epoch 21:  19%|█▉        | 48/247 [01:11<04:54,  1.48s/it]\n",
      "Epoch 21:  20%|█▉        | 49/247 [01:13<04:53,  1.48s/it]\n",
      "Epoch 21:  20%|██        | 50/247 [01:14<04:52,  1.48s/it]\n",
      "Epoch 21:  21%|██        | 51/247 [01:16<04:49,  1.48s/it]\n",
      "Epoch 21:  21%|██        | 52/247 [01:17<04:46,  1.47s/it]\n",
      "Epoch 21:  21%|██▏       | 53/247 [01:19<04:46,  1.47s/it]\n",
      "Epoch 21:  22%|██▏       | 54/247 [01:20<04:44,  1.47s/it]\n",
      "Epoch 21:  22%|██▏       | 55/247 [01:22<04:42,  1.47s/it]\n",
      "Epoch 21:  23%|██▎       | 56/247 [01:23<04:41,  1.47s/it]\n",
      "Epoch 21:  23%|██▎       | 57/247 [01:24<04:39,  1.47s/it]\n",
      "Epoch 21:  23%|██▎       | 58/247 [01:26<04:39,  1.48s/it]\n",
      "Epoch 21:  24%|██▍       | 59/247 [01:27<04:37,  1.48s/it]\n",
      "Epoch 21:  24%|██▍       | 60/247 [01:29<04:38,  1.49s/it]\n",
      "Epoch 21:  25%|██▍       | 61/247 [01:30<04:37,  1.49s/it]\n",
      "Epoch 21:  25%|██▌       | 62/247 [01:32<04:38,  1.50s/it]\n",
      "Epoch 21:  26%|██▌       | 63/247 [01:33<04:35,  1.50s/it]\n",
      "Epoch 21:  26%|██▌       | 64/247 [01:35<04:42,  1.55s/it]\n",
      "Epoch 21:  26%|██▋       | 65/247 [01:37<04:37,  1.53s/it]\n",
      "Epoch 21:  27%|██▋       | 66/247 [01:38<04:34,  1.51s/it]\n",
      "Epoch 21:  27%|██▋       | 67/247 [01:40<04:30,  1.50s/it]\n",
      "Epoch 21:  28%|██▊       | 68/247 [01:41<04:27,  1.49s/it]\n",
      "Epoch 21:  28%|██▊       | 69/247 [01:43<04:24,  1.48s/it]\n",
      "Epoch 21:  28%|██▊       | 70/247 [01:44<04:21,  1.48s/it]\n",
      "Epoch 21:  29%|██▊       | 71/247 [01:45<04:20,  1.48s/it]\n",
      "Epoch 21:  29%|██▉       | 72/247 [01:47<04:21,  1.49s/it]\n",
      "Epoch 21:  30%|██▉       | 73/247 [01:48<04:18,  1.49s/it]\n",
      "Epoch 21:  30%|██▉       | 74/247 [01:50<04:17,  1.49s/it]\n",
      "Epoch 21:  30%|███       | 75/247 [01:51<04:15,  1.49s/it]\n",
      "Epoch 21:  31%|███       | 76/247 [01:53<04:17,  1.51s/it]\n",
      "Epoch 21:  31%|███       | 77/247 [01:54<04:15,  1.50s/it]\n",
      "Epoch 21:  32%|███▏      | 78/247 [01:56<04:14,  1.51s/it]\n",
      "Epoch 21:  32%|███▏      | 79/247 [01:57<04:09,  1.49s/it]\n",
      "Epoch 21:  32%|███▏      | 80/247 [01:59<04:08,  1.49s/it]\n",
      "Epoch 21:  33%|███▎      | 81/247 [02:00<04:09,  1.50s/it]\n",
      "Epoch 21:  33%|███▎      | 82/247 [02:02<04:05,  1.49s/it]\n",
      "Epoch 21:  34%|███▎      | 83/247 [02:03<04:02,  1.48s/it]\n",
      "Epoch 21:  34%|███▍      | 84/247 [02:05<03:59,  1.47s/it]\n",
      "Epoch 21:  34%|███▍      | 85/247 [02:06<03:59,  1.48s/it]\n",
      "Epoch 21:  35%|███▍      | 86/247 [02:08<03:58,  1.48s/it]\n",
      "Epoch 21:  35%|███▌      | 87/247 [02:09<03:55,  1.47s/it]\n",
      "Epoch 21:  36%|███▌      | 88/247 [02:11<03:54,  1.47s/it]\n",
      "Epoch 21:  36%|███▌      | 89/247 [02:12<04:01,  1.53s/it]\n",
      "Epoch 21:  36%|███▋      | 90/247 [02:14<04:01,  1.54s/it]\n",
      "Epoch 21:  37%|███▋      | 91/247 [02:15<03:56,  1.52s/it]\n",
      "Epoch 21:  37%|███▋      | 92/247 [02:17<03:53,  1.51s/it]\n",
      "Epoch 21:  38%|███▊      | 93/247 [02:18<03:51,  1.50s/it]\n",
      "Epoch 21:  38%|███▊      | 94/247 [02:20<03:51,  1.51s/it]\n",
      "Epoch 21:  38%|███▊      | 95/247 [02:21<03:49,  1.51s/it]\n",
      "Epoch 21:  39%|███▉      | 96/247 [02:23<03:47,  1.51s/it]\n",
      "Epoch 21:  39%|███▉      | 97/247 [02:24<03:46,  1.51s/it]\n",
      "Epoch 21:  40%|███▉      | 98/247 [02:26<03:43,  1.50s/it]\n",
      "Epoch 21:  40%|████      | 99/247 [02:27<03:41,  1.50s/it]\n",
      "Epoch 21:  40%|████      | 100/247 [02:29<03:39,  1.49s/it]\n",
      "Epoch 21:  41%|████      | 101/247 [02:30<03:36,  1.48s/it]\n",
      "Epoch 21:  41%|████▏     | 102/247 [02:32<03:34,  1.48s/it]\n",
      "Epoch 21:  42%|████▏     | 103/247 [02:33<03:32,  1.48s/it]\n",
      "Epoch 21:  42%|████▏     | 104/247 [02:35<03:31,  1.48s/it]\n",
      "Epoch 21:  43%|████▎     | 105/247 [02:36<03:31,  1.49s/it]\n",
      "Epoch 21:  43%|████▎     | 106/247 [02:38<03:30,  1.49s/it]\n",
      "Epoch 21:  43%|████▎     | 107/247 [02:39<03:29,  1.49s/it]\n",
      "Epoch 21:  44%|████▎     | 108/247 [02:41<03:27,  1.49s/it]\n",
      "Epoch 21:  44%|████▍     | 109/247 [02:42<03:24,  1.48s/it]\n",
      "Epoch 21:  45%|████▍     | 110/247 [02:44<03:23,  1.49s/it]\n",
      "Epoch 21:  45%|████▍     | 111/247 [02:45<03:21,  1.48s/it]\n",
      "Epoch 21:  45%|████▌     | 112/247 [02:47<03:20,  1.48s/it]\n",
      "Epoch 21:  46%|████▌     | 113/247 [02:48<03:19,  1.49s/it]\n",
      "Epoch 21:  46%|████▌     | 114/247 [02:50<03:25,  1.55s/it]\n",
      "Epoch 21:  47%|████▋     | 115/247 [02:51<03:20,  1.52s/it]\n",
      "Epoch 21:  47%|████▋     | 116/247 [02:53<03:28,  1.59s/it]\n",
      "Epoch 21:  47%|████▋     | 117/247 [02:55<03:21,  1.55s/it]\n",
      "Epoch 21:  48%|████▊     | 118/247 [02:56<03:18,  1.54s/it]\n",
      "Epoch 21:  48%|████▊     | 119/247 [02:58<03:15,  1.53s/it]\n",
      "Epoch 21:  49%|████▊     | 120/247 [02:59<03:12,  1.52s/it]\n",
      "Epoch 21:  49%|████▉     | 121/247 [03:01<03:10,  1.51s/it]\n",
      "Epoch 21:  49%|████▉     | 122/247 [03:02<03:07,  1.50s/it]\n",
      "Epoch 21:  50%|████▉     | 123/247 [03:04<03:06,  1.50s/it]\n",
      "Epoch 21:  50%|█████     | 124/247 [03:05<03:05,  1.51s/it]\n",
      "Epoch 21:  51%|█████     | 125/247 [03:07<03:03,  1.50s/it]\n",
      "Epoch 21:  51%|█████     | 126/247 [03:08<03:01,  1.50s/it]\n",
      "Epoch 21:  51%|█████▏    | 127/247 [03:10<02:59,  1.50s/it]\n",
      "Epoch 21:  52%|█████▏    | 128/247 [03:11<02:58,  1.50s/it]\n",
      "Epoch 21:  52%|█████▏    | 129/247 [03:13<02:55,  1.49s/it]\n",
      "Epoch 21:  53%|█████▎    | 130/247 [03:14<02:53,  1.48s/it]\n",
      "Epoch 21:  53%|█████▎    | 131/247 [03:15<02:51,  1.48s/it]\n",
      "Epoch 21:  53%|█████▎    | 132/247 [03:17<02:50,  1.49s/it]\n",
      "Epoch 21:  54%|█████▍    | 133/247 [03:18<02:48,  1.48s/it]\n",
      "Epoch 21:  54%|█████▍    | 134/247 [03:20<02:46,  1.48s/it]\n",
      "Epoch 21:  55%|█████▍    | 135/247 [03:21<02:45,  1.48s/it]\n",
      "Epoch 21:  55%|█████▌    | 136/247 [03:23<02:44,  1.48s/it]\n",
      "Epoch 21:  55%|█████▌    | 137/247 [03:24<02:43,  1.48s/it]\n",
      "Epoch 21:  56%|█████▌    | 138/247 [03:26<02:41,  1.48s/it]\n",
      "Epoch 21:  56%|█████▋    | 139/247 [03:27<02:40,  1.49s/it]\n",
      "Epoch 21:  57%|█████▋    | 140/247 [03:29<02:39,  1.49s/it]\n",
      "Epoch 21:  57%|█████▋    | 141/247 [03:30<02:38,  1.49s/it]\n",
      "Epoch 21:  57%|█████▋    | 142/247 [03:32<02:36,  1.50s/it]\n",
      "Epoch 21:  58%|█████▊    | 143/247 [03:33<02:35,  1.50s/it]\n",
      "Epoch 21:  58%|█████▊    | 144/247 [03:35<02:34,  1.50s/it]\n",
      "Epoch 21:  59%|█████▊    | 145/247 [03:36<02:36,  1.53s/it]\n",
      "Epoch 21:  59%|█████▉    | 146/247 [03:38<02:32,  1.51s/it]\n",
      "Epoch 21:  60%|█████▉    | 147/247 [03:39<02:29,  1.49s/it]\n",
      "Epoch 21:  60%|█████▉    | 148/247 [03:41<02:26,  1.48s/it]\n",
      "Epoch 21:  60%|██████    | 149/247 [03:42<02:24,  1.47s/it]\n",
      "Epoch 21:  61%|██████    | 150/247 [03:44<02:22,  1.47s/it]\n",
      "Epoch 21:  61%|██████    | 151/247 [03:45<02:22,  1.48s/it]\n",
      "Epoch 21:  62%|██████▏   | 152/247 [03:47<02:20,  1.48s/it]\n",
      "Epoch 21:  62%|██████▏   | 153/247 [03:48<02:18,  1.48s/it]\n",
      "Epoch 21:  62%|██████▏   | 154/247 [03:50<02:17,  1.47s/it]\n",
      "Epoch 21:  63%|██████▎   | 155/247 [03:51<02:15,  1.48s/it]\n",
      "Epoch 21:  63%|██████▎   | 156/247 [03:53<02:14,  1.48s/it]\n",
      "Epoch 21:  64%|██████▎   | 157/247 [03:54<02:12,  1.47s/it]\n",
      "Epoch 21:  64%|██████▍   | 158/247 [03:56<02:11,  1.48s/it]\n",
      "Epoch 21:  64%|██████▍   | 159/247 [03:57<02:10,  1.48s/it]\n",
      "Epoch 21:  65%|██████▍   | 160/247 [03:59<02:08,  1.48s/it]\n",
      "Epoch 21:  65%|██████▌   | 161/247 [04:00<02:06,  1.47s/it]\n",
      "Epoch 21:  66%|██████▌   | 162/247 [04:01<02:05,  1.47s/it]\n",
      "Epoch 21:  66%|██████▌   | 163/247 [04:03<02:03,  1.47s/it]\n",
      "Epoch 21:  66%|██████▋   | 164/247 [04:04<02:02,  1.47s/it]\n",
      "Epoch 21:  67%|██████▋   | 165/247 [04:06<02:00,  1.47s/it]\n",
      "Epoch 21:  67%|██████▋   | 166/247 [04:07<01:59,  1.48s/it]\n",
      "Epoch 21:  68%|██████▊   | 167/247 [04:09<01:58,  1.48s/it]\n",
      "Epoch 21:  68%|██████▊   | 168/247 [04:10<01:55,  1.46s/it]\n",
      "Epoch 21:  68%|██████▊   | 169/247 [04:12<01:54,  1.46s/it]\n",
      "Epoch 21:  69%|██████▉   | 170/247 [04:13<01:53,  1.47s/it]\n",
      "Epoch 21:  69%|██████▉   | 171/247 [04:15<01:56,  1.53s/it]\n",
      "Epoch 21:  70%|██████▉   | 172/247 [04:16<01:53,  1.52s/it]\n",
      "Epoch 21:  70%|███████   | 173/247 [04:18<01:51,  1.50s/it]\n",
      "Epoch 21:  70%|███████   | 174/247 [04:19<01:49,  1.50s/it]\n",
      "Epoch 21:  71%|███████   | 175/247 [04:21<01:48,  1.50s/it]\n",
      "Epoch 21:  71%|███████▏  | 176/247 [04:22<01:47,  1.51s/it]\n",
      "Epoch 21:  72%|███████▏  | 177/247 [04:24<01:45,  1.51s/it]\n",
      "Epoch 21:  72%|███████▏  | 178/247 [04:25<01:44,  1.51s/it]\n",
      "Epoch 21:  72%|███████▏  | 179/247 [04:27<01:42,  1.51s/it]\n",
      "Epoch 21:  73%|███████▎  | 180/247 [04:28<01:41,  1.52s/it]\n",
      "Epoch 21:  73%|███████▎  | 181/247 [04:30<01:39,  1.51s/it]\n",
      "Epoch 21:  74%|███████▎  | 182/247 [04:31<01:37,  1.51s/it]\n",
      "Epoch 21:  74%|███████▍  | 183/247 [04:33<01:36,  1.50s/it]\n",
      "Epoch 21:  74%|███████▍  | 184/247 [04:34<01:34,  1.49s/it]\n",
      "Epoch 21:  75%|███████▍  | 185/247 [04:36<01:31,  1.48s/it]\n",
      "Epoch 21:  75%|███████▌  | 186/247 [04:37<01:31,  1.49s/it]\n",
      "Epoch 21:  76%|███████▌  | 187/247 [04:39<01:30,  1.51s/it]\n",
      "Epoch 21:  76%|███████▌  | 188/247 [04:40<01:28,  1.51s/it]\n",
      "Epoch 21:  77%|███████▋  | 189/247 [04:42<01:26,  1.50s/it]\n",
      "Epoch 21:  77%|███████▋  | 190/247 [04:43<01:25,  1.50s/it]\n",
      "Epoch 21:  77%|███████▋  | 191/247 [04:45<01:24,  1.50s/it]\n",
      "Epoch 21:  78%|███████▊  | 192/247 [04:46<01:22,  1.49s/it]\n",
      "Epoch 21:  78%|███████▊  | 193/247 [04:48<01:20,  1.49s/it]\n",
      "Epoch 21:  79%|███████▊  | 194/247 [04:49<01:19,  1.49s/it]\n",
      "Epoch 21:  79%|███████▉  | 195/247 [04:51<01:17,  1.49s/it]\n",
      "Epoch 21:  79%|███████▉  | 196/247 [04:52<01:16,  1.49s/it]\n",
      "Epoch 21:  80%|███████▉  | 197/247 [04:54<01:17,  1.54s/it]\n",
      "Epoch 21:  80%|████████  | 198/247 [04:55<01:14,  1.52s/it]\n",
      "Epoch 21:  81%|████████  | 199/247 [04:57<01:12,  1.51s/it]\n",
      "Epoch 21:  81%|████████  | 200/247 [04:58<01:10,  1.49s/it]\n",
      "Epoch 21:  81%|████████▏ | 201/247 [05:00<01:08,  1.49s/it]\n",
      "Epoch 21:  82%|████████▏ | 202/247 [05:01<01:06,  1.48s/it]\n",
      "Epoch 21:  82%|████████▏ | 203/247 [05:03<01:05,  1.49s/it]\n",
      "Epoch 21:  83%|████████▎ | 204/247 [05:04<01:04,  1.49s/it]\n",
      "Epoch 21:  83%|████████▎ | 205/247 [05:06<01:02,  1.49s/it]\n",
      "Epoch 21:  83%|████████▎ | 206/247 [05:07<01:00,  1.49s/it]\n",
      "Epoch 21:  84%|████████▍ | 207/247 [05:09<00:59,  1.49s/it]\n",
      "Epoch 21:  84%|████████▍ | 208/247 [05:10<00:57,  1.47s/it]\n",
      "Epoch 21:  85%|████████▍ | 209/247 [05:12<00:55,  1.47s/it]\n",
      "Epoch 21:  85%|████████▌ | 210/247 [05:13<00:54,  1.48s/it]\n",
      "Epoch 21:  85%|████████▌ | 211/247 [05:15<00:53,  1.49s/it]\n",
      "Epoch 21:  86%|████████▌ | 212/247 [05:16<00:51,  1.48s/it]\n",
      "Epoch 21:  86%|████████▌ | 213/247 [05:18<00:50,  1.49s/it]\n",
      "Epoch 21:  87%|████████▋ | 214/247 [05:19<00:48,  1.47s/it]\n",
      "Epoch 21:  87%|████████▋ | 215/247 [05:21<00:46,  1.46s/it]\n",
      "Epoch 21:  87%|████████▋ | 216/247 [05:22<00:45,  1.45s/it]\n",
      "Epoch 21:  88%|████████▊ | 217/247 [05:24<00:44,  1.47s/it]\n",
      "Epoch 21:  88%|████████▊ | 218/247 [05:25<00:42,  1.48s/it]\n",
      "Epoch 21:  89%|████████▊ | 219/247 [05:26<00:41,  1.48s/it]\n",
      "Epoch 21:  89%|████████▉ | 220/247 [05:28<00:39,  1.48s/it]\n",
      "Epoch 21:  89%|████████▉ | 221/247 [05:29<00:38,  1.49s/it]\n",
      "Epoch 21:  90%|████████▉ | 222/247 [05:31<00:37,  1.48s/it]\n",
      "Epoch 21:  90%|█████████ | 223/247 [05:32<00:35,  1.49s/it]\n",
      "Epoch 21:  91%|█████████ | 224/247 [05:34<00:34,  1.48s/it]\n",
      "Epoch 21:  91%|█████████ | 225/247 [05:35<00:32,  1.49s/it]\n",
      "Epoch 21:  91%|█████████▏| 226/247 [05:37<00:31,  1.49s/it]\n",
      "Epoch 21:  92%|█████████▏| 227/247 [05:39<00:31,  1.56s/it]\n",
      "Epoch 21:  92%|█████████▏| 228/247 [05:40<00:29,  1.54s/it]\n",
      "Epoch 21:  93%|█████████▎| 229/247 [05:42<00:27,  1.50s/it]\n",
      "Epoch 21:  93%|█████████▎| 230/247 [05:43<00:25,  1.50s/it]\n",
      "Epoch 21:  94%|█████████▎| 231/247 [05:45<00:24,  1.51s/it]\n",
      "Epoch 21:  94%|█████████▍| 232/247 [05:46<00:22,  1.51s/it]\n",
      "Epoch 21:  94%|█████████▍| 233/247 [05:48<00:21,  1.51s/it]\n",
      "Epoch 21:  95%|█████████▍| 234/247 [05:49<00:19,  1.51s/it]\n",
      "Epoch 21:  95%|█████████▌| 235/247 [05:51<00:18,  1.51s/it]\n",
      "Epoch 21:  96%|█████████▌| 236/247 [05:52<00:16,  1.52s/it]\n",
      "Epoch 21:  96%|█████████▌| 237/247 [05:54<00:15,  1.53s/it]\n",
      "Epoch 21:  96%|█████████▋| 238/247 [05:55<00:13,  1.55s/it]\n",
      "Epoch 21:  97%|█████████▋| 239/247 [05:57<00:12,  1.53s/it]\n",
      "Epoch 21:  97%|█████████▋| 240/247 [05:58<00:10,  1.50s/it]\n",
      "Epoch 21:  98%|█████████▊| 241/247 [06:00<00:08,  1.50s/it]\n",
      "Epoch 21:  98%|█████████▊| 242/247 [06:01<00:07,  1.49s/it]\n",
      "Epoch 21:  98%|█████████▊| 243/247 [06:03<00:05,  1.49s/it]\n",
      "Epoch 21:  99%|█████████▉| 244/247 [06:04<00:04,  1.49s/it]\n",
      "Epoch 21:  99%|█████████▉| 245/247 [06:06<00:02,  1.48s/it]\n",
      "Epoch 21: 100%|█████████▉| 246/247 [06:07<00:01,  1.49s/it]\n",
      "Epoch 21: 100%|██████████| 247/247 [06:08<00:00,  1.41s/it]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 21: 100%|██████████| 247/247 [06:10<00:00,  1.50s/it,  epoch 21 | loss 0.0744 | Train coarse acc 0.944 |Train coarse Prec 0.940 | Train coarse Rec 0.932 | Train fine acc 0.002 |Train fine Prec 0.002 | Train fine Rec 0.002]\n",
      "Evaluate test set:   0%|          | 0/51 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Evaluate test set:   2%|▏         | 1/51 [00:01<00:58,  1.17s/it]\n",
      "Evaluate test set:   4%|▍         | 2/51 [00:01<00:44,  1.11it/s]\n",
      "Evaluate test set:   6%|▌         | 3/51 [00:02<00:39,  1.22it/s]\n",
      "Evaluate test set:   8%|▊         | 4/51 [00:03<00:36,  1.28it/s]\n",
      "Evaluate test set:  10%|▉         | 5/51 [00:04<00:34,  1.32it/s]\n",
      "Evaluate test set:  12%|█▏        | 6/51 [00:04<00:35,  1.27it/s]\n",
      "Evaluate test set:  14%|█▎        | 7/51 [00:05<00:33,  1.30it/s]\n",
      "Evaluate test set:  16%|█▌        | 8/51 [00:06<00:32,  1.33it/s]\n",
      "Evaluate test set:  18%|█▊        | 9/51 [00:07<00:31,  1.34it/s]\n",
      "Evaluate test set:  20%|█▉        | 10/51 [00:07<00:30,  1.34it/s]\n",
      "Evaluate test set:  22%|██▏       | 11/51 [00:08<00:29,  1.34it/s]\n",
      "Evaluate test set:  24%|██▎       | 12/51 [00:09<00:28,  1.35it/s]\n",
      "Evaluate test set:  25%|██▌       | 13/51 [00:10<00:28,  1.35it/s]\n",
      "Evaluate test set:  27%|██▋       | 14/51 [00:10<00:27,  1.36it/s]\n",
      "Evaluate test set:  29%|██▉       | 15/51 [00:11<00:26,  1.36it/s]\n",
      "Evaluate test set:  31%|███▏      | 16/51 [00:12<00:25,  1.37it/s]\n",
      "Evaluate test set:  33%|███▎      | 17/51 [00:12<00:25,  1.36it/s]\n",
      "Evaluate test set:  35%|███▌      | 18/51 [00:13<00:24,  1.37it/s]\n",
      "Evaluate test set:  37%|███▋      | 19/51 [00:14<00:23,  1.37it/s]\n",
      "Evaluate test set:  39%|███▉      | 20/51 [00:15<00:22,  1.36it/s]\n",
      "Evaluate test set:  41%|████      | 21/51 [00:15<00:22,  1.35it/s]\n",
      "Evaluate test set:  43%|████▎     | 22/51 [00:16<00:21,  1.35it/s]\n",
      "Evaluate test set:  45%|████▌     | 23/51 [00:17<00:20,  1.35it/s]\n",
      "Evaluate test set:  47%|████▋     | 24/51 [00:18<00:19,  1.37it/s]\n",
      "Evaluate test set:  49%|████▉     | 25/51 [00:18<00:18,  1.38it/s]\n",
      "Evaluate test set:  51%|█████     | 26/51 [00:19<00:18,  1.37it/s]\n",
      "Evaluate test set:  53%|█████▎    | 27/51 [00:20<00:17,  1.37it/s]\n",
      "Evaluate test set:  55%|█████▍    | 28/51 [00:21<00:16,  1.37it/s]\n",
      "Evaluate test set:  57%|█████▋    | 29/51 [00:21<00:15,  1.38it/s]\n",
      "Evaluate test set:  59%|█████▉    | 30/51 [00:22<00:15,  1.38it/s]\n",
      "Evaluate test set:  61%|██████    | 31/51 [00:23<00:14,  1.39it/s]\n",
      "Evaluate test set:  63%|██████▎   | 32/51 [00:23<00:13,  1.39it/s]\n",
      "Evaluate test set:  65%|██████▍   | 33/51 [00:24<00:12,  1.39it/s]\n",
      "Evaluate test set:  67%|██████▋   | 34/51 [00:25<00:12,  1.40it/s]\n",
      "Evaluate test set:  69%|██████▊   | 35/51 [00:26<00:11,  1.40it/s]\n",
      "Evaluate test set:  71%|███████   | 36/51 [00:26<00:10,  1.40it/s]\n",
      "Evaluate test set:  73%|███████▎  | 37/51 [00:27<00:09,  1.40it/s]\n",
      "Evaluate test set:  75%|███████▍  | 38/51 [00:28<00:09,  1.40it/s]\n",
      "Evaluate test set:  76%|███████▋  | 39/51 [00:28<00:08,  1.40it/s]\n",
      "Evaluate test set:  78%|███████▊  | 40/51 [00:29<00:07,  1.40it/s]\n",
      "Evaluate test set:  80%|████████  | 41/51 [00:30<00:07,  1.40it/s]\n",
      "Evaluate test set:  82%|████████▏ | 42/51 [00:30<00:06,  1.40it/s]\n",
      "Evaluate test set:  84%|████████▍ | 43/51 [00:31<00:05,  1.40it/s]\n",
      "Evaluate test set:  86%|████████▋ | 44/51 [00:32<00:04,  1.40it/s]\n",
      "Evaluate test set:  88%|████████▊ | 45/51 [00:33<00:04,  1.40it/s]\n",
      "Evaluate test set:  90%|█████████ | 46/51 [00:33<00:03,  1.40it/s]\n",
      "Evaluate test set:  92%|█████████▏| 47/51 [00:34<00:02,  1.40it/s]\n",
      "Evaluate test set:  94%|█████████▍| 48/51 [00:35<00:02,  1.40it/s]\n",
      "Evaluate test set:  96%|█████████▌| 49/51 [00:35<00:01,  1.40it/s]\n",
      "Evaluate test set:  98%|█████████▊| 50/51 [00:36<00:00,  1.40it/s]\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:37<00:00,  1.43it/s]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m ####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:37<00:00,  1.35it/s,  loss 0.0768 | Train coarse acc 0.816 |Train coarse Prec 0.805 | Train coarse Rec 0.793 | Train fine acc 0.005 |Train fine Prec 0.004 | Train fine Rec 0.004]\n",
      "Epoch 22:   0%|          | 0/247 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Epoch 22:   0%|          | 1/247 [00:01<07:28,  1.82s/it]\n",
      "Epoch 22:   1%|          | 2/247 [00:03<06:31,  1.60s/it]\n",
      "Epoch 22:   1%|          | 3/247 [00:04<06:14,  1.53s/it]\n",
      "Epoch 22:   2%|▏         | 4/247 [00:06<06:03,  1.50s/it]\n",
      "Epoch 22:   2%|▏         | 5/247 [00:07<05:57,  1.48s/it]\n",
      "Epoch 22:   2%|▏         | 6/247 [00:09<05:52,  1.46s/it]\n",
      "Epoch 22:   3%|▎         | 7/247 [00:10<05:50,  1.46s/it]\n",
      "Epoch 22:   3%|▎         | 8/247 [00:11<05:47,  1.45s/it]\n",
      "Epoch 22:   4%|▎         | 9/247 [00:13<05:45,  1.45s/it]\n",
      "Epoch 22:   4%|▍         | 10/247 [00:14<05:43,  1.45s/it]\n",
      "Epoch 22:   4%|▍         | 11/247 [00:16<05:43,  1.46s/it]\n",
      "Epoch 22:   5%|▍         | 12/247 [00:17<05:40,  1.45s/it]\n",
      "Epoch 22:   5%|▌         | 13/247 [00:19<05:40,  1.46s/it]\n",
      "Epoch 22:   6%|▌         | 14/247 [00:20<05:39,  1.46s/it]\n",
      "Epoch 22:   6%|▌         | 15/247 [00:22<05:42,  1.48s/it]\n",
      "Epoch 22:   6%|▋         | 16/247 [00:23<05:48,  1.51s/it]\n",
      "Epoch 22:   7%|▋         | 17/247 [00:25<05:58,  1.56s/it]\n",
      "Epoch 22:   7%|▋         | 18/247 [00:27<05:57,  1.56s/it]\n",
      "Epoch 22:   8%|▊         | 19/247 [00:28<05:51,  1.54s/it]\n",
      "Epoch 22:   8%|▊         | 20/247 [00:30<05:50,  1.54s/it]\n",
      "Epoch 22:   9%|▊         | 21/247 [00:31<05:45,  1.53s/it]\n",
      "Epoch 22:   9%|▉         | 22/247 [00:33<05:42,  1.52s/it]\n",
      "Epoch 22:   9%|▉         | 23/247 [00:34<05:40,  1.52s/it]\n",
      "Epoch 22:  10%|▉         | 24/247 [00:36<05:38,  1.52s/it]\n",
      "Epoch 22:  10%|█         | 25/247 [00:37<05:37,  1.52s/it]\n",
      "Epoch 22:  11%|█         | 26/247 [00:39<05:35,  1.52s/it]\n",
      "Epoch 22:  11%|█         | 27/247 [00:40<05:32,  1.51s/it]\n",
      "Epoch 22:  11%|█▏        | 28/247 [00:42<05:31,  1.52s/it]\n",
      "Epoch 22:  12%|█▏        | 29/247 [00:43<05:29,  1.51s/it]\n",
      "Epoch 22:  12%|█▏        | 30/247 [00:45<05:30,  1.52s/it]\n",
      "Epoch 22:  13%|█▎        | 31/247 [00:46<05:26,  1.51s/it]\n",
      "Epoch 22:  13%|█▎        | 32/247 [00:48<05:28,  1.53s/it]\n",
      "Epoch 22:  13%|█▎        | 33/247 [00:49<05:24,  1.52s/it]\n",
      "Epoch 22:  14%|█▍        | 34/247 [00:51<05:23,  1.52s/it]\n",
      "Epoch 22:  14%|█▍        | 35/247 [00:52<05:21,  1.52s/it]\n",
      "Epoch 22:  15%|█▍        | 36/247 [00:54<05:20,  1.52s/it]\n",
      "Epoch 22:  15%|█▍        | 37/247 [00:55<05:16,  1.51s/it]\n",
      "Epoch 22:  15%|█▌        | 38/247 [00:57<05:16,  1.51s/it]\n",
      "Epoch 22:  16%|█▌        | 39/247 [00:58<05:12,  1.50s/it]\n",
      "Epoch 22:  16%|█▌        | 40/247 [01:00<05:12,  1.51s/it]\n",
      "Epoch 22:  17%|█▋        | 41/247 [01:01<05:10,  1.51s/it]\n",
      "Epoch 22:  17%|█▋        | 42/247 [01:03<05:18,  1.55s/it]\n",
      "Epoch 22:  17%|█▋        | 43/247 [01:05<05:16,  1.55s/it]\n",
      "Epoch 22:  18%|█▊        | 44/247 [01:06<05:13,  1.54s/it]\n",
      "Epoch 22:  18%|█▊        | 45/247 [01:08<05:07,  1.52s/it]\n",
      "Epoch 22:  19%|█▊        | 46/247 [01:09<05:05,  1.52s/it]\n",
      "Epoch 22:  19%|█▉        | 47/247 [01:11<05:02,  1.51s/it]\n",
      "Epoch 22:  19%|█▉        | 48/247 [01:12<05:04,  1.53s/it]\n",
      "Epoch 22:  20%|█▉        | 49/247 [01:14<05:02,  1.53s/it]\n",
      "Epoch 22:  20%|██        | 50/247 [01:15<05:01,  1.53s/it]\n",
      "Epoch 22:  21%|██        | 51/247 [01:17<04:57,  1.52s/it]\n",
      "Epoch 22:  21%|██        | 52/247 [01:18<04:56,  1.52s/it]\n",
      "Epoch 22:  21%|██▏       | 53/247 [01:20<04:52,  1.51s/it]\n",
      "Epoch 22:  22%|██▏       | 54/247 [01:21<04:53,  1.52s/it]\n",
      "Epoch 22:  22%|██▏       | 55/247 [01:23<04:52,  1.53s/it]\n",
      "Epoch 22:  23%|██▎       | 56/247 [01:24<04:51,  1.53s/it]\n",
      "Epoch 22:  23%|██▎       | 57/247 [01:26<04:49,  1.52s/it]\n",
      "Epoch 22:  23%|██▎       | 58/247 [01:27<04:51,  1.54s/it]\n",
      "Epoch 22:  24%|██▍       | 59/247 [01:29<04:48,  1.53s/it]\n",
      "Epoch 22:  24%|██▍       | 60/247 [01:30<04:45,  1.53s/it]\n",
      "Epoch 22:  25%|██▍       | 61/247 [01:32<04:44,  1.53s/it]\n",
      "Epoch 22:  25%|██▌       | 62/247 [01:33<04:43,  1.53s/it]\n",
      "Epoch 22:  26%|██▌       | 63/247 [01:35<04:42,  1.54s/it]\n",
      "Epoch 22:  26%|██▌       | 64/247 [01:37<04:39,  1.53s/it]\n",
      "Epoch 22:  26%|██▋       | 65/247 [01:38<04:35,  1.52s/it]\n",
      "Epoch 22:  27%|██▋       | 66/247 [01:40<04:36,  1.53s/it]\n",
      "Epoch 22:  27%|██▋       | 67/247 [01:41<04:35,  1.53s/it]\n",
      "Epoch 22:  28%|██▊       | 68/247 [01:43<04:44,  1.59s/it]\n",
      "Epoch 22:  28%|██▊       | 69/247 [01:44<04:41,  1.58s/it]\n",
      "Epoch 22:  28%|██▊       | 70/247 [01:46<04:37,  1.57s/it]\n",
      "Epoch 22:  29%|██▊       | 71/247 [01:47<04:34,  1.56s/it]\n",
      "Epoch 22:  29%|██▉       | 72/247 [01:49<04:31,  1.55s/it]\n",
      "Epoch 22:  30%|██▉       | 73/247 [01:51<04:30,  1.56s/it]\n",
      "Epoch 22:  30%|██▉       | 74/247 [01:52<04:27,  1.54s/it]\n",
      "Epoch 22:  30%|███       | 75/247 [01:54<04:25,  1.54s/it]\n",
      "Epoch 22:  31%|███       | 76/247 [01:55<04:23,  1.54s/it]\n",
      "Epoch 22:  31%|███       | 77/247 [01:57<04:20,  1.53s/it]\n",
      "Epoch 22:  32%|███▏      | 78/247 [01:58<04:19,  1.54s/it]\n",
      "Epoch 22:  32%|███▏      | 79/247 [02:00<04:15,  1.52s/it]\n",
      "Epoch 22:  32%|███▏      | 80/247 [02:01<04:14,  1.52s/it]\n",
      "Epoch 22:  33%|███▎      | 81/247 [02:03<04:13,  1.53s/it]\n",
      "Epoch 22:  33%|███▎      | 82/247 [02:04<04:12,  1.53s/it]\n",
      "Epoch 22:  34%|███▎      | 83/247 [02:06<04:09,  1.52s/it]\n",
      "Epoch 22:  34%|███▍      | 84/247 [02:07<04:09,  1.53s/it]\n",
      "Epoch 22:  34%|███▍      | 85/247 [02:09<04:06,  1.52s/it]\n",
      "Epoch 22:  35%|███▍      | 86/247 [02:10<04:06,  1.53s/it]\n",
      "Epoch 22:  35%|███▌      | 87/247 [02:12<04:04,  1.53s/it]\n",
      "Epoch 22:  36%|███▌      | 88/247 [02:13<04:03,  1.53s/it]\n",
      "Epoch 22:  36%|███▌      | 89/247 [02:15<03:59,  1.52s/it]\n",
      "Epoch 22:  36%|███▋      | 90/247 [02:17<03:59,  1.53s/it]\n",
      "Epoch 22:  37%|███▋      | 91/247 [02:18<03:57,  1.52s/it]\n",
      "Epoch 22:  37%|███▋      | 92/247 [02:20<03:55,  1.52s/it]\n",
      "Epoch 22:  38%|███▊      | 93/247 [02:21<03:52,  1.51s/it]\n",
      "Epoch 22:  38%|███▊      | 94/247 [02:23<03:58,  1.56s/it]\n",
      "Epoch 22:  38%|███▊      | 95/247 [02:24<03:53,  1.54s/it]\n",
      "Epoch 22:  39%|███▉      | 96/247 [02:26<03:51,  1.53s/it]\n",
      "Epoch 22:  39%|███▉      | 97/247 [02:27<03:48,  1.53s/it]\n",
      "Epoch 22:  40%|███▉      | 98/247 [02:29<03:47,  1.52s/it]\n",
      "Epoch 22:  40%|████      | 99/247 [02:30<03:43,  1.51s/it]\n",
      "Epoch 22:  40%|████      | 100/247 [02:32<03:41,  1.50s/it]\n",
      "Epoch 22:  41%|████      | 101/247 [02:33<03:38,  1.49s/it]\n",
      "Epoch 22:  41%|████▏     | 102/247 [02:35<03:35,  1.49s/it]\n",
      "Epoch 22:  42%|████▏     | 103/247 [02:36<03:32,  1.48s/it]\n",
      "Epoch 22:  42%|████▏     | 104/247 [02:38<03:32,  1.49s/it]\n",
      "Epoch 22:  43%|████▎     | 105/247 [02:39<03:32,  1.50s/it]\n",
      "Epoch 22:  43%|████▎     | 106/247 [02:41<03:32,  1.50s/it]\n",
      "Epoch 22:  43%|████▎     | 107/247 [02:42<03:32,  1.51s/it]\n",
      "Epoch 22:  44%|████▎     | 108/247 [02:44<03:31,  1.52s/it]\n",
      "Epoch 22:  44%|████▍     | 109/247 [02:45<03:30,  1.53s/it]\n",
      "Epoch 22:  45%|████▍     | 110/247 [02:47<03:28,  1.52s/it]\n",
      "Epoch 22:  45%|████▍     | 111/247 [02:48<03:26,  1.52s/it]\n",
      "Epoch 22:  45%|████▌     | 112/247 [02:50<03:25,  1.52s/it]\n",
      "Epoch 22:  46%|████▌     | 113/247 [02:51<03:23,  1.52s/it]\n",
      "Epoch 22:  46%|████▌     | 114/247 [02:53<03:23,  1.53s/it]\n",
      "Epoch 22:  47%|████▋     | 115/247 [02:54<03:20,  1.52s/it]\n",
      "Epoch 22:  47%|████▋     | 116/247 [02:56<03:20,  1.53s/it]\n",
      "Epoch 22:  47%|████▋     | 117/247 [02:57<03:17,  1.52s/it]\n",
      "Epoch 22:  48%|████▊     | 118/247 [02:59<03:16,  1.52s/it]\n",
      "Epoch 22:  48%|████▊     | 119/247 [03:00<03:14,  1.52s/it]\n",
      "Epoch 22:  49%|████▊     | 120/247 [03:02<03:20,  1.58s/it]\n",
      "Epoch 22:  49%|████▉     | 121/247 [03:04<03:16,  1.56s/it]\n",
      "Epoch 22:  49%|████▉     | 122/247 [03:05<03:14,  1.56s/it]\n",
      "Epoch 22:  50%|████▉     | 123/247 [03:07<03:11,  1.55s/it]\n",
      "Epoch 22:  50%|█████     | 124/247 [03:08<03:08,  1.54s/it]\n",
      "Epoch 22:  51%|█████     | 125/247 [03:10<03:06,  1.53s/it]\n",
      "Epoch 22:  51%|█████     | 126/247 [03:11<03:05,  1.53s/it]\n",
      "Epoch 22:  51%|█████▏    | 127/247 [03:13<03:04,  1.53s/it]\n",
      "Epoch 22:  52%|█████▏    | 128/247 [03:14<03:00,  1.52s/it]\n",
      "Epoch 22:  52%|█████▏    | 129/247 [03:16<02:58,  1.51s/it]\n",
      "Epoch 22:  53%|█████▎    | 130/247 [03:17<02:58,  1.53s/it]\n",
      "Epoch 22:  53%|█████▎    | 131/247 [03:19<02:58,  1.53s/it]\n",
      "Epoch 22:  53%|█████▎    | 132/247 [03:21<02:56,  1.54s/it]\n",
      "Epoch 22:  54%|█████▍    | 133/247 [03:22<02:53,  1.52s/it]\n",
      "Epoch 22:  54%|█████▍    | 134/247 [03:24<02:51,  1.52s/it]\n",
      "Epoch 22:  55%|█████▍    | 135/247 [03:25<02:49,  1.52s/it]\n",
      "Epoch 22:  55%|█████▌    | 136/247 [03:27<02:49,  1.52s/it]\n",
      "Epoch 22:  55%|█████▌    | 137/247 [03:28<02:48,  1.53s/it]\n",
      "Epoch 22:  56%|█████▌    | 138/247 [03:30<02:45,  1.52s/it]\n",
      "Epoch 22:  56%|█████▋    | 139/247 [03:31<02:44,  1.52s/it]\n",
      "Epoch 22:  57%|█████▋    | 140/247 [03:33<02:42,  1.52s/it]\n",
      "Epoch 22:  57%|█████▋    | 141/247 [03:34<02:39,  1.51s/it]\n",
      "Epoch 22:  57%|█████▋    | 142/247 [03:36<02:38,  1.51s/it]\n",
      "Epoch 22:  58%|█████▊    | 143/247 [03:37<02:35,  1.50s/it]\n",
      "Epoch 22:  58%|█████▊    | 144/247 [03:39<02:34,  1.50s/it]\n",
      "Epoch 22:  59%|█████▊    | 145/247 [03:40<02:35,  1.53s/it]\n",
      "Epoch 22:  59%|█████▉    | 146/247 [03:42<02:33,  1.52s/it]\n",
      "Epoch 22:  60%|█████▉    | 147/247 [03:43<02:31,  1.52s/it]\n",
      "Epoch 22:  60%|█████▉    | 148/247 [03:45<02:30,  1.52s/it]\n",
      "Epoch 22:  60%|██████    | 149/247 [03:46<02:27,  1.51s/it]\n",
      "Epoch 22:  61%|██████    | 150/247 [03:48<02:25,  1.50s/it]\n",
      "Epoch 22:  61%|██████    | 151/247 [03:49<02:23,  1.50s/it]\n",
      "Epoch 22:  62%|██████▏   | 152/247 [03:51<02:22,  1.50s/it]\n",
      "Epoch 22:  62%|██████▏   | 153/247 [03:52<02:21,  1.50s/it]\n",
      "Epoch 22:  62%|██████▏   | 154/247 [03:54<02:18,  1.49s/it]\n",
      "Epoch 22:  63%|██████▎   | 155/247 [03:55<02:17,  1.49s/it]\n",
      "Epoch 22:  63%|██████▎   | 156/247 [03:57<02:16,  1.49s/it]\n",
      "Epoch 22:  64%|██████▎   | 157/247 [03:58<02:14,  1.49s/it]\n",
      "Epoch 22:  64%|██████▍   | 158/247 [04:00<02:12,  1.49s/it]\n",
      "Epoch 22:  64%|██████▍   | 159/247 [04:01<02:10,  1.49s/it]\n",
      "Epoch 22:  65%|██████▍   | 160/247 [04:03<02:09,  1.49s/it]\n",
      "Epoch 22:  65%|██████▌   | 161/247 [04:04<02:08,  1.50s/it]\n",
      "Epoch 22:  66%|██████▌   | 162/247 [04:06<02:06,  1.49s/it]\n",
      "Epoch 22:  66%|██████▌   | 163/247 [04:07<02:05,  1.49s/it]\n",
      "Epoch 22:  66%|██████▋   | 164/247 [04:09<02:03,  1.48s/it]\n",
      "Epoch 22:  67%|██████▋   | 165/247 [04:10<02:01,  1.49s/it]\n",
      "Epoch 22:  67%|██████▋   | 166/247 [04:12<02:00,  1.49s/it]\n",
      "Epoch 22:  68%|██████▊   | 167/247 [04:13<01:58,  1.49s/it]\n",
      "Epoch 22:  68%|██████▊   | 168/247 [04:15<01:57,  1.49s/it]\n",
      "Epoch 22:  68%|██████▊   | 169/247 [04:16<01:56,  1.50s/it]\n",
      "Epoch 22:  69%|██████▉   | 170/247 [04:18<01:59,  1.56s/it]\n",
      "Epoch 22:  69%|██████▉   | 171/247 [04:19<01:57,  1.54s/it]\n",
      "Epoch 22:  70%|██████▉   | 172/247 [04:21<01:54,  1.53s/it]\n",
      "Epoch 22:  70%|███████   | 173/247 [04:22<01:52,  1.52s/it]\n",
      "Epoch 22:  70%|███████   | 174/247 [04:24<01:51,  1.53s/it]\n",
      "Epoch 22:  71%|███████   | 175/247 [04:25<01:48,  1.51s/it]\n",
      "Epoch 22:  71%|███████▏  | 176/247 [04:27<01:47,  1.51s/it]\n",
      "Epoch 22:  72%|███████▏  | 177/247 [04:28<01:45,  1.51s/it]\n",
      "Epoch 22:  72%|███████▏  | 178/247 [04:30<01:43,  1.50s/it]\n",
      "Epoch 22:  72%|███████▏  | 179/247 [04:31<01:42,  1.50s/it]\n",
      "Epoch 22:  73%|███████▎  | 180/247 [04:33<01:40,  1.50s/it]\n",
      "Epoch 22:  73%|███████▎  | 181/247 [04:34<01:38,  1.50s/it]\n",
      "Epoch 22:  74%|███████▎  | 182/247 [04:36<01:37,  1.50s/it]\n",
      "Epoch 22:  74%|███████▍  | 183/247 [04:37<01:36,  1.50s/it]\n",
      "Epoch 22:  74%|███████▍  | 184/247 [04:39<01:34,  1.50s/it]\n",
      "Epoch 22:  75%|███████▍  | 185/247 [04:40<01:32,  1.49s/it]\n",
      "Epoch 22:  75%|███████▌  | 186/247 [04:42<01:31,  1.49s/it]\n",
      "Epoch 22:  76%|███████▌  | 187/247 [04:43<01:29,  1.49s/it]\n",
      "Epoch 22:  76%|███████▌  | 188/247 [04:45<01:28,  1.49s/it]\n",
      "Epoch 22:  77%|███████▋  | 189/247 [04:46<01:26,  1.49s/it]\n",
      "Epoch 22:  77%|███████▋  | 190/247 [04:48<01:24,  1.49s/it]\n",
      "Epoch 22:  77%|███████▋  | 191/247 [04:49<01:23,  1.49s/it]\n",
      "Epoch 22:  78%|███████▊  | 192/247 [04:51<01:22,  1.50s/it]\n",
      "Epoch 22:  78%|███████▊  | 193/247 [04:52<01:20,  1.50s/it]\n",
      "Epoch 22:  79%|███████▊  | 194/247 [04:54<01:19,  1.50s/it]\n",
      "Epoch 22:  79%|███████▉  | 195/247 [04:55<01:18,  1.51s/it]\n",
      "Epoch 22:  79%|███████▉  | 196/247 [04:57<01:16,  1.50s/it]\n",
      "Epoch 22:  80%|███████▉  | 197/247 [04:58<01:15,  1.50s/it]\n",
      "Epoch 22:  80%|████████  | 198/247 [05:00<01:13,  1.50s/it]\n",
      "Epoch 22:  81%|████████  | 199/247 [05:01<01:11,  1.50s/it]\n",
      "Epoch 22:  81%|████████  | 200/247 [05:03<01:12,  1.55s/it]\n",
      "Epoch 22:  81%|████████▏ | 201/247 [05:04<01:10,  1.54s/it]\n",
      "Epoch 22:  82%|████████▏ | 202/247 [05:06<01:08,  1.53s/it]\n",
      "Epoch 22:  82%|████████▏ | 203/247 [05:07<01:06,  1.52s/it]\n",
      "Epoch 22:  83%|████████▎ | 204/247 [05:09<01:05,  1.51s/it]\n",
      "Epoch 22:  83%|████████▎ | 205/247 [05:10<01:03,  1.52s/it]\n",
      "Epoch 22:  83%|████████▎ | 206/247 [05:12<01:02,  1.52s/it]\n",
      "Epoch 22:  84%|████████▍ | 207/247 [05:13<01:00,  1.51s/it]\n",
      "Epoch 22:  84%|████████▍ | 208/247 [05:15<00:59,  1.52s/it]\n",
      "Epoch 22:  85%|████████▍ | 209/247 [05:16<00:56,  1.50s/it]\n",
      "Epoch 22:  85%|████████▌ | 210/247 [05:18<00:55,  1.51s/it]\n",
      "Epoch 22:  85%|████████▌ | 211/247 [05:19<00:53,  1.50s/it]\n",
      "Epoch 22:  86%|████████▌ | 212/247 [05:21<00:52,  1.49s/it]\n",
      "Epoch 22:  86%|████████▌ | 213/247 [05:22<00:50,  1.48s/it]\n",
      "Epoch 22:  87%|████████▋ | 214/247 [05:24<00:49,  1.50s/it]\n",
      "Epoch 22:  87%|████████▋ | 215/247 [05:25<00:47,  1.49s/it]\n",
      "Epoch 22:  87%|████████▋ | 216/247 [05:27<00:46,  1.49s/it]\n",
      "Epoch 22:  88%|████████▊ | 217/247 [05:28<00:44,  1.49s/it]\n",
      "Epoch 22:  88%|████████▊ | 218/247 [05:30<00:43,  1.50s/it]\n",
      "Epoch 22:  89%|████████▊ | 219/247 [05:31<00:42,  1.51s/it]\n",
      "Epoch 22:  89%|████████▉ | 220/247 [05:33<00:40,  1.49s/it]\n",
      "Epoch 22:  89%|████████▉ | 221/247 [05:34<00:38,  1.49s/it]\n",
      "Epoch 22:  90%|████████▉ | 222/247 [05:36<00:37,  1.51s/it]\n",
      "Epoch 22:  90%|█████████ | 223/247 [05:37<00:36,  1.51s/it]\n",
      "Epoch 22:  91%|█████████ | 224/247 [05:39<00:34,  1.50s/it]\n",
      "Epoch 22:  91%|█████████ | 225/247 [05:40<00:33,  1.50s/it]\n",
      "Epoch 22:  91%|█████████▏| 226/247 [05:42<00:32,  1.55s/it]\n",
      "Epoch 22:  92%|█████████▏| 227/247 [05:44<00:30,  1.54s/it]\n",
      "Epoch 22:  92%|█████████▏| 228/247 [05:45<00:28,  1.52s/it]\n",
      "Epoch 22:  93%|█████████▎| 229/247 [05:47<00:27,  1.52s/it]\n",
      "Epoch 22:  93%|█████████▎| 230/247 [05:48<00:25,  1.51s/it]\n",
      "Epoch 22:  94%|█████████▎| 231/247 [05:50<00:23,  1.50s/it]\n",
      "Epoch 22:  94%|█████████▍| 232/247 [05:51<00:22,  1.49s/it]\n",
      "Epoch 22:  94%|█████████▍| 233/247 [05:53<00:20,  1.50s/it]\n",
      "Epoch 22:  95%|█████████▍| 234/247 [05:54<00:19,  1.51s/it]\n",
      "Epoch 22:  95%|█████████▌| 235/247 [05:56<00:18,  1.51s/it]\n",
      "Epoch 22:  96%|█████████▌| 236/247 [05:57<00:16,  1.51s/it]\n",
      "Epoch 22:  96%|█████████▌| 237/247 [05:59<00:15,  1.51s/it]\n",
      "Epoch 22:  96%|█████████▋| 238/247 [06:00<00:13,  1.53s/it]\n",
      "Epoch 22:  97%|█████████▋| 239/247 [06:02<00:12,  1.52s/it]\n",
      "Epoch 22:  97%|█████████▋| 240/247 [06:03<00:10,  1.51s/it]\n",
      "Epoch 22:  98%|█████████▊| 241/247 [06:05<00:09,  1.65s/it]\n",
      "Epoch 22:  98%|█████████▊| 242/247 [06:07<00:07,  1.60s/it]\n",
      "Epoch 22:  98%|█████████▊| 243/247 [06:08<00:06,  1.57s/it]\n",
      "Epoch 22:  99%|█████████▉| 244/247 [06:10<00:04,  1.55s/it]\n",
      "Epoch 22:  99%|█████████▉| 245/247 [06:11<00:03,  1.54s/it]\n",
      "Epoch 22: 100%|█████████▉| 246/247 [06:13<00:01,  1.51s/it]\n",
      "Epoch 22: 100%|██████████| 247/247 [06:14<00:00,  1.45s/it]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 22: 100%|██████████| 247/247 [06:16<00:00,  1.52s/it,  epoch 22 | loss 0.0724 | Train coarse acc 0.946 |Train coarse Prec 0.944 | Train coarse Rec 0.938 | Train fine acc 0.001 |Train fine Prec 0.001 | Train fine Rec 0.001]\n",
      "Evaluate test set:   0%|          | 0/51 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Evaluate test set:   2%|▏         | 1/51 [00:01<01:01,  1.23s/it]\n",
      "Evaluate test set:   4%|▍         | 2/51 [00:02<00:47,  1.03it/s]\n",
      "Evaluate test set:   6%|▌         | 3/51 [00:02<00:41,  1.15it/s]\n",
      "Evaluate test set:   8%|▊         | 4/51 [00:03<00:38,  1.22it/s]\n",
      "Evaluate test set:  10%|▉         | 5/51 [00:04<00:36,  1.25it/s]\n",
      "Evaluate test set:  12%|█▏        | 6/51 [00:05<00:36,  1.25it/s]\n",
      "Evaluate test set:  14%|█▎        | 7/51 [00:05<00:34,  1.28it/s]\n",
      "Evaluate test set:  16%|█▌        | 8/51 [00:06<00:33,  1.30it/s]\n",
      "Evaluate test set:  18%|█▊        | 9/51 [00:07<00:31,  1.31it/s]\n",
      "Evaluate test set:  20%|█▉        | 10/51 [00:08<00:30,  1.33it/s]\n",
      "Evaluate test set:  22%|██▏       | 11/51 [00:08<00:29,  1.33it/s]\n",
      "Evaluate test set:  24%|██▎       | 12/51 [00:09<00:29,  1.33it/s]\n",
      "Evaluate test set:  25%|██▌       | 13/51 [00:10<00:28,  1.32it/s]\n",
      "Evaluate test set:  27%|██▋       | 14/51 [00:11<00:28,  1.30it/s]\n",
      "Evaluate test set:  29%|██▉       | 15/51 [00:11<00:27,  1.29it/s]\n",
      "Evaluate test set:  31%|███▏      | 16/51 [00:12<00:27,  1.29it/s]\n",
      "Evaluate test set:  33%|███▎      | 17/51 [00:13<00:26,  1.30it/s]\n",
      "Evaluate test set:  35%|███▌      | 18/51 [00:14<00:25,  1.29it/s]\n",
      "Evaluate test set:  37%|███▋      | 19/51 [00:14<00:24,  1.29it/s]\n",
      "Evaluate test set:  39%|███▉      | 20/51 [00:15<00:23,  1.30it/s]\n",
      "Evaluate test set:  41%|████      | 21/51 [00:16<00:23,  1.30it/s]\n",
      "Evaluate test set:  43%|████▎     | 22/51 [00:17<00:22,  1.30it/s]\n",
      "Evaluate test set:  45%|████▌     | 23/51 [00:18<00:21,  1.29it/s]\n",
      "Evaluate test set:  47%|████▋     | 24/51 [00:18<00:20,  1.29it/s]\n",
      "Evaluate test set:  49%|████▉     | 25/51 [00:19<00:20,  1.29it/s]\n",
      "Evaluate test set:  51%|█████     | 26/51 [00:20<00:19,  1.28it/s]\n",
      "Evaluate test set:  53%|█████▎    | 27/51 [00:21<00:18,  1.27it/s]\n",
      "Evaluate test set:  55%|█████▍    | 28/51 [00:21<00:17,  1.29it/s]\n",
      "Evaluate test set:  57%|█████▋    | 29/51 [00:22<00:16,  1.31it/s]\n",
      "Evaluate test set:  59%|█████▉    | 30/51 [00:23<00:15,  1.32it/s]\n",
      "Evaluate test set:  61%|██████    | 31/51 [00:24<00:15,  1.32it/s]\n",
      "Evaluate test set:  63%|██████▎   | 32/51 [00:24<00:14,  1.32it/s]\n",
      "Evaluate test set:  65%|██████▍   | 33/51 [00:25<00:13,  1.32it/s]\n",
      "Evaluate test set:  67%|██████▋   | 34/51 [00:26<00:12,  1.33it/s]\n",
      "Evaluate test set:  69%|██████▊   | 35/51 [00:27<00:12,  1.32it/s]\n",
      "Evaluate test set:  71%|███████   | 36/51 [00:27<00:11,  1.33it/s]\n",
      "Evaluate test set:  73%|███████▎  | 37/51 [00:28<00:10,  1.33it/s]\n",
      "Evaluate test set:  75%|███████▍  | 38/51 [00:29<00:09,  1.33it/s]\n",
      "Evaluate test set:  76%|███████▋  | 39/51 [00:30<00:08,  1.34it/s]\n",
      "Evaluate test set:  78%|███████▊  | 40/51 [00:30<00:08,  1.34it/s]\n",
      "Evaluate test set:  80%|████████  | 41/51 [00:31<00:07,  1.34it/s]\n",
      "Evaluate test set:  82%|████████▏ | 42/51 [00:32<00:06,  1.33it/s]\n",
      "Evaluate test set:  84%|████████▍ | 43/51 [00:33<00:05,  1.34it/s]\n",
      "Evaluate test set:  86%|████████▋ | 44/51 [00:33<00:05,  1.33it/s]\n",
      "Evaluate test set:  88%|████████▊ | 45/51 [00:34<00:04,  1.33it/s]\n",
      "Evaluate test set:  90%|█████████ | 46/51 [00:35<00:03,  1.33it/s]\n",
      "Evaluate test set:  92%|█████████▏| 47/51 [00:36<00:02,  1.34it/s]\n",
      "Evaluate test set:  94%|█████████▍| 48/51 [00:36<00:02,  1.35it/s]\n",
      "Evaluate test set:  96%|█████████▌| 49/51 [00:37<00:01,  1.35it/s]\n",
      "Evaluate test set:  98%|█████████▊| 50/51 [00:38<00:00,  1.34it/s]\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:39<00:00,  1.37it/s]\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m /home/ngocbach/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "Evaluate test set: 100%|██████████| 51/51 [00:39<00:00,  1.29it/s,  loss 0.0785 | Train coarse acc 0.817 |Train coarse Prec 0.819 | Train coarse Rec 0.798 | Train fine acc 0.008 |Train fine Prec 0.006 | Train fine Rec 0.006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m Saved PyTorch Model State to model 0 ltn_combine binary 1e-05 0.75 2023-12-14\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m ####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23:   0%|          | 0/247 [00:00<?, ?it/s]/home/ngocbach/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(hyper_parameter_tune pid=446250)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "Epoch 23:   0%|          | 1/247 [00:01<07:52,  1.92s/it]\n",
      "Epoch 23:   1%|          | 2/247 [00:03<06:42,  1.64s/it]\n",
      "Epoch 23:   1%|          | 3/247 [00:04<06:19,  1.55s/it]\n",
      "Epoch 23:   2%|▏         | 4/247 [00:06<06:03,  1.50s/it]\n",
      "Epoch 23:   2%|▏         | 5/247 [00:07<05:59,  1.48s/it]\n",
      "Epoch 23:   2%|▏         | 6/247 [00:09<05:53,  1.47s/it]\n",
      "Epoch 23:   3%|▎         | 7/247 [00:10<05:50,  1.46s/it]\n",
      "Epoch 23:   3%|▎         | 8/247 [00:12<05:49,  1.46s/it]\n",
      "Epoch 23:   4%|▎         | 9/247 [00:13<05:57,  1.50s/it]\n",
      "Epoch 23:   4%|▍         | 10/247 [00:15<05:49,  1.48s/it]\n",
      "Epoch 23:   4%|▍         | 11/247 [00:16<05:46,  1.47s/it]\n",
      "Epoch 23:   5%|▍         | 12/247 [00:17<05:45,  1.47s/it]\n",
      "Epoch 23:   5%|▌         | 13/247 [00:19<05:43,  1.47s/it]\n",
      "Epoch 23:   6%|▌         | 14/247 [00:20<05:41,  1.47s/it]\n",
      "Epoch 23:   6%|▌         | 15/247 [00:22<05:38,  1.46s/it]\n",
      "Epoch 23:   6%|▋         | 16/247 [00:23<05:37,  1.46s/it]\n",
      "Epoch 23:   7%|▋         | 17/247 [00:25<05:37,  1.47s/it]\n",
      "Epoch 23:   7%|▋         | 18/247 [00:26<05:34,  1.46s/it]\n",
      "Epoch 23:   8%|▊         | 19/247 [00:28<05:29,  1.45s/it]\n",
      "Epoch 23:   8%|▊         | 20/247 [00:29<05:29,  1.45s/it]\n",
      "Epoch 23:   9%|▊         | 21/247 [00:31<05:26,  1.45s/it]\n",
      "Epoch 23:   9%|▉         | 22/247 [00:32<05:25,  1.45s/it]\n",
      "Epoch 23:   9%|▉         | 23/247 [00:33<05:25,  1.45s/it]\n",
      "Epoch 23:  10%|▉         | 24/247 [00:35<05:22,  1.45s/it]\n",
      "Epoch 23:  10%|█         | 25/247 [00:36<05:21,  1.45s/it]\n",
      "Epoch 23:  11%|█         | 26/247 [00:38<05:21,  1.45s/it]\n",
      "Epoch 23:  11%|█         | 27/247 [00:39<05:19,  1.45s/it]\n",
      "Epoch 23:  11%|█▏        | 28/247 [00:41<05:16,  1.45s/it]\n",
      "Epoch 23:  12%|█▏        | 29/247 [00:42<05:17,  1.45s/it]\n",
      "Epoch 23:  12%|█▏        | 30/247 [00:44<05:17,  1.46s/it]\n",
      "Epoch 23:  13%|█▎        | 31/247 [00:45<05:16,  1.46s/it]\n",
      "Epoch 23:  13%|█▎        | 32/247 [00:47<05:13,  1.46s/it]\n",
      "Epoch 23:  13%|█▎        | 33/247 [00:48<05:12,  1.46s/it]\n",
      "Epoch 23:  14%|█▍        | 34/247 [00:50<05:17,  1.49s/it]\n",
      "Epoch 23:  14%|█▍        | 35/247 [00:51<05:15,  1.49s/it]\n",
      "Epoch 23:  15%|█▍        | 36/247 [00:53<05:11,  1.48s/it]\n",
      "Epoch 23:  15%|█▍        | 37/247 [00:54<05:05,  1.46s/it]\n",
      "Epoch 23:  15%|█▌        | 38/247 [00:55<05:03,  1.45s/it]\n",
      "Epoch 23:  16%|█▌        | 39/247 [00:57<05:00,  1.45s/it]\n",
      "Epoch 23:  16%|█▌        | 40/247 [00:58<04:59,  1.45s/it]\n",
      "Epoch 23:  17%|█▋        | 41/247 [01:00<04:56,  1.44s/it]\n",
      "Epoch 23:  17%|█▋        | 42/247 [01:01<04:56,  1.45s/it]\n",
      "Epoch 23:  17%|█▋        | 43/247 [01:03<04:57,  1.46s/it]\n",
      "Epoch 23:  18%|█▊        | 44/247 [01:04<04:56,  1.46s/it]\n",
      "Epoch 23:  18%|█▊        | 45/247 [01:05<04:51,  1.44s/it]\n",
      "Epoch 23:  19%|█▊        | 46/247 [01:07<04:47,  1.43s/it]\n",
      "Epoch 23:  19%|█▉        | 47/247 [01:08<04:44,  1.42s/it]\n",
      "Epoch 23:  19%|█▉        | 48/247 [01:10<04:42,  1.42s/it]\n",
      "Epoch 23:  20%|█▉        | 49/247 [01:11<04:39,  1.41s/it]\n",
      "Epoch 23:  20%|██        | 50/247 [01:12<04:37,  1.41s/it]\n",
      "Epoch 23:  21%|██        | 51/247 [01:14<04:34,  1.40s/it]\n",
      "Epoch 23:  21%|██        | 52/247 [01:15<04:33,  1.40s/it]\n",
      "Epoch 23:  21%|██▏       | 53/247 [01:17<04:32,  1.41s/it]\n",
      "Epoch 23:  22%|██▏       | 54/247 [01:18<04:36,  1.43s/it]\n",
      "Epoch 23:  22%|██▏       | 55/247 [01:20<04:39,  1.46s/it]\n",
      "Epoch 23:  23%|██▎       | 56/247 [01:21<04:37,  1.45s/it]\n",
      "Epoch 23:  23%|██▎       | 57/247 [01:23<04:35,  1.45s/it]\n",
      "Epoch 23:  23%|██▎       | 58/247 [01:24<04:31,  1.43s/it]\n",
      "Epoch 23:  24%|██▍       | 59/247 [01:26<04:34,  1.46s/it]\n",
      "Epoch 23:  24%|██▍       | 60/247 [01:27<04:29,  1.44s/it]\n",
      "Epoch 23:  25%|██▍       | 61/247 [01:28<04:25,  1.43s/it]\n",
      "Epoch 23:  25%|██▌       | 62/247 [01:30<04:22,  1.42s/it]\n",
      "Epoch 23:  26%|██▌       | 63/247 [01:31<04:21,  1.42s/it]\n",
      "Epoch 23:  26%|██▌       | 64/247 [01:33<04:23,  1.44s/it]\n",
      "Epoch 23:  26%|██▋       | 65/247 [01:34<04:23,  1.45s/it]\n",
      "Epoch 23:  27%|██▋       | 66/247 [01:35<04:19,  1.43s/it]\n",
      "Epoch 23:  27%|██▋       | 67/247 [01:37<04:17,  1.43s/it]\n",
      "Epoch 23:  28%|██▊       | 68/247 [01:38<04:16,  1.43s/it]\n",
      "Epoch 23:  28%|██▊       | 69/247 [01:40<04:13,  1.43s/it]\n",
      "Epoch 23:  28%|██▊       | 70/247 [01:41<04:11,  1.42s/it]\n",
      "Epoch 23:  29%|██▊       | 71/247 [01:43<04:11,  1.43s/it]\n",
      "Epoch 23:  29%|██▉       | 72/247 [01:44<04:07,  1.41s/it]\n",
      "Epoch 23:  30%|██▉       | 73/247 [01:45<04:10,  1.44s/it]\n",
      "Epoch 23:  30%|██▉       | 74/247 [01:47<04:13,  1.47s/it]\n",
      "Epoch 23:  30%|███       | 75/247 [01:49<04:15,  1.49s/it]\n",
      "Epoch 23:  31%|███       | 76/247 [01:50<04:14,  1.49s/it]\n",
      "Epoch 23:  31%|███       | 77/247 [01:52<04:11,  1.48s/it]\n",
      "Epoch 23:  32%|███▏      | 78/247 [01:53<04:08,  1.47s/it]\n",
      "Epoch 23:  32%|███▏      | 79/247 [01:54<04:07,  1.47s/it]\n",
      "Epoch 23:  32%|███▏      | 80/247 [01:56<04:07,  1.48s/it]\n",
      "Epoch 23:  33%|███▎      | 81/247 [01:57<04:04,  1.48s/it]\n",
      "Epoch 23:  33%|███▎      | 82/247 [01:59<04:03,  1.48s/it]\n",
      "Epoch 23:  34%|███▎      | 83/247 [02:00<04:02,  1.48s/it]\n",
      "Epoch 23:  34%|███▍      | 84/247 [02:02<04:12,  1.55s/it]\n",
      "Epoch 23:  34%|███▍      | 85/247 [02:04<04:08,  1.54s/it]\n",
      "Epoch 23:  35%|███▍      | 86/247 [02:05<04:04,  1.52s/it]\n",
      "Epoch 23:  35%|███▌      | 87/247 [02:07<04:01,  1.51s/it]\n",
      "Epoch 23:  36%|███▌      | 88/247 [02:08<04:00,  1.51s/it]\n",
      "Epoch 23:  36%|███▌      | 89/247 [02:10<03:57,  1.50s/it]\n",
      "Epoch 23:  36%|███▋      | 90/247 [02:11<03:55,  1.50s/it]\n",
      "Epoch 23:  37%|███▋      | 91/247 [02:12<03:51,  1.48s/it]\n",
      "Epoch 23:  37%|███▋      | 92/247 [02:14<03:49,  1.48s/it]\n",
      "Epoch 23:  38%|███▊      | 93/247 [02:15<03:46,  1.47s/it]\n",
      "Epoch 23:  38%|███▊      | 94/247 [02:17<03:49,  1.50s/it]\n",
      "Epoch 23:  38%|███▊      | 95/247 [02:18<03:47,  1.49s/it]\n",
      "Epoch 23:  39%|███▉      | 96/247 [02:20<03:41,  1.47s/it]\n",
      "Epoch 23:  39%|███▉      | 97/247 [02:21<03:36,  1.44s/it]\n",
      "Epoch 23:  40%|███▉      | 98/247 [02:23<03:33,  1.43s/it]\n",
      "Epoch 23:  40%|████      | 99/247 [02:24<03:31,  1.43s/it]\n",
      "Epoch 23:  40%|████      | 100/247 [02:25<03:25,  1.40s/it]\n",
      "Epoch 23:  41%|████      | 101/247 [02:27<03:24,  1.40s/it]\n",
      "Epoch 23:  41%|████▏     | 102/247 [02:28<03:23,  1.40s/it]\n",
      "Epoch 23:  42%|████▏     | 103/247 [02:30<03:23,  1.41s/it]\n",
      "Epoch 23:  42%|████▏     | 104/247 [02:31<03:20,  1.40s/it]\n",
      "Epoch 23:  43%|████▎     | 105/247 [02:32<03:18,  1.40s/it]\n",
      "Epoch 23:  43%|████▎     | 106/247 [02:34<03:15,  1.38s/it]\n",
      "Epoch 23:  43%|████▎     | 107/247 [02:35<03:14,  1.39s/it]\n",
      "Epoch 23:  44%|████▎     | 108/247 [02:37<03:12,  1.39s/it]\n",
      "Epoch 23:  44%|████▍     | 109/247 [02:38<03:11,  1.39s/it]\n",
      "Epoch 23:  45%|████▍     | 110/247 [02:39<03:10,  1.39s/it]\n",
      "Epoch 23:  45%|████▍     | 111/247 [02:41<03:10,  1.40s/it]\n",
      "Epoch 23:  45%|████▌     | 112/247 [02:42<03:08,  1.40s/it]\n",
      "Epoch 23:  46%|████▌     | 113/247 [02:44<03:14,  1.45s/it]\n",
      "Epoch 23:  46%|████▌     | 114/247 [02:45<03:08,  1.42s/it]\n",
      "Epoch 23:  47%|████▋     | 115/247 [02:47<03:11,  1.45s/it]\n",
      "Epoch 23:  47%|████▋     | 116/247 [02:48<03:13,  1.47s/it]\n",
      "Epoch 23:  47%|████▋     | 117/247 [02:50<03:12,  1.48s/it]\n",
      "Epoch 23:  48%|████▊     | 118/247 [02:51<03:12,  1.49s/it]\n",
      "Epoch 23:  48%|████▊     | 119/247 [02:53<03:08,  1.47s/it]\n",
      "Epoch 23:  49%|████▊     | 120/247 [02:54<03:04,  1.45s/it]\n",
      "Epoch 23:  49%|████▉     | 121/247 [02:55<03:03,  1.46s/it]\n",
      "Epoch 23:  49%|████▉     | 122/247 [02:57<03:00,  1.45s/it]\n",
      "Epoch 23:  50%|████▉     | 123/247 [02:58<02:57,  1.43s/it]\n",
      "Epoch 23:  50%|█████     | 124/247 [03:00<02:52,  1.40s/it]\n",
      "Epoch 23:  51%|█████     | 125/247 [03:01<02:52,  1.41s/it]\n",
      "Epoch 23:  51%|█████     | 126/247 [03:02<02:50,  1.41s/it]\n",
      "Epoch 23:  51%|█████▏    | 127/247 [03:04<02:48,  1.40s/it]\n",
      "Epoch 23:  52%|█████▏    | 128/247 [03:05<02:46,  1.40s/it]\n",
      "Epoch 23:  52%|█████▏    | 129/247 [03:07<02:46,  1.41s/it]\n",
      "Epoch 23:  53%|█████▎    | 130/247 [03:08<02:45,  1.41s/it]\n",
      "Epoch 23:  53%|█████▎    | 131/247 [03:09<02:42,  1.40s/it]\n",
      "Epoch 23:  53%|█████▎    | 132/247 [03:11<02:41,  1.41s/it]\n",
      "Epoch 23:  54%|█████▍    | 133/247 [03:12<02:43,  1.44s/it]\n",
      "Epoch 23:  54%|█████▍    | 134/247 [03:14<02:45,  1.47s/it]\n",
      "Epoch 23:  55%|█████▍    | 135/247 [03:15<02:45,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray and start hyperparameter tuning\n",
    "# Resource will be used accordingly. The default is for gg colab notebook\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=2, num_gpus=1, ignore_reinit_error=True)\n",
    "\n",
    "# define tuner object\n",
    "# TODO: Change metric when necessary\n",
    "results = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(hyper_parameter_tune),\n",
    "        resources={\"cpu\": 2, \"gpu\": 1}\n",
    "    ),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=1,\n",
    "    ),\n",
    "    param_space = search_space\n",
    ")\n",
    "results.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 2 ltn_combine softmarginal 5e-06 0.75 2023-11-29\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config['lr'] = 5e-06\n",
    "config['beta'] = 1\n",
    "# Update description:\n",
    "description = 'model ' + str(vit_model_index) + \\\n",
    "    ' ' + \"ltn_combine\" + ' ' + \"softmarginal\" + \\\n",
    "    ' ' + str(config['lr']) + ' ' + str(config['beta']) + ' ' + str(\"2023-11-29\")\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load checkpoint and get the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "load_checkpoint_path = \"/home/ngocbach/model/model_model 0 ltn_combine binary 1e-05 0.75 2023-12-14.pth\"\n",
    "checkpoint = torch.load(load_checkpoint_path)\n",
    "\n",
    "# Load model and optimizer states\n",
    "base_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "# Load scheduler state, if available in the checkpoint\n",
    "if 'scheduler' in checkpoint:\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    # Retrieve the epoch information if available in the checkpoint\n",
    "    loaded_epoch = scheduler.last_epoch\n",
    "\n",
    "print('load checkpoint successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_index = slice(num_coarse_label)\n",
    "fine_index = slice(num_coarse_label, num_all_label)\n",
    "\n",
    "coarse_label_ground_truth = []\n",
    "coarse_label_prediction = []\n",
    "fine_label_ground_truth = []\n",
    "fine_label_prediction = []\n",
    "image_path_list = []\n",
    "# Iterate through the test data and make predictions\n",
    "for batch_idx, (data, labels_coarse, labels_fine, image_path) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "\n",
    "    prediction = base_model(data).cpu().detach()\n",
    "\n",
    "    prediction_coarse_label = prediction[:, coarse_index]\n",
    "    coarse_label_prediction_batch = torch.argmax(\n",
    "        prediction_coarse_label, dim=1)\n",
    "    coarse_label_prediction.extend(coarse_label_prediction_batch)\n",
    "    coarse_label_ground_truth.extend(labels_coarse)\n",
    "\n",
    "    prediction_fine_label = prediction[:, fine_index]\n",
    "    fine_label_prediction_batch = torch.argmax(\n",
    "        prediction_fine_label, dim=1) + num_coarse_label\n",
    "    fine_label_prediction.extend(fine_label_prediction_batch)\n",
    "    fine_label_ground_truth.extend(labels_fine)\n",
    "\n",
    "    # get image path\n",
    "    image_path_list.extend(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming coarse_label and fine_label are lists of tensors\n",
    "coarse_label = coarse_label_prediction\n",
    "fine_label = fine_label_prediction\n",
    "\n",
    "# Your coarse_to_fine dictionary\n",
    "coarse_to_fine = {0: [7, 8, 9, 10], 1: [11, 12, 13], 2: [14, 15, 16, 17],\n",
    "                  3: [18, 19, 20, 21, 22, 23], 4: [24, 25, 26, 27, 28],\n",
    "                  5: [29], 6: [30]}\n",
    "\n",
    "# Convert tensors to integers\n",
    "coarse_label = [int(label.item()) for label in coarse_label]\n",
    "fine_label = [int(label.item()) for label in fine_label]\n",
    "\n",
    "count = 0\n",
    "# Count of pairs without one-to-one correspondence\n",
    "for i in range(len(coarse_label_prediction)):\n",
    "    if (fine_label[i] in coarse_to_fine[coarse_label[i]]):\n",
    "        count = count + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num consistancy: 0.959284392350401\n"
     ]
    }
   ],
   "source": [
    "print(\"num consistancy:\" ,count / len(coarse_label_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Coarse Labels: 0.8140445561246653\n",
      "Accuracy for Coarse Labels: 0.8149290561381863\n",
      "F1 Score for Fine Labels: 0.6545073338014396\n",
      "Accuracy for Fine Labels: 0.649599012954966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming coarse_label, fine_label, coarse_label_ground_truth, and fine_label_ground_truth are lists of tensors\n",
    "# Your coarse_to_fine dictionary\n",
    "coarse_to_fine = {0: [7, 8, 9, 10], 1: [11, 12, 13], 2: [14, 15, 16, 17],\n",
    "                  3: [18, 19, 20, 21, 22, 23], 4: [24, 25, 26, 27, 28],\n",
    "                  5: [29], 6: [30]}\n",
    "\n",
    "# Calculate F1 and accuracy for coarse labels\n",
    "f1_coarse = f1_score(coarse_label_ground_truth, coarse_label_prediction, average='weighted')\n",
    "accuracy_coarse = accuracy_score(coarse_label_ground_truth, coarse_label_prediction)\n",
    "\n",
    "# Calculate F1 and accuracy for fine labels\n",
    "f1_fine = f1_score(fine_label_ground_truth, fine_label_prediction, average='weighted')\n",
    "accuracy_fine = accuracy_score(fine_label_ground_truth, fine_label_prediction)\n",
    "\n",
    "print(f\"F1 Score for Coarse Labels: {f1_coarse}\")\n",
    "print(f\"Accuracy for Coarse Labels: {accuracy_coarse}\")\n",
    "print(f\"F1 Score for Fine Labels: {f1_fine}\")\n",
    "print(f\"Accuracy for Fine Labels: {accuracy_fine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
