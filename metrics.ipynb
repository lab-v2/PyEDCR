{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-25T07:50:26.627974Z",
     "start_time": "2023-10-25T07:50:26.435158Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import abc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import termcolor\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from vision_models import lrs, vit_model_names\n",
    "from metacognitive_pipeline import fine_grain_classes, n_classes\n",
    "\n",
    "class Context(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "    \n",
    "class Plot(Context):\n",
    "    def __init__(self,\n",
    "                 fig_sizes: tuple = None):\n",
    "        if fig_sizes:\n",
    "            plt.figure(figsize=fig_sizes)\n",
    "\n",
    "    def __enter__(self):\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        plt.show()\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        \n",
    "data_dir = 'results/'  # Set the directory where your .npy files are located\n",
    "\n",
    "# Initialize dictionaries to store training and test accuracy data for each model\n",
    "model_train_data = {}\n",
    "model_test_data = {}\n",
    "\n",
    "test_true = np.load(os.path.join(data_dir, 'test_true.npy'))\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    train_match = re.match(pattern=r'(.+?)_train_(loss|acc)_lr(.+?)_e(\\d+?).npy',\n",
    "                           string=filename)\n",
    "    test_match = re.match(pattern=r'(.+?)_test_pred_lr(.+?)_e(\\d+?).npy',\n",
    "                          string=filename)\n",
    "\n",
    "    if train_match:\n",
    "        model_name = train_match.group(1)\n",
    "        metric = train_match.group(2)\n",
    "        lr_value = float(train_match.group(3))\n",
    "        num_epochs = int(train_match.group(4)) + 1\n",
    "\n",
    "        # Load the data from the .npy file\n",
    "        data = np.load(os.path.join(data_dir, filename))\n",
    "\n",
    "        # Store the data in the model_data dictionary\n",
    "        if model_name not in model_train_data:\n",
    "            model_train_data[model_name] = {}\n",
    "        if metric not in model_train_data[model_name]:\n",
    "            model_train_data[model_name][metric] = {}\n",
    "        if lr_value not in model_train_data[model_name][metric]:\n",
    "            model_train_data[model_name][metric][lr_value] = {}\n",
    "\n",
    "        model_train_data[model_name][metric][lr_value][num_epochs] = data[-1]\n",
    "    elif test_match:\n",
    "        model_name = test_match.group(1)\n",
    "        lr_value = float(test_match.group(2))\n",
    "        num_epochs = int(test_match.group(3)) + 1\n",
    "\n",
    "        # Load the test data from the .npy file\n",
    "        test_pred = np.load(os.path.join(data_dir, filename))\n",
    "\n",
    "        # Store the data in the model_test_data dictionary\n",
    "        if model_name not in model_test_data:\n",
    "            model_test_data[model_name] = {}\n",
    "        if lr_value not in model_test_data[model_name]:\n",
    "            model_test_data[model_name][lr_value] = {}\n",
    "\n",
    "        model_test_data[model_name][lr_value][num_epochs] = \\\n",
    "            {'acc': accuracy_score(test_true, test_pred), \n",
    "             'cm': confusion_matrix(test_true, test_pred),\n",
    "             'pre': precision_score(test_true, test_pred, labels=range(n_classes), average=None),\n",
    "             'rec': recall_score(test_true, test_pred, labels=range(n_classes), average=None),\n",
    "             'f1': f1_score(test_true, test_pred, labels=range(n_classes), average=None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8826ef905a164c15"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def plot_train_metrics():\n",
    "    # Create plots for training metric vs. epoch for each model\n",
    "    for model_name, model_data in sorted(model_train_data.items()):\n",
    "        print('\\n' + '#'* (100 + len(model_name)))\n",
    "        print('#'* 50 + f'{model_name}' + '#'* 50)\n",
    "        print('#'* (100 + len(model_name)) + '\\n')\n",
    "        for metric, metric_data in model_data.items():\n",
    "            with Plot():\n",
    "                plt.title(f\"{model_name} training {metric} vs. epoch\")\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel(metric.capitalize())\n",
    "    \n",
    "                for lr_value, lr_data in sorted(metric_data.items()):\n",
    "                    epochs, data = zip(*sorted(lr_data.items())) # Sort the data based on the number of epochs\n",
    "                    plt.plot(epochs, data, label=f'lr={lr_value}')\n",
    "                    plt.xticks(np.arange(min(epochs), max(epochs)+1, 1)) # Set the x-axis ticks to be integers\n",
    "    \n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "# plot_train_metrics()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T07:50:27.672091Z",
     "start_time": "2023-10-25T07:50:27.645933Z"
    }
   },
   "id": "77ea4d8f8e19eb6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b4fda72717cd425"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤════════════╤════════════╤════════════╕\n",
      "│ Model Name   │   lr=1e-05 │   lr=1e-06 │   lr=5e-05 │\n",
      "╞══════════════╪════════════╪════════════╪════════════╡\n",
      "│ vit_b_16     │   0.639112 │   0.699568 │   0.645281 │\n",
      "├──────────────┼────────────┼────────────┼────────────┤\n",
      "│ vit_b_32     │   0.558914 │   0.636644 │   0.537323 │\n",
      "├──────────────┼────────────┼────────────┼────────────┤\n",
      "│ vit_l_16     │   0.699568 │   0.702653 │   0.661937 │\n",
      "├──────────────┼────────────┼────────────┼────────────┤\n",
      "│ vit_l_32     │   0.582357 │   0.673041 │   0.624306 │\n",
      "╘══════════════╧════════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "def plot_test_metrics():\n",
    "    # Create a dictionary to store accuracy values for each model and learning rate\n",
    "    accuracy_data = {}\n",
    "\n",
    "    # Now, create plots for test accuracy vs. epoch for each model\n",
    "    for model_name, model_data in sorted(model_test_data.items()):\n",
    "        for lr_value, lr_data in sorted(model_data.items()):\n",
    "            # Collect the accuracy after the last epoch\n",
    "            last_epoch = sorted(lr_data.items())[-1][1]\n",
    "            accuracy = last_epoch['acc']\n",
    "\n",
    "            # Store the accuracy in the dictionary\n",
    "            if model_name not in accuracy_data:\n",
    "                accuracy_data[model_name] = {}\n",
    "            accuracy_data[model_name][f'lr={lr_value}'] = accuracy\n",
    "\n",
    "\n",
    "    # Get a list of all learning rates in the data\n",
    "    all_learning_rates = sorted(set(lr for model_data in accuracy_data.values() for lr in model_data))\n",
    "\n",
    "    # Generate the 2-D table with manual headers\n",
    "    headers = [\"Model Name\"] + all_learning_rates\n",
    "    table = []\n",
    "\n",
    "    for model_name in accuracy_data:\n",
    "        row = [model_name] + [accuracy_data[model_name].get(lr, \"N/A\") for lr in all_learning_rates]\n",
    "        table.append(row)\n",
    "\n",
    "    # Print the table using tabulate\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "plot_test_metrics()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T07:50:29.256600Z",
     "start_time": "2023-10-25T07:50:29.245428Z"
    }
   },
   "id": "473ad4b8260120a1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def plot_verbose_test_metrics(cms: bool = False, \n",
    "                              class_wise_accuracies: bool = False):\n",
    "    # Now, create plots for test accuracy vs. epoch for each model\n",
    "    for model_name, model_data in sorted(model_test_data.items()):\n",
    "        print('\\n' + '#'* (100 + len(model_name)))\n",
    "        print('#'* 50 + f'{model_name}' + '#'* 50)\n",
    "        print('#'* (100 + len(model_name)) + '\\n')\n",
    "        metric = 'Accuracy'\n",
    "        \n",
    "        with Plot():\n",
    "            plt.title(f\"{model_name} - Test {metric} vs. Epoch\")\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(metric)\n",
    "    \n",
    "            for lr_value, lr_data in sorted(model_data.items()):\n",
    "                # Sort the data based on the number of epochs\n",
    "                epochs, epoch_data = zip(*sorted(lr_data.items()))\n",
    "                plt.plot(epochs, [curr_data['acc'] for curr_data in epoch_data], label=f'lr={lr_value}')\n",
    "                plt.xticks(np.arange(min(epochs), max(epochs)+1, 1)) # Set the x-axis ticks to be integers\n",
    "    \n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "        \n",
    "        if cms or class_wise_accuracies:\n",
    "            for lr_value, lr_data in sorted(model_data.items()):\n",
    "                epochs, epoch_data = zip(*sorted(lr_data.items()))\n",
    "                \n",
    "                if cms:\n",
    "                    with Plot():\n",
    "                        plt.figure(figsize=(12, 9))\n",
    "                        sns.heatmap(epoch_data[-1]['cm'], \n",
    "                                    annot=True, \n",
    "                                    fmt=\"d\",  \n",
    "                                    xticklabels=fine_grain_classes, \n",
    "                                    yticklabels=fine_grain_classes\n",
    "                                    )\n",
    "        \n",
    "                        plt.xlabel('Predicted')\n",
    "                        plt.ylabel('Actual')\n",
    "                        plt.title(f'{model_name}, lr={lr_value} Confusion Matrix')\n",
    "                \n",
    "                if class_wise_accuracies:\n",
    "                    for class_label, class_name in enumerate(fine_grain_classes):\n",
    "                        precision = epoch_data[-1]['pre'][class_label]\n",
    "                        print(f'{model_name}, lr={lr_value}, {class_name}: Precision = {precision:.2f}')\n",
    "\n",
    "            \n",
    "# plot_verbose_test_metrics()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T23:52:03.903804Z",
     "start_time": "2023-10-24T23:52:03.879417Z"
    }
   },
   "id": "dff362a55920918d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDCR Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff5d015e77a12310"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "108"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_and_lrs_folders = os.listdir(f'figs')\n",
    "assert (len(vit_model_names) - 1) * (len(vit_model_names) - 2) * len(lrs) ** 2  == len(models_and_lrs_folders) - 1\n",
    "\n",
    "len(models_and_lrs_folders) - 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:00:17.755242Z",
     "start_time": "2023-10-25T08:00:17.724569Z"
    }
   },
   "id": "2c57b049b41cafb1"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def print_EDCR_results():\n",
    "    for filename in models_and_lrs_folders:\n",
    "        match = re.match(pattern=rf'main_(.+?)_lr(.+?)_secondary_(.+?)_lr(.+)',\n",
    "                         string=filename)\n",
    "        if match:\n",
    "            main_model_name, main_lr, secondary_model_name, secondary_lr = (match.group(i) for i in range(1,5))\n",
    "            prior_predictions = np.load(os.path.join(data_dir, \n",
    "                                                     rf'{main_model_name}_test_pred_lr{main_lr}_e3.npy'))\n",
    "            prior_acc = accuracy_score(y_true=test_true, \n",
    "                                       y_pred=prior_predictions)\n",
    "            \n",
    "            post_predictions = np.load(f'{data_dir}/figs/{match.group(0)}/results.npy')\n",
    "            posterior_acc = accuracy_score(y_true=test_true, \n",
    "                                           y_pred=post_predictions)\n",
    "            print('#' * 100 + f'Main: {main_model_name} with lr {main_lr}, '\n",
    "                              f'secondary: {secondary_model_name} with lr {secondary_lr}\\n'\n",
    "                  f'Prior acc:{prior_acc}, post acc: {posterior_acc}\\n')\n",
    "            print(termcolor.colored(f\"Total acc change {'+' if posterior_acc > prior_acc else ''}\"\n",
    "                                    f\"{round((posterior_acc - prior_acc)*100, 3)}%\", \n",
    "                                    'green' if posterior_acc > prior_acc else 'red'))\n",
    "\n",
    "# print_EDCR_results()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:00:22.875108Z",
     "start_time": "2023-10-25T08:00:22.857360Z"
    }
   },
   "id": "b2bbfa3c5f3930f9"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/vit_b_32_test_pred_lr0.0005_e3.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m match:\n\u001B[1;32m     12\u001B[0m     main_model_name, main_lr, secondary_model_name, secondary_lr \u001B[38;5;241m=\u001B[39m (match\u001B[38;5;241m.\u001B[39mgroup(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m---> 13\u001B[0m     prior_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mrf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmain_model_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_test_pred_lr\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmain_lr\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_e3.npy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     prior_acc \u001B[38;5;241m=\u001B[39m accuracy_score(y_true\u001B[38;5;241m=\u001B[39mtest_true, \n\u001B[1;32m     15\u001B[0m                                y_pred\u001B[38;5;241m=\u001B[39mprior_predictions)\n\u001B[1;32m     17\u001B[0m     post_predictions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/figs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmatch\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/results.npy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/metacognitive_error_correction/venv/lib/python3.9/site-packages/numpy/lib/npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'results/vit_b_32_test_pred_lr0.0005_e3.npy'"
     ]
    }
   ],
   "source": [
    "# Sample data structure (replace this with your actual data)\n",
    "data = {}  # Create an empty dictionary to store the accuracy data\n",
    "\n",
    "# Track the maximal accuracy value across all tables\n",
    "max_accuracy = -1.0\n",
    "\n",
    "# Iterate through filenames to collect accuracy data\n",
    "for filename in models_and_lrs_folders:\n",
    "    match = re.match(pattern=rf'main_(.+?)_lr(.+?)_secondary_(.+?)_lr(.+)', \n",
    "                     string=filename)\n",
    "    if match:\n",
    "        main_model_name, main_lr, secondary_model_name, secondary_lr = (match.group(i) for i in range(1, 5))\n",
    "        prior_predictions = np.load(os.path.join(data_dir, rf'{main_model_name}_test_pred_lr{main_lr}_e3.npy'))\n",
    "        prior_acc = accuracy_score(y_true=test_true, \n",
    "                                   y_pred=prior_predictions)\n",
    "\n",
    "        post_predictions = np.load(f'{data_dir}/figs/{match.group(0)}/results.npy')\n",
    "        posterior_acc = accuracy_score(y_true=test_true, \n",
    "                                       y_pred=post_predictions)\n",
    "\n",
    "        # Update the maximal accuracy value\n",
    "        if posterior_acc > max_accuracy:\n",
    "            max_accuracy = posterior_acc\n",
    "\n",
    "        # Store accuracy data in the data dictionary\n",
    "        if main_model_name not in data:\n",
    "            data[main_model_name] = {}\n",
    "        if secondary_model_name not in data[main_model_name]:\n",
    "            data[main_model_name][secondary_model_name] = {}\n",
    "            \n",
    "        data[main_model_name][secondary_model_name][float(secondary_lr)] = {'prior': prior_acc, \n",
    "                                                                            'post': posterior_acc}\n",
    "\n",
    "# Loop through each main model and generate a table\n",
    "for main_model, secondary_data in data.items():\n",
    "    table_data = []\n",
    "\n",
    "    # Get a list of learning rates from the first secondary model\n",
    "    learning_rates = sorted(secondary_data.get(list(secondary_data.keys())[0], {}).keys())\n",
    "\n",
    "    # Sort the secondary model names\n",
    "    sorted_secondary_models = sorted(secondary_data.keys())\n",
    "\n",
    "    # Create the header row with learning rates\n",
    "    header = ['Secondary Model'] + learning_rates\n",
    "    table_data.append(header)\n",
    "\n",
    "    # Add rows for each secondary model, ensuring they are sorted\n",
    "    for secondary_model in sorted_secondary_models:\n",
    "        accuracy_data = secondary_data[secondary_model]\n",
    "        row = [secondary_model] + [accuracy_data.get(lr, '') for lr in learning_rates]\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Find the maximal accuracy value and its position in the table\n",
    "    max_acc_value = max(max(row[1:] for row in table_data[1:]))\n",
    "    max_acc_row, max_acc_col = [(i, row.index(max_acc_value)) \n",
    "                                for i, row in enumerate(table_data) if max_acc_value in row][0]\n",
    "\n",
    "    # Colorize the maximal accuracy cell in green\n",
    "    table_data[max_acc_row][max_acc_col] = termcolor.colored(text=max_acc_value, \n",
    "                                                             color='green')\n",
    "\n",
    "    # Create the table using tabulate\n",
    "    table = tabulate(tabular_data=table_data, \n",
    "                     headers='firstrow', \n",
    "                     tablefmt='grid')\n",
    "\n",
    "    # Print the main model name and the corresponding table\n",
    "    print(f\"Main Model: {main_model}\")\n",
    "    print(table)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print the maximal accuracy value across all tables (already colored in green)\n",
    "print(\"Maximal Accuracy Value: \", max_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T08:00:27.480602Z",
     "start_time": "2023-10-25T08:00:27.371060Z"
    }
   },
   "id": "3f6e34912281d5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2c312902f7756dff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
