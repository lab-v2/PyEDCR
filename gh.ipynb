{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c72514e-f325-49a6-86cd-0a69e3ca7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD is now at 8688d0f3 13.12 update\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Input GitHub username and PAT\n",
    "github_username = \"lab-v2\"\n",
    "token = \"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\"\n",
    "\n",
    "os.environ[\"GITHUB_AUTH\"] = f\"{github_username}:{token}\" # Set up GitHub credentials\n",
    "\n",
    "#Clone the private repository\n",
    "github_repo = 'metacognitive_error_detection_and_correction_v2'\n",
    "\n",
    "local_username = 'jkrichel'\n",
    "if os.getcwd().split('/')[-1] != local_username:\n",
    "        os.chdir('..')\n",
    "\n",
    "commit_SHA = ''\n",
    "\n",
    "if os.path.exists(github_repo) and os.path.isdir(github_repo):\n",
    "    os.chdir(github_repo)\n",
    "    if len(commit_SHA):\n",
    "        ! git reset --hard {commit_SHA}\n",
    "        ! git cherry-pick {commit_SHA}\n",
    "    else:\n",
    "        ! git reset --hard HEAD\n",
    "        ! git pull\n",
    "else:\n",
    "    ! git clone https://{os.environ[\"GITHUB_AUTH\"]}@github.com/{github_username}/{github_repo}.git\n",
    "    os.chdir(github_repo)\n",
    "    ! git config pull.rebase false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0d5bee-f172-4c4e-b247-677c5b7ba8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['vit_l_16']\n",
      "Epochs num: 10\n",
      "Learning rates: [0.0001]\n",
      "Total number of train images: 7823\n",
      "Total number of test images: 1621\n",
      "Using cuda\n",
      "Fine-tuning vit_l_16 with 303333407 parameters for 10 epochs using lr=0.0001 on cuda...\n",
      "####################################################################################################\n",
      "\n",
      "Completed batch num 10/245 in 1.52 seconds. Batch fine-grain loss: 2.94, batch coarse-grain loss: 2.95, alpha value: 0.93\n",
      "Completed batch num 20/245 in 1.52 seconds. Batch fine-grain loss: 2.68, batch coarse-grain loss: 2.2, alpha value: 0.89\n",
      "Completed batch num 30/245 in 1.53 seconds. Batch fine-grain loss: 2.36, batch coarse-grain loss: 1.73, alpha value: 0.93\n",
      "Completed batch num 40/245 in 1.53 seconds. Batch fine-grain loss: 2.38, batch coarse-grain loss: 1.44, alpha value: 0.97\n",
      "Completed batch num 50/245 in 1.54 seconds. Batch fine-grain loss: 1.84, batch coarse-grain loss: 1.27, alpha value: 1.01\n",
      "Completed batch num 60/245 in 1.54 seconds. Batch fine-grain loss: 2.23, batch coarse-grain loss: 1.56, alpha value: 1.02\n",
      "Completed batch num 70/245 in 1.54 seconds. Batch fine-grain loss: 1.89, batch coarse-grain loss: 1.75, alpha value: 1.01\n",
      "Completed batch num 80/245 in 1.54 seconds. Batch fine-grain loss: 2.2, batch coarse-grain loss: 2.08, alpha value: 0.99\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 2.09, batch coarse-grain loss: 1.26, alpha value: 0.98\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 1.75, batch coarse-grain loss: 1.16, alpha value: 0.99\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 1.5, batch coarse-grain loss: 1.3, alpha value: 1.0\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 1.76, batch coarse-grain loss: 1.43, alpha value: 0.99\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 1.64, batch coarse-grain loss: 1.48, alpha value: 0.99\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 2.16, batch coarse-grain loss: 1.43, alpha value: 0.99\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 2.11, batch coarse-grain loss: 1.58, alpha value: 1.0\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 1.63, batch coarse-grain loss: 1.27, alpha value: 1.0\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 1.96, batch coarse-grain loss: 1.4, alpha value: 1.0\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 1.61, batch coarse-grain loss: 1.33, alpha value: 1.0\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 1.78, batch coarse-grain loss: 1.29, alpha value: 1.0\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 1.57, batch coarse-grain loss: 1.19, alpha value: 1.0\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 1.6, batch coarse-grain loss: 1.3, alpha value: 1.0\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 1.32, batch coarse-grain loss: 1.22, alpha value: 1.0\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 1.55, batch coarse-grain loss: 1.23, alpha value: 1.0\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 1.79, batch coarse-grain loss: 1.37, alpha value: 1.0\n",
      "\n",
      "Epoch 1/10 done in 6 minutes, \n",
      "Training fine loss: 1.97\n",
      "training coarse loss: 1.52\n",
      "training fine accuracy: 39.15, fine f1: 38.44%\n",
      "training coarse accuracy: 47.36%, coarse f1: 38.33%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m57.8\u001b[0m%, fine f1: \u001b[92m54.94\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m60.52\u001b[0m%, coarse f1: \u001b[92m53.77\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m553\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m34.11\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 1.38, batch coarse-grain loss: 0.93, alpha value: 0.95\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.82, alpha value: 0.99\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 1.09, batch coarse-grain loss: 1.18, alpha value: 1.01\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 1.45, batch coarse-grain loss: 1.22, alpha value: 1.05\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 1.19, batch coarse-grain loss: 1.02, alpha value: 1.05\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 0.98, alpha value: 1.05\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 1.75, batch coarse-grain loss: 1.79, alpha value: 1.01\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 1.48, batch coarse-grain loss: 1.33, alpha value: 0.99\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 1.01, batch coarse-grain loss: 1.22, alpha value: 0.96\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 1.04, alpha value: 0.94\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 1.34, batch coarse-grain loss: 0.92, alpha value: 0.95\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 1.57, batch coarse-grain loss: 0.96, alpha value: 0.95\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 1.62, batch coarse-grain loss: 1.0, alpha value: 0.96\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 1.56, batch coarse-grain loss: 0.86, alpha value: 0.98\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 1.24, batch coarse-grain loss: 0.59, alpha value: 0.99\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 1.76, batch coarse-grain loss: 0.85, alpha value: 1.01\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 1.37, batch coarse-grain loss: 0.99, alpha value: 1.02\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 1.55, batch coarse-grain loss: 0.81, alpha value: 1.03\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 1.24, batch coarse-grain loss: 0.84, alpha value: 1.04\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 1.35, batch coarse-grain loss: 1.01, alpha value: 1.05\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 1.49, batch coarse-grain loss: 1.23, alpha value: 1.04\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 1.34, alpha value: 1.03\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 1.59, batch coarse-grain loss: 1.54, alpha value: 1.01\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 2.06, alpha value: 0.97\n",
      "\n",
      "Epoch 2/10 done in 6 minutes, \n",
      "Training fine loss: 1.36\n",
      "training coarse loss: 1.09\n",
      "training fine accuracy: 57.86, fine f1: 57.77%\n",
      "training coarse accuracy: 65.59%, coarse f1: 52.95%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m59.35\u001b[0m%, fine f1: \u001b[92m57.87\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m50.22\u001b[0m%, coarse f1: \u001b[92m64.79\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m808\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m49.85\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.68, alpha value: 0.79\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 1.03, batch coarse-grain loss: 0.54, alpha value: 0.94\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.6, alpha value: 1.05\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.53, alpha value: 1.09\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.6, alpha value: 1.1\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 1.15, alpha value: 1.08\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 1.31, alpha value: 1.01\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 1.25, batch coarse-grain loss: 1.23, alpha value: 0.95\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 1.37, batch coarse-grain loss: 1.09, alpha value: 0.94\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.93, alpha value: 0.94\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.56, alpha value: 0.96\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 1.53, batch coarse-grain loss: 0.89, alpha value: 0.99\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.89, alpha value: 1.0\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.7, alpha value: 1.01\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 1.08, batch coarse-grain loss: 0.97, alpha value: 1.02\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 1.68, batch coarse-grain loss: 1.06, alpha value: 1.03\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 1.32, batch coarse-grain loss: 0.61, alpha value: 1.04\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 1.29, batch coarse-grain loss: 0.91, alpha value: 1.05\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 1.26, batch coarse-grain loss: 0.94, alpha value: 1.05\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 1.49, batch coarse-grain loss: 1.12, alpha value: 1.04\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.95, batch coarse-grain loss: 1.12, alpha value: 1.02\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 1.28, alpha value: 0.99\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 1.41, batch coarse-grain loss: 1.61, alpha value: 0.97\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 1.27, batch coarse-grain loss: 1.0, alpha value: 0.96\n",
      "\n",
      "Epoch 3/10 done in 6 minutes, \n",
      "Training fine loss: 1.12\n",
      "training coarse loss: 0.9\n",
      "training fine accuracy: 64.8, fine f1: 64.76%\n",
      "training coarse accuracy: 71.3%, coarse f1: 65.32%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m64.84\u001b[0m%, fine f1: \u001b[92m62.36\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m73.29\u001b[0m%, coarse f1: \u001b[92m59.81\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m295\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m18.2\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.45, batch coarse-grain loss: 0.42, alpha value: 0.8\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.65, alpha value: 0.91\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.53, alpha value: 0.98\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.73, batch coarse-grain loss: 0.48, alpha value: 1.02\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 0.81, alpha value: 1.03\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.82, alpha value: 1.04\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.78, alpha value: 1.04\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.62, alpha value: 1.03\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.99, alpha value: 1.01\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.77, batch coarse-grain loss: 0.85, alpha value: 0.99\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.98, batch coarse-grain loss: 0.8, alpha value: 0.96\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 1.1, batch coarse-grain loss: 0.79, alpha value: 0.96\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.77, alpha value: 0.98\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.99, batch coarse-grain loss: 0.83, alpha value: 0.99\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.64, alpha value: 0.99\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.74, alpha value: 1.0\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.99, batch coarse-grain loss: 0.54, alpha value: 1.02\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.74, alpha value: 1.02\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 1.19, batch coarse-grain loss: 0.59, alpha value: 1.02\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.98, alpha value: 1.02\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 1.19, batch coarse-grain loss: 0.73, alpha value: 1.02\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 1.35, batch coarse-grain loss: 0.96, alpha value: 1.01\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 1.44, batch coarse-grain loss: 1.12, alpha value: 1.01\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.88, alpha value: 1.01\n",
      "\n",
      "Epoch 4/10 done in 6 minutes, \n",
      "Training fine loss: 0.94\n",
      "training coarse loss: 0.73\n",
      "training fine accuracy: 71.28, fine f1: 71.21%\n",
      "training coarse accuracy: 78.32%, coarse f1: 74.78%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m62.31\u001b[0m%, fine f1: \u001b[92m60.42\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m72.92\u001b[0m%, coarse f1: \u001b[92m60.22\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m331\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m20.42\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.98, batch coarse-grain loss: 0.72, alpha value: 0.9\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.68, alpha value: 0.91\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.69, batch coarse-grain loss: 0.67, alpha value: 0.95\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.87, batch coarse-grain loss: 0.45, alpha value: 1.0\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.59, alpha value: 1.01\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 1.2, batch coarse-grain loss: 0.58, alpha value: 1.04\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 0.57, alpha value: 1.05\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 0.61, alpha value: 1.02\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.63, batch coarse-grain loss: 0.57, alpha value: 1.02\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.59, batch coarse-grain loss: 0.58, alpha value: 1.0\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.84, alpha value: 1.0\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.78, alpha value: 0.99\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.59, alpha value: 0.98\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 1.09, batch coarse-grain loss: 0.77, alpha value: 0.97\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 1.03, batch coarse-grain loss: 0.67, alpha value: 0.98\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.76, alpha value: 0.98\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.5, alpha value: 0.99\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.46, alpha value: 1.0\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 0.91, batch coarse-grain loss: 0.58, alpha value: 1.01\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.88, batch coarse-grain loss: 0.77, alpha value: 1.01\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.71, batch coarse-grain loss: 0.65, alpha value: 1.01\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.65, alpha value: 1.01\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 1.03, batch coarse-grain loss: 0.69, alpha value: 1.02\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.73, alpha value: 1.02\n",
      "\n",
      "Epoch 5/10 done in 6 minutes, \n",
      "Training fine loss: 0.84\n",
      "training coarse loss: 0.64\n",
      "training fine accuracy: 73.91, fine f1: 74.0%\n",
      "training coarse accuracy: 81.98%, coarse f1: 78.88%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m63.79\u001b[0m%, fine f1: \u001b[92m61.25\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m78.04\u001b[0m%, coarse f1: \u001b[92m59.45\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m252\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m15.55\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 0.7, alpha value: 1.03\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.55, alpha value: 1.01\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 1.1, batch coarse-grain loss: 0.53, alpha value: 1.13\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.69, batch coarse-grain loss: 0.77, alpha value: 1.08\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.54, alpha value: 1.05\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.86, alpha value: 0.95\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.61, batch coarse-grain loss: 0.44, alpha value: 0.9\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.65, batch coarse-grain loss: 0.37, alpha value: 0.91\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.51, alpha value: 0.92\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.47, alpha value: 0.94\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.4, alpha value: 0.96\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.43, alpha value: 0.97\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.62, alpha value: 0.98\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.32, batch coarse-grain loss: 0.29, alpha value: 0.98\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.79, alpha value: 0.99\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.41, alpha value: 1.0\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.53, alpha value: 1.0\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.24, batch coarse-grain loss: 0.36, alpha value: 0.99\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 1.23, batch coarse-grain loss: 0.97, alpha value: 1.0\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.78, alpha value: 1.01\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.41, alpha value: 1.01\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.69, batch coarse-grain loss: 0.31, alpha value: 1.02\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 0.82, batch coarse-grain loss: 0.56, alpha value: 1.02\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.43, alpha value: 1.03\n",
      "\n",
      "Epoch 6/10 done in 6 minutes, \n",
      "Training fine loss: 0.75\n",
      "training coarse loss: 0.56\n",
      "training fine accuracy: 77.04, fine f1: 77.17%\n",
      "training coarse accuracy: 83.43%, coarse f1: 80.27%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m62.68\u001b[0m%, fine f1: \u001b[92m60.92\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m80.32\u001b[0m%, coarse f1: \u001b[92m59.39\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m190\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m11.72\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.58, batch coarse-grain loss: 0.52, alpha value: 0.92\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.58, batch coarse-grain loss: 0.52, alpha value: 1.0\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.59, batch coarse-grain loss: 0.59, alpha value: 0.99\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.46, alpha value: 0.99\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.45, alpha value: 1.05\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.44, alpha value: 1.06\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.47, alpha value: 1.04\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.53, alpha value: 1.03\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.73, batch coarse-grain loss: 0.74, alpha value: 1.0\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.67, batch coarse-grain loss: 0.69, alpha value: 1.0\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.65, batch coarse-grain loss: 0.6, alpha value: 0.99\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.59, alpha value: 0.98\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.45, alpha value: 0.99\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.43, alpha value: 0.99\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 0.73, batch coarse-grain loss: 0.48, alpha value: 0.99\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.67, batch coarse-grain loss: 0.68, alpha value: 0.99\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.66, alpha value: 0.99\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.3, alpha value: 0.99\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 0.67, batch coarse-grain loss: 0.59, alpha value: 0.99\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.72, alpha value: 0.99\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.54, alpha value: 0.99\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.46, alpha value: 1.0\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 0.68, batch coarse-grain loss: 0.6, alpha value: 1.0\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 0.71, batch coarse-grain loss: 0.38, alpha value: 1.01\n",
      "\n",
      "Epoch 7/10 done in 6 minutes, \n",
      "Training fine loss: 0.68\n",
      "training coarse loss: 0.52\n",
      "training fine accuracy: 79.15, fine f1: 79.35%\n",
      "training coarse accuracy: 84.67%, coarse f1: 82.27%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m65.08\u001b[0m%, fine f1: \u001b[92m62.27\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m81.68\u001b[0m%, coarse f1: \u001b[92m63.21\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m172\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m10.61\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.49, batch coarse-grain loss: 0.44, alpha value: 0.88\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.42, batch coarse-grain loss: 0.2, alpha value: 0.94\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.43, alpha value: 1.02\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.79, batch coarse-grain loss: 0.45, alpha value: 1.0\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.51, batch coarse-grain loss: 0.35, alpha value: 1.01\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 0.59, batch coarse-grain loss: 0.54, alpha value: 1.03\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.24, alpha value: 1.04\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.61, alpha value: 1.03\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.41, batch coarse-grain loss: 0.43, alpha value: 1.02\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.44, batch coarse-grain loss: 0.48, alpha value: 1.02\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.53, batch coarse-grain loss: 0.52, alpha value: 0.99\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.39, alpha value: 1.0\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.4, alpha value: 0.99\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.53, alpha value: 0.98\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 0.58, batch coarse-grain loss: 0.39, alpha value: 0.99\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.6, alpha value: 1.0\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.57, batch coarse-grain loss: 0.45, alpha value: 0.99\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.49, batch coarse-grain loss: 0.5, alpha value: 0.98\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.47, alpha value: 0.99\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.46, batch coarse-grain loss: 0.49, alpha value: 0.99\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.36, alpha value: 1.0\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.82, alpha value: 1.0\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 0.59, batch coarse-grain loss: 0.52, alpha value: 1.01\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.43, alpha value: 1.01\n",
      "\n",
      "Epoch 8/10 done in 6 minutes, \n",
      "Training fine loss: 0.61\n",
      "training coarse loss: 0.47\n",
      "training fine accuracy: 81.36, fine f1: 81.45%\n",
      "training coarse accuracy: 86.62%, coarse f1: 84.44%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m63.29\u001b[0m%, fine f1: \u001b[92m61.33\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m81.37\u001b[0m%, coarse f1: \u001b[92m61.28\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m164\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m10.12\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.38, batch coarse-grain loss: 0.48, alpha value: 0.91\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.35, batch coarse-grain loss: 0.44, alpha value: 0.86\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.47, alpha value: 0.93\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.22, batch coarse-grain loss: 0.36, alpha value: 0.96\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.24, batch coarse-grain loss: 0.26, alpha value: 1.02\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 0.38, batch coarse-grain loss: 0.31, alpha value: 1.04\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.59, alpha value: 1.03\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.36, batch coarse-grain loss: 0.5, alpha value: 1.04\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 1.12, batch coarse-grain loss: 0.52, alpha value: 1.04\n",
      "Completed batch num 100/245 in 1.55 seconds. Batch fine-grain loss: 0.35, batch coarse-grain loss: 0.36, alpha value: 1.02\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.39, batch coarse-grain loss: 0.39, alpha value: 1.01\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.55, batch coarse-grain loss: 0.32, alpha value: 1.02\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.43, batch coarse-grain loss: 0.53, alpha value: 1.01\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.47, alpha value: 1.0\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.4, alpha value: 1.01\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.69, batch coarse-grain loss: 0.46, alpha value: 1.01\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 0.68, batch coarse-grain loss: 0.55, alpha value: 1.01\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.5, alpha value: 1.02\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 0.64, batch coarse-grain loss: 0.53, alpha value: 1.01\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.57, batch coarse-grain loss: 0.53, alpha value: 1.0\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.56, batch coarse-grain loss: 0.42, alpha value: 1.0\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.58, alpha value: 0.99\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 0.64, batch coarse-grain loss: 0.47, alpha value: 0.99\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.63, alpha value: 0.99\n",
      "\n",
      "Epoch 9/10 done in 6 minutes, \n",
      "Training fine loss: 0.58\n",
      "training coarse loss: 0.46\n",
      "training fine accuracy: 82.04, fine f1: 81.91%\n",
      "training coarse accuracy: 86.55%, coarse f1: 84.78%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m62.99\u001b[0m%, fine f1: \u001b[92m60.82\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m80.14\u001b[0m%, coarse f1: \u001b[92m57.83\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m180\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m11.1\u001b[0m%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 1.55 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.51, alpha value: 1.01\n",
      "Completed batch num 20/245 in 1.55 seconds. Batch fine-grain loss: 0.44, batch coarse-grain loss: 0.53, alpha value: 0.94\n",
      "Completed batch num 30/245 in 1.55 seconds. Batch fine-grain loss: 0.58, batch coarse-grain loss: 0.41, alpha value: 0.93\n",
      "Completed batch num 40/245 in 1.55 seconds. Batch fine-grain loss: 0.19, batch coarse-grain loss: 0.29, alpha value: 0.92\n",
      "Completed batch num 50/245 in 1.55 seconds. Batch fine-grain loss: 0.51, batch coarse-grain loss: 0.33, alpha value: 0.95\n",
      "Completed batch num 60/245 in 1.55 seconds. Batch fine-grain loss: 0.26, batch coarse-grain loss: 0.14, alpha value: 0.97\n",
      "Completed batch num 70/245 in 1.55 seconds. Batch fine-grain loss: 0.24, batch coarse-grain loss: 0.21, alpha value: 0.98\n",
      "Completed batch num 80/245 in 1.55 seconds. Batch fine-grain loss: 0.49, batch coarse-grain loss: 0.32, alpha value: 1.0\n",
      "Completed batch num 90/245 in 1.55 seconds. Batch fine-grain loss: 0.27, batch coarse-grain loss: 0.28, alpha value: 1.01\n",
      "Completed batch num 100/245 in 1.56 seconds. Batch fine-grain loss: 0.56, batch coarse-grain loss: 0.43, alpha value: 1.05\n",
      "Completed batch num 110/245 in 1.55 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.42, alpha value: 1.04\n",
      "Completed batch num 120/245 in 1.55 seconds. Batch fine-grain loss: 0.21, batch coarse-grain loss: 0.15, alpha value: 1.04\n",
      "Completed batch num 130/245 in 1.55 seconds. Batch fine-grain loss: 0.49, batch coarse-grain loss: 0.33, alpha value: 1.06\n",
      "Completed batch num 140/245 in 1.55 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.45, alpha value: 1.06\n",
      "Completed batch num 150/245 in 1.55 seconds. Batch fine-grain loss: 0.61, batch coarse-grain loss: 0.37, alpha value: 1.06\n",
      "Completed batch num 160/245 in 1.55 seconds. Batch fine-grain loss: 0.77, batch coarse-grain loss: 0.62, alpha value: 1.05\n",
      "Completed batch num 170/245 in 1.55 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 0.62, alpha value: 1.05\n",
      "Completed batch num 180/245 in 1.55 seconds. Batch fine-grain loss: 0.49, batch coarse-grain loss: 0.54, alpha value: 1.04\n",
      "Completed batch num 190/245 in 1.55 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.95, alpha value: 1.01\n",
      "Completed batch num 200/245 in 1.55 seconds. Batch fine-grain loss: 0.52, batch coarse-grain loss: 0.65, alpha value: 0.97\n",
      "Completed batch num 210/245 in 1.55 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.49, alpha value: 0.96\n",
      "Completed batch num 220/245 in 1.55 seconds. Batch fine-grain loss: 0.29, batch coarse-grain loss: 0.32, alpha value: 0.95\n",
      "Completed batch num 230/245 in 1.55 seconds. Batch fine-grain loss: 0.41, batch coarse-grain loss: 0.47, alpha value: 0.96\n",
      "Completed batch num 240/245 in 1.55 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.22, alpha value: 0.96\n",
      "\n",
      "Epoch 10/10 done in 6 minutes, \n",
      "Training fine loss: 0.56\n",
      "training coarse loss: 0.45\n",
      "training fine accuracy: 83.05, fine f1: 83.06%\n",
      "training coarse accuracy: 86.41%, coarse f1: 84.06%\n",
      "\n",
      "Testing vit_l_16 on cuda...\n",
      "\n",
      "Test fine accuracy: \u001b[92m63.36\u001b[0m%, fine f1: \u001b[92m61.43\u001b[0m%\n",
      "Test coarse accuracy: \u001b[92m80.38\u001b[0m%, coarse f1: \u001b[92m58.14\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m127\u001b[0m/\u001b[91m1621\u001b[0m which is \u001b[91m7.83\u001b[0m%\n",
      "####################################################################################################\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "from vit_pipeline import run_combined_fine_tuning_pipeline\n",
    "\n",
    "run_combined_fine_tuning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b89206-ded9-4131-917e-bfe7cebd786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['vit_b_16']\n",
      "Epochs num: 10\n",
      "Learning rates: [0.0001]\n",
      "Total number of train images: 7823\n",
      "Total number of test images: 1621\n",
      "Initiating vit_b_16\n",
      "Started fine-tuning individual models with fine_lr=0.0001 and coarse_lr=0.0001for 10 epochs on cuda:0 and cuda:1...\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 3.02, batch coarse-grain loss: 1.4\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 2.95, batch coarse-grain loss: 1.44\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 2.6, batch coarse-grain loss: 1.44\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 2.28, batch coarse-grain loss: 0.64\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 2.45, batch coarse-grain loss: 0.99\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 2.47, batch coarse-grain loss: 1.26\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 2.22, batch coarse-grain loss: 1.01\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 1.99, batch coarse-grain loss: 0.9\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 2.29, batch coarse-grain loss: 1.25\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 2.22, batch coarse-grain loss: 1.34\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 2.32, batch coarse-grain loss: 1.22\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 2.12, batch coarse-grain loss: 0.85\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 1.95, batch coarse-grain loss: 0.9\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 1.84, batch coarse-grain loss: 0.78\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 2.13, batch coarse-grain loss: 1.0\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 2.44, batch coarse-grain loss: 1.43\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 2.01, batch coarse-grain loss: 0.73\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.65, batch coarse-grain loss: 0.97\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 1.66, batch coarse-grain loss: 0.67\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 1.8, batch coarse-grain loss: 0.94\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 1.48, batch coarse-grain loss: 0.54\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 1.68, batch coarse-grain loss: 0.99\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.7, batch coarse-grain loss: 0.59\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 1.52, batch coarse-grain loss: 0.87\n",
      "\n",
      "Epoch 1/10 done in 2 minutes, \n",
      "Training fine loss: 2.08\n",
      "training coarse loss: 1.02\n",
      "training fine accuracy: 36.53, fine f1: 35.25%\n",
      "training coarse accuracy: 62.99%, coarse f1: 54.03%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 49.72%, fine f1: 46.96%\n",
      "Test coarse accuracy: 74.28%, coarse f1: 46.7%\n",
      "\n",
      "Total prior inconsistencies 363/1621 which is 22.39%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 1.59, batch coarse-grain loss: 0.85\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 1.52, batch coarse-grain loss: 0.69\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 1.54, batch coarse-grain loss: 0.6\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 1.57, batch coarse-grain loss: 1.1\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 1.41, batch coarse-grain loss: 0.53\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 1.75, batch coarse-grain loss: 1.01\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 1.37, batch coarse-grain loss: 0.53\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 1.62, batch coarse-grain loss: 0.72\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.41, batch coarse-grain loss: 0.67\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 1.33, batch coarse-grain loss: 0.51\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 1.28, batch coarse-grain loss: 0.47\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.78, batch coarse-grain loss: 0.82\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 1.66, batch coarse-grain loss: 0.65\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 1.8, batch coarse-grain loss: 1.03\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 1.3, batch coarse-grain loss: 0.52\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.59, batch coarse-grain loss: 0.83\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 1.16, batch coarse-grain loss: 0.39\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.58, batch coarse-grain loss: 0.98\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 1.36, batch coarse-grain loss: 0.49\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 1.52, batch coarse-grain loss: 0.86\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 1.52, batch coarse-grain loss: 0.65\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 1.14, batch coarse-grain loss: 0.77\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.74, batch coarse-grain loss: 0.76\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.69\n",
      "\n",
      "Epoch 2/10 done in 2 minutes, \n",
      "Training fine loss: 1.52\n",
      "training coarse loss: 0.74\n",
      "training fine accuracy: 52.82, fine f1: 52.57%\n",
      "training coarse accuracy: 74.59%, coarse f1: 69.02%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 59.65%, fine f1: 57.25%\n",
      "Test coarse accuracy: 77.85%, coarse f1: 61.33%\n",
      "\n",
      "Total prior inconsistencies 313/1621 which is 19.31%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 1.09, batch coarse-grain loss: 0.58\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.45\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 1.23, batch coarse-grain loss: 0.65\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.59\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 1.47, batch coarse-grain loss: 0.62\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 1.47, batch coarse-grain loss: 0.6\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 1.41, batch coarse-grain loss: 0.68\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 1.3, batch coarse-grain loss: 0.53\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.36, batch coarse-grain loss: 0.77\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 1.51, batch coarse-grain loss: 0.65\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 1.38, batch coarse-grain loss: 0.67\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 1.38, batch coarse-grain loss: 0.76\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 1.24, batch coarse-grain loss: 0.62\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 1.19, batch coarse-grain loss: 0.37\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.32, batch coarse-grain loss: 0.68\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 1.33, batch coarse-grain loss: 0.73\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 1.17, batch coarse-grain loss: 0.59\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.63\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 1.58, batch coarse-grain loss: 0.71\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 1.7, batch coarse-grain loss: 0.69\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 1.3, batch coarse-grain loss: 0.8\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 0.56\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 1.23, batch coarse-grain loss: 0.46\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.38, batch coarse-grain loss: 0.61\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.38\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 1.38, batch coarse-grain loss: 0.39\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 1.08, batch coarse-grain loss: 0.28\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 1.25, batch coarse-grain loss: 0.7\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.5, batch coarse-grain loss: 0.47\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 1.22, batch coarse-grain loss: 0.74\n",
      "\n",
      "Epoch 4/10 done in 2 minutes, \n",
      "Training fine loss: 1.2\n",
      "training coarse loss: 0.61\n",
      "training fine accuracy: 62.69, fine f1: 62.72%\n",
      "training coarse accuracy: 79.2%, coarse f1: 75.39%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 65.76%, fine f1: 63.47%\n",
      "Test coarse accuracy: 79.95%, coarse f1: 63.22%\n",
      "\n",
      "Total prior inconsistencies 279/1621 which is 17.21%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.47\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 1.08, batch coarse-grain loss: 0.67\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.71\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.32\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 0.47\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 1.1, batch coarse-grain loss: 0.85\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 1.4, batch coarse-grain loss: 0.89\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.61\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.42\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.9, batch coarse-grain loss: 0.54\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.58\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.07, batch coarse-grain loss: 0.86\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.82\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 0.84, batch coarse-grain loss: 0.39\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.26\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.18, batch coarse-grain loss: 1.0\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 1.47, batch coarse-grain loss: 0.72\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.24, batch coarse-grain loss: 0.59\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 1.23, batch coarse-grain loss: 0.58\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 1.06, batch coarse-grain loss: 0.44\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 1.37, batch coarse-grain loss: 0.53\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 1.44, batch coarse-grain loss: 0.49\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.46, batch coarse-grain loss: 0.95\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 0.42\n",
      "\n",
      "Epoch 5/10 done in 2 minutes, \n",
      "Training fine loss: 1.08\n",
      "training coarse loss: 0.55\n",
      "training fine accuracy: 66.1, fine f1: 66.07%\n",
      "training coarse accuracy: 81.08%, coarse f1: 77.79%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 64.1%, fine f1: 61.78%\n",
      "Test coarse accuracy: 80.69%, coarse f1: 63.9%\n",
      "\n",
      "Total prior inconsistencies 250/1621 which is 15.42%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.89, batch coarse-grain loss: 0.36\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 1.14, batch coarse-grain loss: 0.51\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 1.2, batch coarse-grain loss: 0.53\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 1.09, batch coarse-grain loss: 0.42\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.48, batch coarse-grain loss: 0.38\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.82, batch coarse-grain loss: 0.39\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.95, batch coarse-grain loss: 0.39\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.95, batch coarse-grain loss: 0.26\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 0.9, batch coarse-grain loss: 0.47\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 1.12, batch coarse-grain loss: 0.25\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 1.14, batch coarse-grain loss: 0.28\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 0.88, batch coarse-grain loss: 0.44\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 0.48\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 0.29\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 1.05, batch coarse-grain loss: 0.6\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.02, batch coarse-grain loss: 0.43\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.99, batch coarse-grain loss: 0.69\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.26, batch coarse-grain loss: 0.6\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.31\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.73, batch coarse-grain loss: 0.37\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 0.87, batch coarse-grain loss: 0.36\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 0.9, batch coarse-grain loss: 0.79\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.6, batch coarse-grain loss: 1.01\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 0.45\n",
      "\n",
      "Epoch 6/10 done in 2 minutes, \n",
      "Training fine loss: 1.01\n",
      "training coarse loss: 0.52\n",
      "training fine accuracy: 69.03, fine f1: 68.87%\n",
      "training coarse accuracy: 82.55%, coarse f1: 79.91%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 60.46%, fine f1: 60.02%\n",
      "Test coarse accuracy: 79.77%, coarse f1: 62.06%\n",
      "\n",
      "Total prior inconsistencies 267/1621 which is 16.47%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.83, batch coarse-grain loss: 0.41\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.5\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.29\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.25\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 1.3, batch coarse-grain loss: 0.73\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.83, batch coarse-grain loss: 0.4\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 1.2, batch coarse-grain loss: 0.51\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.63, batch coarse-grain loss: 0.26\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.01, batch coarse-grain loss: 0.3\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.44\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.51\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.12, batch coarse-grain loss: 0.76\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 0.69, batch coarse-grain loss: 0.31\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 0.95, batch coarse-grain loss: 0.4\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 1.02, batch coarse-grain loss: 0.45\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.52\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.77, batch coarse-grain loss: 0.39\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.01, batch coarse-grain loss: 0.5\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 1.03, batch coarse-grain loss: 0.67\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.41, batch coarse-grain loss: 0.29\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 1.12, batch coarse-grain loss: 0.41\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.24\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.42\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.87, batch coarse-grain loss: 0.8\n",
      "\n",
      "Epoch 7/10 done in 2 minutes, \n",
      "Training fine loss: 0.94\n",
      "training coarse loss: 0.47\n",
      "training fine accuracy: 70.91, fine f1: 70.87%\n",
      "training coarse accuracy: 84.21%, coarse f1: 81.4%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 65.14%, fine f1: 62.05%\n",
      "Test coarse accuracy: 82.85%, coarse f1: 58.41%\n",
      "\n",
      "Total prior inconsistencies 245/1621 which is 15.11%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.95, batch coarse-grain loss: 0.45\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.47\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.77, batch coarse-grain loss: 0.58\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.59\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.75, batch coarse-grain loss: 0.3\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.55, batch coarse-grain loss: 0.24\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.47, batch coarse-grain loss: 0.21\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.57, batch coarse-grain loss: 0.31\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.0, batch coarse-grain loss: 0.48\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.56, batch coarse-grain loss: 0.24\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 0.93, batch coarse-grain loss: 0.36\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.11, batch coarse-grain loss: 0.45\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.64\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 0.88, batch coarse-grain loss: 0.36\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.31\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.19, batch coarse-grain loss: 0.5\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.29\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.83\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.44\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.75, batch coarse-grain loss: 0.45\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 0.83, batch coarse-grain loss: 0.71\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 1.15, batch coarse-grain loss: 0.21\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 1.02, batch coarse-grain loss: 0.47\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.37\n",
      "\n",
      "Epoch 8/10 done in 2 minutes, \n",
      "Training fine loss: 0.87\n",
      "training coarse loss: 0.45\n",
      "training fine accuracy: 73.34, fine f1: 73.38%\n",
      "training coarse accuracy: 84.29%, coarse f1: 81.79%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 65.58%, fine f1: 63.7%\n",
      "Test coarse accuracy: 81.62%, coarse f1: 60.22%\n",
      "\n",
      "Total prior inconsistencies 266/1621 which is 16.41%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.43\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.79, batch coarse-grain loss: 0.62\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.51, batch coarse-grain loss: 0.48\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.34\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.44\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.43\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.65, batch coarse-grain loss: 0.31\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.33, batch coarse-grain loss: 0.26\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 1.08, batch coarse-grain loss: 0.6\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.41\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 1.33, batch coarse-grain loss: 0.66\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 0.88, batch coarse-grain loss: 0.4\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.29\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 1.0, batch coarse-grain loss: 0.45\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.31\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 0.8, batch coarse-grain loss: 0.34\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.83, batch coarse-grain loss: 0.53\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 0.75, batch coarse-grain loss: 0.42\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.33\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.28\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 0.71, batch coarse-grain loss: 0.61\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 0.83, batch coarse-grain loss: 0.45\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.27\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.99, batch coarse-grain loss: 0.47\n",
      "\n",
      "Epoch 9/10 done in 2 minutes, \n",
      "Training fine loss: 0.8\n",
      "training coarse loss: 0.43\n",
      "training fine accuracy: 75.55, fine f1: 75.59%\n",
      "training coarse accuracy: 85.77%, coarse f1: 83.63%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 68.6%, fine f1: 66.81%\n",
      "Test coarse accuracy: 81.49%, coarse f1: 68.19%\n",
      "\n",
      "Total prior inconsistencies 256/1621 which is 15.79%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.49\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.98, batch coarse-grain loss: 0.41\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.6, batch coarse-grain loss: 0.42\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.91, batch coarse-grain loss: 0.45\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.48\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.98, batch coarse-grain loss: 0.7\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.53, batch coarse-grain loss: 0.08\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.45, batch coarse-grain loss: 0.32\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 0.43, batch coarse-grain loss: 0.43\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.88, batch coarse-grain loss: 0.3\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 0.62, batch coarse-grain loss: 0.43\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.11\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 1.21, batch coarse-grain loss: 0.48\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 1.15, batch coarse-grain loss: 0.68\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 0.59, batch coarse-grain loss: 0.17\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 1.04, batch coarse-grain loss: 0.39\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.63, batch coarse-grain loss: 0.42\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 1.28, batch coarse-grain loss: 0.62\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.67, batch coarse-grain loss: 0.46\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.5, batch coarse-grain loss: 0.07\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 0.72, batch coarse-grain loss: 0.35\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 0.61, batch coarse-grain loss: 0.51\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 0.81, batch coarse-grain loss: 0.49\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.73, batch coarse-grain loss: 0.33\n",
      "\n",
      "Epoch 10/10 done in 2 minutes, \n",
      "Training fine loss: 0.76\n",
      "training coarse loss: 0.4\n",
      "training fine accuracy: 76.7, fine f1: 76.77%\n",
      "training coarse accuracy: 86.91%, coarse f1: 85.04%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 67.37%, fine f1: 65.76%\n",
      "Test coarse accuracy: 83.41%, coarse f1: 67.53%\n",
      "\n",
      "Total prior inconsistencies 238/1621 which is 14.68%\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Initiating vit_b_16\n",
      "Started fine-tuning individual models with fine_lr=0.0001 and coarse_lr=0.0001for 10 epochs on cuda:0 and cuda:1...\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.96, batch coarse-grain loss: 0.44\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.92, batch coarse-grain loss: 0.38\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.54, batch coarse-grain loss: 0.12\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.67, batch coarse-grain loss: 0.34\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.1\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.2\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.74, batch coarse-grain loss: 0.4\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.35\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 0.76, batch coarse-grain loss: 0.29\n",
      "Completed batch num 100/245 in 0 seconds. Batch fine-grain loss: 0.54, batch coarse-grain loss: 0.24\n",
      "Completed batch num 110/245 in 0 seconds. Batch fine-grain loss: 0.43, batch coarse-grain loss: 0.45\n",
      "Completed batch num 120/245 in 0 seconds. Batch fine-grain loss: 1.14, batch coarse-grain loss: 0.58\n",
      "Completed batch num 130/245 in 0 seconds. Batch fine-grain loss: 0.75, batch coarse-grain loss: 0.34\n",
      "Completed batch num 140/245 in 0 seconds. Batch fine-grain loss: 0.28, batch coarse-grain loss: 0.16\n",
      "Completed batch num 150/245 in 0 seconds. Batch fine-grain loss: 1.0, batch coarse-grain loss: 0.19\n",
      "Completed batch num 160/245 in 0 seconds. Batch fine-grain loss: 0.78, batch coarse-grain loss: 0.42\n",
      "Completed batch num 170/245 in 0 seconds. Batch fine-grain loss: 0.97, batch coarse-grain loss: 0.45\n",
      "Completed batch num 180/245 in 0 seconds. Batch fine-grain loss: 0.68, batch coarse-grain loss: 0.46\n",
      "Completed batch num 190/245 in 0 seconds. Batch fine-grain loss: 0.82, batch coarse-grain loss: 0.56\n",
      "Completed batch num 200/245 in 0 seconds. Batch fine-grain loss: 0.86, batch coarse-grain loss: 0.44\n",
      "Completed batch num 210/245 in 0 seconds. Batch fine-grain loss: 0.54, batch coarse-grain loss: 0.25\n",
      "Completed batch num 220/245 in 0 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.17\n",
      "Completed batch num 230/245 in 0 seconds. Batch fine-grain loss: 0.7, batch coarse-grain loss: 0.48\n",
      "Completed batch num 240/245 in 0 seconds. Batch fine-grain loss: 0.5, batch coarse-grain loss: 0.34\n",
      "\n",
      "Epoch 1/10 done in 2 minutes, \n",
      "Training fine loss: 0.73\n",
      "training coarse loss: 0.4\n",
      "training fine accuracy: 77.44, fine f1: 77.61%\n",
      "training coarse accuracy: 86.86%, coarse f1: 85.46%\n",
      "\n",
      "Started testing...\n",
      "\n",
      "Test fine accuracy: 67.86%, fine f1: 65.81%\n",
      "Test coarse accuracy: 81.43%, coarse f1: 62.54%\n",
      "\n",
      "Total prior inconsistencies 253/1621 which is 15.61%\n",
      "####################################################################################################\n",
      "Completed batch num 10/245 in 0 seconds. Batch fine-grain loss: 0.37, batch coarse-grain loss: 0.36\n",
      "Completed batch num 20/245 in 0 seconds. Batch fine-grain loss: 0.82, batch coarse-grain loss: 0.46\n",
      "Completed batch num 30/245 in 0 seconds. Batch fine-grain loss: 0.94, batch coarse-grain loss: 0.32\n",
      "Completed batch num 40/245 in 0 seconds. Batch fine-grain loss: 0.47, batch coarse-grain loss: 0.37\n",
      "Completed batch num 50/245 in 0 seconds. Batch fine-grain loss: 0.57, batch coarse-grain loss: 0.48\n",
      "Completed batch num 60/245 in 0 seconds. Batch fine-grain loss: 0.66, batch coarse-grain loss: 0.31\n",
      "Completed batch num 70/245 in 0 seconds. Batch fine-grain loss: 0.57, batch coarse-grain loss: 0.39\n",
      "Completed batch num 80/245 in 0 seconds. Batch fine-grain loss: 0.28, batch coarse-grain loss: 0.28\n",
      "Completed batch num 90/245 in 0 seconds. Batch fine-grain loss: 0.56, batch coarse-grain loss: 0.44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvit_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_individual_fine_tuning_pipeline\n\u001b[0;32m----> 3\u001b[0m \u001b[43mrun_individual_fine_tuning_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/metacognitive_error_detection_and_correction_v2/vit_pipeline.py:611\u001b[0m, in \u001b[0;36mrun_individual_fine_tuning_pipeline\u001b[0;34m(debug)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitiating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuner\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_handlers\u001b[38;5;241m.\u001b[39mClearSession():\n\u001b[0;32m--> 611\u001b[0m     \u001b[43mfine_tune_individual_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfine_tuners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfine_tuners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_fine_grain_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_fine_grain_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mnum_coarse_grain_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_coarse_grain_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/metacognitive_error_detection_and_correction_v2/vit_pipeline.py:309\u001b[0m, in \u001b[0;36mfine_tune_individual_models\u001b[0;34m(fine_tuners, devices, loaders, num_fine_grain_classes, num_coarse_grain_classes, fine_lr, coarse_lr)\u001b[0m\n\u001b[1;32m    306\u001b[0m batch_fine_grain_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    307\u001b[0m batch_coarse_grain_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 309\u001b[0m \u001b[43mfine_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m coarse_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    312\u001b[0m running_fine_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_fine_grain_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/envs/pytorch-gpu-1.13.1/lib/python3.10/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vit_pipeline import run_individual_fine_tuning_pipeline\n",
    "\n",
    "run_individual_fine_tuning_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae115fc-f199-49e0-84a7-8c1d61eef167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: individual_results/ (stored 0%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e8_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e5_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e8_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e4_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e5_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e3_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e1_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/test_true.npy (deflated 99%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e8_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e9_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e3_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/test_true_coarse.npy (deflated 99%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e0_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e2_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e1_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e9_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e5_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e1_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e2_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e2_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e5_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e0_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e4_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e4_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e6_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e9_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e7_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e6_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e1_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e3_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e3_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e7_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e6_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e0_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e8_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e8_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e2_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e0_fine_individual.npy (deflated 90%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e5_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e7_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e0_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e4_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e3_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e8_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e3_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e7_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e5_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e1_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e4_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e7_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e9_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e9_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e9_coarse_individual.npy (deflated 94%)\n",
      "  adding: individual_results/test_true_fine_individual.npy (deflated 47%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e1_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e2_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e3.npy (deflated 92%)\n",
      "  adding: individual_results/test_true_coarse_individual.npy (deflated 47%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e6_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e7_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e2_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-06_e0_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e4_coarse_individual.npy (deflated 95%)\n",
      "  adding: individual_results/vit_b_16_test_pred_lr0.0001_e6_fine_individual.npy (deflated 91%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr1e-05_e3_coarse.npy (deflated 95%)\n",
      "  adding: individual_results/vit_l_16_test_pred_lr0.0001_e6_fine_individual.npy (deflated 91%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "github_repo = 'metacognitive_error_detection_and_correction_v2'\n",
    "\n",
    "if os.path.exists(github_repo) and os.path.isdir(github_repo):\n",
    "    os.chdir(github_repo)\n",
    "    \n",
    "! zip -r individual_results.zip individual_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46c4ff-5c83-4dd2-b582-cc035474cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-1.13.1",
   "language": "python",
   "name": "pytorch-gpu-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
