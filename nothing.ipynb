{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pipeline import *\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import context_handlers\n",
    "import models\n",
    "import utils\n",
    "import data_preprocessing\n",
    "from ltn_support import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sat_testing_value(logits_to_predicate,\n",
    "                         prediction, labels_coarse, labels_fine):\n",
    "    \"\"\"\n",
    "    compute satagg function for rules\n",
    "    argument:\n",
    "      - logits_to_predicate: get the satisfaction of a variable given the label\n",
    "      - prediction: output of fine tuner, \n",
    "      - labels_coarse, labels_fine: ground truth of coarse and fine label\n",
    "      - fine_to_coarse: dictionary mapping fine-grain class to coarse-grain class\n",
    "\n",
    "    return:\n",
    "      sat_agg: sat_agg for all the rules\n",
    "\n",
    "    \"\"\"\n",
    "    Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "    And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "    Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "    Forall = ltn.Quantifier(\n",
    "        ltn.fuzzy_ops.AggregPMeanError(p=4), quantifier=\"f\")\n",
    "    SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "    fine_label_dict = {name: label for label, name in enumerate(data_preprocessing.fine_grain_classes)}\n",
    "    coarse_label_dict = {name: label + len(data_preprocessing.fine_grain_classes) for label, name in enumerate(data_preprocessing.coarse_grain_classes)}\n",
    "    labels_fine = labels_fine.detach().to('cpu')\n",
    "    labels_coarse = labels_coarse.detach().to('cpu') + len(data_preprocessing.fine_grain_classes)\n",
    "        \n",
    "    # Define constant\n",
    "    l = {}\n",
    "    num_labels = len(data_preprocessing.fine_grain_classes) + len(data_preprocessing.coarse_grain_classes)\n",
    "    for label in range(num_labels):\n",
    "        one_hot = torch.zeros(num_labels)\n",
    "        one_hot[label] = 1.0\n",
    "        l[label] = ltn.Constant(one_hot, trainable=True)\n",
    "\n",
    "    # Define variables\n",
    "    x_variables = {}\n",
    "    x = ltn.Variable(\"x\", prediction)\n",
    "\n",
    "    for name, label in fine_label_dict.items():\n",
    "        x_variables[label] = ltn.Variable(\n",
    "            name, prediction[labels_fine == label])\n",
    "    for name, label in coarse_label_dict.items():\n",
    "        x_variables[label] = ltn.Variable(\n",
    "            name, prediction[labels_coarse == label])\n",
    "\n",
    "    sat_agg_label = []\n",
    "\n",
    "    # Rewrite the inconsistency code (Forall(x, Implies(P(x,coarse_label), Not(P(x,coarse_to_not_fine))))\n",
    "    for coarse_label, i in coarse_label_dict.items():\n",
    "        for fine_label, j in fine_label_dict.items():\n",
    "            corresponding_coarse_label = data_preprocessing.fine_to_course_idx[j] + len(fine_label_dict)\n",
    "            if (corresponding_coarse_label != i):\n",
    "                satisfaction = Forall(x,\n",
    "                                      Implies(logits_to_predicate(x,l[i]), \n",
    "                                              Not(logits_to_predicate(x,l[j]))\n",
    "                                      )\n",
    "                )\n",
    "                sat_agg_label.append([0, \n",
    "                                      f\"for all x, P(x, l[{coarse_label}]) imply -P(x, l[{fine_label}])\", \n",
    "                                      satisfaction.value.detach().item()])\n",
    "            else:\n",
    "                satisfaction = Forall(x,\n",
    "                                      Implies(logits_to_predicate(x,l[i]), \n",
    "                                              Not(logits_to_predicate(x,l[j]))\n",
    "                                      )\n",
    "                )\n",
    "                sat_agg_label.append([0, \n",
    "                                      f\"wrong rules: for all x, P(x, l[{coarse_label}]) imply -P(x, l[{fine_label}])\", \n",
    "                                      satisfaction.value.detach().item()])\n",
    "    # Coarse labels: for all x[i], x[i] -> l[i]\n",
    "\n",
    "    for coarse_label, i in coarse_label_dict.items():\n",
    "        satisfaction = Forall(x_variables[i], logits_to_predicate(x_variables[i], l[i]))\n",
    "        sat_agg_label.append([1, \n",
    "                              f'for all {coarse_label}, P(x[{coarse_label}], l[{coarse_label}])', \n",
    "                              satisfaction.value.detach().item()])\n",
    "\n",
    "    # Coarse Label: for all x, - (P(x, l[coarse_1] and x[different coarse]}\n",
    "\n",
    "    for coarse_label_1, i in coarse_label_dict.items():\n",
    "        for coarse_label_2, j in coarse_label_dict.items():\n",
    "            if i != j :\n",
    "                satisfaction = Forall(x, Not(And(logits_to_predicate(x, l[i]), logits_to_predicate(x, l[j]))))\n",
    "                sat_agg_label.append([2, \n",
    "                                      f\"for all x, - (P(x, {coarse_label_1}) and P(x,{coarse_label_2}))\", \n",
    "                                      satisfaction.value.detach().item()])\n",
    "\n",
    "    # Fine labels: for all x[i], x[i] -> l[i]\n",
    "\n",
    "    for fine_label, i in fine_label_dict.items():\n",
    "        satisfaction = Forall(x_variables[i], logits_to_predicate(x_variables[i], l[i]))\n",
    "        sat_agg_label.append([1, \n",
    "                              f'for all {fine_label}, P(x[{fine_label}], l[{fine_label}])', \n",
    "                              satisfaction.value.detach().item()])\n",
    "    # Fine Label: for all x[fine], - {x[fine] and x[different fine]}\n",
    "\n",
    "    for fine_label_1, i in fine_label_dict.items():\n",
    "        for fine_label_2, j in fine_label_dict.items():\n",
    "            if i != j :\n",
    "                satisfaction = Forall(x, Not(And(logits_to_predicate(x, l[i]), logits_to_predicate(x, l[j]))))\n",
    "                sat_agg_label.append([2, \n",
    "                                      f\"for all x, - (P(x, {fine_label_1}) and P(x,{fine_label_2}))\", \n",
    "                                      satisfaction.value.detach().item()])\n",
    "\n",
    "\n",
    "    return sat_agg_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vit_b_16 on mps...\n"
     ]
    }
   ],
   "source": [
    "fine_tuner = fine_tuners[0]\n",
    "device = devices[0]\n",
    "\n",
    "logits_to_predicate = ltn.Predicate(LogitsToPredicate()).to(ltn.device)\n",
    "\n",
    "test_loader = loaders['test']\n",
    "fine_tuner.to(device)\n",
    "fine_tuner.eval()\n",
    "\n",
    "test_fine_prediction = []\n",
    "test_coarse_prediction = []\n",
    "\n",
    "test_fine_ground_truth = []\n",
    "test_coarse_ground_truth = []\n",
    "\n",
    "name_list = []\n",
    "\n",
    "print(f'Testing {fine_tuner} on {device}...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    if utils.is_local():\n",
    "        from tqdm import tqdm\n",
    "        gen = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    else:\n",
    "        gen = enumerate(test_loader)\n",
    "\n",
    "    for i, data in gen:\n",
    "        X, Y_fine_grain, names, Y_coarse_grain = data[0].to(device), data[1].to(device), data[2], data[3].to(device)\n",
    "\n",
    "        Y_pred = fine_tuner(X)\n",
    "        \n",
    "        if i == 0:\n",
    "            Y_pred_all = Y_pred\n",
    "            Y_fine_grain_all = Y_fine_grain\n",
    "            Y_coarse_grain_all = Y_coarse_grain\n",
    "        else:\n",
    "            Y_pred_all = torch.cat([Y_pred, Y_pred_all], dim=0)\n",
    "            Y_fine_grain_all = torch.cat([Y_fine_grain, Y_fine_grain_all], dim=0)\n",
    "            Y_coarse_grain_all = torch.cat([Y_coarse_grain, Y_coarse_grain_all], dim=0)\n",
    "\n",
    "        Y_pred_fine_grain = Y_pred[:, :len(data_preprocessing.fine_grain_classes)]\n",
    "        Y_pred_coarse_grain = Y_pred[:, len(data_preprocessing.fine_grain_classes):]\n",
    "\n",
    "        predicted_fine = torch.max(Y_pred_fine_grain, 1)[1]\n",
    "        predicted_coarse = torch.max(Y_pred_coarse_grain, 1)[1]\n",
    "\n",
    "        test_fine_ground_truth += Y_fine_grain.tolist()\n",
    "        test_coarse_ground_truth += Y_coarse_grain.tolist()\n",
    "\n",
    "        test_fine_prediction += predicted_fine.tolist()\n",
    "        test_coarse_prediction += predicted_coarse.tolist()\n",
    "\n",
    "        name_list += names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with \u001b[94mBCE\u001b[0m loss\n",
      "\n",
      "Fine-grain prior combined accuracy: \u001b[92m69.15\u001b[0m%, fine-grain prior combined average f1: \u001b[92m67.26\u001b[0m%\n",
      "Coarse-grain prior combined accuracy: \u001b[92m84.33\u001b[0m%, coarse-grain prior combined average f1: \u001b[92m82.67\u001b[0m%\n",
      "\n",
      "Total prior inconsistencies \u001b[91m60\u001b[0m/\u001b[91m1621\u001b[0m (\u001b[91m3.7\u001b[0m%)\n"
     ]
    }
   ],
   "source": [
    "rule_and_confidence_score_list = compute_sat_testing_value(logits_to_predicate,\n",
    "                        Y_pred_all, Y_coarse_grain_all, Y_fine_grain_all)\n",
    "\n",
    "test_fine_accuracy, test_coarse_accuracy = (\n",
    "    get_and_print_metrics(fine_predictions=test_fine_prediction,\n",
    "                          coarse_predictions=test_coarse_prediction, \n",
    "                          loss=\"BCE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the list\n",
    "filtered_list = [item for item in rule_and_confidence_score_list if item[0] == 0 and \"wrong\" in item[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  'wrong rules: for all x, P(x, l[Air Defense]) imply -P(x, l[30N6E])',\n",
       "  0.7139595150947571],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Air Defense]) imply -P(x, l[Iskander])',\n",
       "  0.5634914636611938],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Air Defense]) imply -P(x, l[Pantsir-S1])',\n",
       "  0.6213214993476868],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Air Defense]) imply -P(x, l[Rs-24])',\n",
       "  0.5094217658042908],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BMD]) imply -P(x, l[BMD])',\n",
       "  0.6057076454162598],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BMP]) imply -P(x, l[BMP-1])',\n",
       "  0.7136292457580566],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BMP]) imply -P(x, l[BMP-2])',\n",
       "  0.6492296457290649],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BMP]) imply -P(x, l[BMP-T15])',\n",
       "  0.6144813299179077],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BTR]) imply -P(x, l[BRDM])',\n",
       "  0.5974348783493042],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BTR]) imply -P(x, l[BTR-60])',\n",
       "  0.49617326259613037],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BTR]) imply -P(x, l[BTR-70])',\n",
       "  0.6836447715759277],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[BTR]) imply -P(x, l[BTR-80])',\n",
       "  0.6567354202270508],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[MT_LB]) imply -P(x, l[MT_LB])',\n",
       "  0.5794230699539185],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Self Propelled Artillery]) imply -P(x, l[2S19_MSTA])',\n",
       "  0.6076446771621704],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Self Propelled Artillery]) imply -P(x, l[BM-30])',\n",
       "  0.6194638013839722],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Self Propelled Artillery]) imply -P(x, l[D-30])',\n",
       "  0.5944221019744873],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Self Propelled Artillery]) imply -P(x, l[TOS-1])',\n",
       "  0.6678714156150818],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Self Propelled Artillery]) imply -P(x, l[Tornado])',\n",
       "  0.6832035779953003],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-14])',\n",
       "  0.6099908351898193],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-62])',\n",
       "  0.6090419292449951],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-64])',\n",
       "  0.7096904516220093],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-72])',\n",
       "  0.7086467742919922],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-80])',\n",
       "  0.6768724918365479],\n",
       " [0,\n",
       "  'wrong rules: for all x, P(x, l[Tank]) imply -P(x, l[T-90])',\n",
       "  0.6817224621772766]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in rule_and_confidence_score_list:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rule_and_confidence_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for all x, - (P(x, Tornado) and P(x,TOS-1))'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cr37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
